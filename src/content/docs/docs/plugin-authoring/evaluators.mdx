---
title: Writing a Genkit Evaluator
description: Learn how to write custom Genkit evaluators for heuristic and LLM-based assessments, including defining prompts, scoring functions, and evaluator actions.
---

import LanguageSelector from '../../../../components/LanguageSelector.astro';
import CopyMarkdownButton from '../../../../components/CopyMarkdownButton.astro';
import LanguageContent from '../../../../components/LanguageContent.astro';

<div style="display: flex; justify-content: space-between; align-items: center; gap: 1rem; margin: 1rem 0 1rem 0;">
  <LanguageSelector supportedLanguages="js go python" />
  <CopyMarkdownButton />
</div>

<LanguageContent lang="js go python">

You can extend Genkit to support custom evaluation, using either
an LLM as a judge, or by programmatic (heuristic) evaluation.

## Evaluator definition

Evaluators are functions that assess an LLM's response. There are two main
approaches to automated evaluation: heuristic evaluation and LLM-based
evaluation. In the heuristic approach, you define a deterministic function.
By contrast, in an LLM-based assessment, the content is fed back to an LLM,
and the LLM is asked to score the output according to criteria set in a
prompt.

<LanguageContent lang="js">

The `ai.defineEvaluator` method, which you use to define an
evaluator action in Genkit, supports either approach. This
document explores a couple of examples of how to use this
method for heuristic and LLM-based evaluations.

</LanguageContent>

<LanguageContent lang="go">

The `ai.DefineEvaluator` function, which you use to define an
evaluator action in Genkit, supports either approach. This
document explores a couple of examples of how to use this
function for heuristic and LLM-based evaluations.

</LanguageContent>

<LanguageContent lang="python">

The `define_evaluator` method, which you use to define an
evaluator action in Genkit, supports either approach. This
document explores a couple of examples of how to use this
method for heuristic and LLM-based evaluations.

</LanguageContent>

### LLM-based Evaluators

An LLM-based evaluator leverages an LLM to evaluate
the `input`, `context`, and `output` of your generative AI
feature.

LLM-based evaluators in Genkit are made up of 3 components:

- A prompt
- A scoring function
- An evaluator action

#### Define the prompt

For this example, the evaluator leverages an LLM to determine whether a
food (the `output`) is delicious or not. First, provide context to the LLM,
then describe what you want it to do, and finally, give it a few examples
to base its response on.

<LanguageContent lang="js">

Genkit's `definePrompt` utility provides an easy way to define prompts with
input and output validation. The following code is an example of
setting up an evaluation prompt with `definePrompt`.

```ts
import { z } from "genkit";

const DELICIOUSNESS_VALUES = ['yes', 'no', 'maybe'] as const;

const DeliciousnessDetectionResponseSchema = z.object({
  reason: z.string(),
  verdict: z.enum(DELICIOUSNESS_VALUES),
});

function getDeliciousnessPrompt(ai: Genkit) {
  return  ai.definePrompt({
      name: 'deliciousnessPrompt',
      input: {
        schema: z.object({
          responseToTest: z.string(),
        }),
      },
      output: {
        schema: DeliciousnessDetectionResponseSchema,
      }
      prompt: `You are a food critic. Assess whether the provided output sounds delicious, giving only "yes" (delicious), "no" (not delicious), or "maybe" (undecided) as the verdict.

      Examples:
      Output: Chicken parm sandwich
      Response: { "reason": "A classic and beloved dish.", "verdict": "yes" }

      Output: Boston Logan Airport tarmac
      Response: { "reason": "Not edible.", "verdict": "no" }

      Output: A juicy piece of gossip
      Response: { "reason": "Metaphorically 'tasty' but not food.", "verdict": "maybe" }

      New Output: {{ responseToTest }}
      Response:
      `
  });
}
```

</LanguageContent>

<LanguageContent lang="go">

In Go, you'll define the prompt directly in the scoring function. Here, we define
the response structure for the LLM to follow:

```go
type DeliciousnessResponse struct {
    Reason  string `json:"reason"`
    Verdict string `json:"verdict"`
}

const deliciousnessPrompt = `You are a food critic. Assess whether the provided output sounds delicious, giving only "yes" (delicious), "no" (not delicious), or "maybe" (undecided) as the verdict.

Examples:
Output: Chicken parm sandwich
Response: { "reason": "A classic and beloved dish.", "verdict": "yes" }

Output: Boston Logan Airport tarmac
Response: { "reason": "Not edible.", "verdict": "no" }

Output: A juicy piece of gossip
Response: { "reason": "Metaphorically 'tasty' but not food.", "verdict": "maybe" }

New Output: %s
Response:
`
```

</LanguageContent>

<LanguageContent lang="python">

In Python, you'll define the prompt directly. Here, we define
the response structure for the LLM to follow:

```python
from pydantic import BaseModel

class DeliciousnessResponse(BaseModel):
    reason: str
    verdict: str

DELICIOUSNESS_PROMPT = """You are a food critic. Assess whether the provided output sounds delicious, giving only "yes" (delicious), "no" (not delicious), or "maybe" (undecided) as the verdict.

Examples:
Output: Chicken parm sandwich
Response: { "reason": "A classic and beloved dish.", "verdict": "yes" }

Output: Boston Logan Airport tarmac
Response: { "reason": "Not edible.", "verdict": "no" }

Output: A juicy piece of gossip
Response: { "reason": "Metaphorically 'tasty' but not food.", "verdict": "maybe" }

New Output: {response_to_test}
Response:
"""
```

</LanguageContent>

#### Define the scoring function

Define a function that takes an example that includes `output` as
required by the prompt, and scores the result. Genkit testcases include
`input` as a required field, with `output` and `context` as optional fields.
It is the responsibility of the evaluator to validate that all fields
required for evaluation are present.

<LanguageContent lang="js">

```ts
import { ModelArgument } from 'genkit';
import { BaseEvalDataPoint, Score } from 'genkit/evaluator';

/**
 * Score an individual test case for delciousness.
 */
export async function deliciousnessScore<CustomModelOptions extends z.ZodTypeAny>(
  ai: Genkit,
  judgeLlm: ModelArgument<CustomModelOptions>,
  dataPoint: BaseEvalDataPoint,
  judgeConfig?: CustomModelOptions,
): Promise<Score> {
  const d = dataPoint;
  // Validate the input has required fields
  if (!d.output) {
    throw new Error('Output is required for Deliciousness detection');
  }

  // Hydrate the prompt and generate an evaluation result
  const deliciousnessPrompt = getDeliciousnessPrompt(ai);
  const response = await deliciousnessPrompt(
    {
      responseToTest: d.output as string,
    },
    {
      model: judgeLlm,
      config: judgeConfig,
    },
  );

  // Parse the output
  const parsedResponse = response.output;
  if (!parsedResponse) {
    throw new Error(`Unable to parse evaluator response: ${response.text}`);
  }

  // Return a scored response
  return {
    score: parsedResponse.verdict,
    details: { reasoning: parsedResponse.reason },
  };
}
```

</LanguageContent>

<LanguageContent lang="go">

```go
import (
    "context"
    "encoding/json"
    "errors"
    "fmt"
    
    "github.com/firebase/genkit/go/ai"
)

// Score an individual test case for deliciousness
func deliciousnessScore(
    ctx context.Context,
    judgeLLM ai.Model,
    example *ai.Example,
) (*ai.Score, error) {
    // Validate the input has required fields
    if example.Output == nil {
        return nil, errors.New("output is required for deliciousness detection")
    }
    
    outputStr, ok := example.Output.(string)
    if !ok {
        return nil, errors.New("output must be a string")
    }
    
    // Generate the prompt with the output to test
    prompt := fmt.Sprintf(deliciousnessPrompt, outputStr)
    
    // Call the judge LLM
    resp, err := ai.Generate(ctx, judgeLLM,
        ai.WithPrompt(prompt),
        ai.WithOutputFormat(ai.OutputFormatJSON),
    )
    if err != nil {
        return nil, fmt.Errorf("failed to generate evaluation: %w", err)
    }
    
    // Parse the response
    var result DeliciousnessResponse
    if err := json.Unmarshal([]byte(resp.Text), &result); err != nil {
        return nil, fmt.Errorf("failed to parse evaluator response: %w", err)
    }
    
    // Return a scored response
    return &ai.Score{
        Score:   result.Verdict,
        Details: map[string]any{"reasoning": result.Reason},
    }, nil
}
```

</LanguageContent>

<LanguageContent lang="python">

```python
import json
from typing import Any
from genkit import ai
from genkit.types import BaseEvalDataPoint, Score

# Assumes DeliciousnessResponse and DELICIOUSNESS_PROMPT are defined as in the previous step.

async def deliciousness_score(
    judge_llm: ai.Model,
    data_point: BaseEvalDataPoint,
) -> Score:
    """Score an individual test case for delciousness."""
    if not data_point.output:
        raise ValueError("Output is required for Deliciousness detection")

    prompt = DELICIOUSNESS_PROMPT.format(response_to_test=data_point.output)
    
    response = await ai.generate(
        model=judge_llm,
        prompt=prompt,
        output_format=ai.OutputFormat.JSON,
    )

    try:
        parsed_response = DeliciousnessResponse.model_validate_json(response.text)
    except Exception as e:
        raise ValueError(f"Unable to parse evaluator response: {response.text}") from e

    return Score(
        score=parsed_response.verdict,
        details={'reasoning': parsed_response.reason},
    )
```

</LanguageContent>

#### Define the evaluator action

The final step is to write a function that defines the evaluator action.

<LanguageContent lang="js">

```ts
import { EvaluatorAction } from 'genkit/evaluator';

/**
 * Create the Deliciousness evaluator action.
 */
export function createDeliciousnessEvaluator<ModelCustomOptions extends z.ZodTypeAny>(
  ai: Genkit,
  judge: ModelArgument<ModelCustomOptions>,
  judgeConfig?: z.infer<ModelCustomOptions>,
): EvaluatorAction {
  return ai.defineEvaluator(
    {
      name: `myCustomEvals/deliciousnessEvaluator`,
      displayName: 'Deliciousness',
      definition: 'Determines if output is considered delicous.',
      isBilled: true,
    },
    async (datapoint: BaseEvalDataPoint) => {
      const score = await deliciousnessScore(ai, judge, datapoint, judgeConfig);
      return {
        testCaseId: datapoint.testCaseId,
        evaluation: score,
      };
    },
  );
}
```

The `defineEvaluator` method is similar to other Genkit constructors like
`defineFlow` and `defineRetriever`. This method requires an `EvaluatorFn`
to be provided as a callback. The `EvaluatorFn` method accepts a
`BaseEvalDataPoint` object, which corresponds to a single entry in a
dataset under evaluation, along with an optional custom-options
parameter if specified. The function processes the datapoint and
returns an `EvalResponse` object.

The Zod Schemas for `BaseEvalDataPoint` and `EvalResponse` are
as follows.

##### `BaseEvalDataPoint`

```ts
export const BaseEvalDataPoint = z.object({
  testCaseId: z.string(),
  input: z.unknown(),
  output: z.unknown().optional(),
  context: z.array(z.unknown()).optional(),
  reference: z.unknown().optional(),
  testCaseId: z.string().optional(),
  traceIds: z.array(z.string()).optional(),
});

export const EvalResponse = z.object({
  sampleIndex: z.number().optional(),
  testCaseId: z.string(),
  traceId: z.string().optional(),
  spanId: z.string().optional(),
  evaluation: z.union([ScoreSchema, z.array(ScoreSchema)]),
});
```

##### `ScoreSchema`

```ts
const ScoreSchema = z.object({
  id: z.string().describe('Optional ID to differentiate multiple scores').optional(),
  score: z.union([z.number(), z.string(), z.boolean()]).optional(),
  error: z.string().optional(),
  details: z
    .object({
      reasoning: z.string().optional(),
    })
    .passthrough()
    .optional(),
});
```

</LanguageContent>

<LanguageContent lang="go">

```go
import (
    "github.com/firebase/genkit/go/ai"
    "github.com/firebase/genkit/go/core/api"
)

// Create the Deliciousness evaluator action
func createDeliciousnessEvaluator(
    r api.Registry,
    judgeLLM ai.Model,
) ai.Evaluator {
    return ai.DefineEvaluator(r, "myCustomEvals", "deliciousnessEvaluator",
        &ai.EvaluatorOptions{
            DisplayName: "Deliciousness",
            Definition:  "Determines if output is considered delicious",
            IsBilled:    true,
        },
        func(ctx context.Context, req *ai.EvaluatorCallbackRequest) (*ai.EvaluatorCallbackResponse, error) {
            // Score the example
            score, err := deliciousnessScore(ctx, judgeLLM, &req.Input)
            if err != nil {
                return nil, err
            }
            
            return &ai.EvaluatorCallbackResponse{
                TestCaseId: req.Input.TestCaseId,
                Evaluation: []ai.Score{*score},
            }, nil
        },
    )
}
```

The `DefineEvaluator` function is similar to other Genkit constructors like
`DefineFlow` and `DefineRetriever`. This function requires an `EvaluatorFunc`
to be provided as a callback. The `EvaluatorFunc` accepts an
`EvaluatorCallbackRequest` object containing an `Example` (a single entry in a
dataset under evaluation) and optional configuration. The function processes the
example and returns an `EvaluatorCallbackResponse` object.

The Go type definitions for the evaluator components are:

##### `Example`

```go
type Example struct {
    TestCaseId string   `json:"testCaseId,omitempty"`
    Input      any      `json:"input"`
    Output     any      `json:"output,omitempty"`
    Context    []any    `json:"context,omitempty"`
    Reference  any      `json:"reference,omitempty"`
    TraceIds   []string `json:"traceIds,omitempty"`
}
```

##### `Score`

```go
type Score struct {
    Id      string         `json:"id,omitempty"`
    Score   any            `json:"score,omitempty"`
    Status  string         `json:"status,omitempty"` // "UNKNOWN", "FAIL", or "PASS"
    Error   string         `json:"error,omitempty"`
    Details map[string]any `json:"details,omitempty"`
}
```

</LanguageContent>

<LanguageContent lang="python">

```python
from genkit import GenkitRegistry, ai
from genkit.types import BaseEvalDataPoint, EvalFnResponse, Score
from typing import Any

# Assumes deliciousness_score is defined as in the previous step.

def create_deliciousness_evaluator(
    registry: GenkitRegistry,
    judge_llm: ai.Model,
):
    async def eval_fn(datapoint: BaseEvalDataPoint, options: Any | None) -> EvalFnResponse:
        score = await deliciousness_score(judge_llm, datapoint)
        return EvalFnResponse(
            test_case_id=datapoint.test_case_id,
            evaluation=score,
        )

    return registry.define_evaluator(
        name="myCustomEvals/deliciousnessEvaluator",
        display_name="Deliciousness",
        definition="Determines if output is considered delicous.",
        is_billed=True,
        fn=eval_fn,
    )
```

The `define_evaluator` method is similar to other Genkit constructors like
`define_flow`. This method requires an `EvaluatorFn`
to be provided as a callback. The `EvaluatorFn` method accepts a
`BaseEvalDataPoint` object, which corresponds to a single entry in a
dataset under evaluation, along with an optional custom-options
parameter if specified. The function processes the datapoint and
returns an `EvalFnResponse` object.

The Pydantic models for `BaseEvalDataPoint` and `EvalFnResponse` are
as follows.

##### `BaseEvalDataPoint`

```python
class BaseEvalDataPoint(BaseModel):
    input: Any | None = None
    output: Any | None = None
    context: list | None = None
    reference: Any | None = None
    test_case_id: str | None = Field(None, alias='testCaseId')
    trace_ids: list[str] | None = Field(None, alias='traceIds')
```

##### `EvalFnResponse`

```python
class EvalFnResponse(BaseModel):
    sample_index: float | None = Field(None, alias='sampleIndex')
    test_case_id: str = Field(..., alias='testCaseId')
    trace_id: str | None = Field(None, alias='traceId')
    span_id: str | None = Field(None, alias='spanId')
    evaluation: Score | list[Score]
```

##### `Score`

```python
class Score(BaseModel):
    id: str | None = Field(None, description='Optional ID to differentiate different scores')
    score: float | str | bool | None = None
    status: EvalStatusEnum | None = None
    error: str | None = None
    details: Details | None = None
```

</LanguageContent>

The evaluator definition lets you provide a name, a user-readable
display name, and a definition for the evaluator. The display name and
definition are displayed along with evaluation results in the Dev UI.
It also has an optional `isBilled` field that marks whether this evaluator
can result in billing (e.g., it uses a billed LLM or API). If an evaluator
is billed, the UI prompts the user for a confirmation in the CLI before
allowing them to run an evaluation. This step helps guard against
unintended expenses.

### Heuristic Evaluators

A heuristic evaluator can be any function used to evaluate the `input`, `context`,
or `output` of your generative AI feature.

Heuristic evaluators in Genkit are made up of 2 components:

- A scoring function
- An evaluator action

#### Define the scoring function

As with the LLM-based evaluator, define the scoring function. In this case,
the scoring function does not need a judge LLM.

<LanguageContent lang="js">

```ts
import { BaseEvalDataPoint, Score } from 'genkit/evaluator';

const US_PHONE_REGEX = /[\+]?[(]?[0-9]{3}[)]?[-\s\.]?[0-9]{3}[-\s\.]?[0-9]{4}/i;

/**
 * Scores whether a datapoint output contains a US Phone number.
 */
export async function usPhoneRegexScore(dataPoint: BaseEvalDataPoint): Promise<Score> {
  const d = dataPoint;
  if (!d.output || typeof d.output !== 'string') {
    throw new Error('String output is required for regex matching');
  }
  const matches = US_PHONE_REGEX.test(d.output as string);
  const reasoning = matches ? `Output matched US_PHONE_REGEX` : `Output did not match US_PHONE_REGEX`;
  return {
    score: matches,
    details: { reasoning },
  };
}
```

</LanguageContent>

<LanguageContent lang="go">

```go
import (
    "errors"
    "regexp"
    
    "github.com/firebase/genkit/go/ai"
)

var usPhoneRegex = regexp.MustCompile(`[\+]?[(]?[0-9]{3}[)]?[-\s\.]?[0-9]{3}[-\s\.]?[0-9]{4}`)

// Scores whether a datapoint output contains a US Phone number
func usPhoneRegexScore(example *ai.Example) (*ai.Score, error) {
    if example.Output == nil {
        return nil, errors.New("output was not provided")
    }
    
    outputStr, ok := example.Output.(string)
    if !ok {
        return nil, errors.New("output must be a string")
    }
    
    matches := usPhoneRegex.MatchString(outputStr)
    status := ai.ScoreStatusFail
    if matches {
        status = ai.ScoreStatusPass
    }
    
    reasoning := "Output did not match US_PHONE_REGEX"
    if matches {
        reasoning = "Output matched US_PHONE_REGEX"
    }
    
    return &ai.Score{
        Score:  matches,
        Status: status.String(),
        Details: map[string]any{"reasoning": reasoning},
    }, nil
}
```

</LanguageContent>

<LanguageContent lang="python">

```python
import re
from genkit.types import BaseEvalDataPoint, Score, EvalStatusEnum

US_PHONE_REGEX = re.compile(r'[\+]?[(]?[0-9]{3}[)]?[-\s\.]?[0-9]{3}[-\s\.]?[0-9]{4}')

async def us_phone_regex_score(data_point: BaseEvalDataPoint) -> Score:
    """Scores whether a datapoint output contains a US Phone number."""
    if not data_point.output or not isinstance(data_point.output, str):
        raise ValueError('String output is required for regex matching')
    
    matches = bool(US_PHONE_REGEX.search(data_point.output))
    reasoning = "Output matched US_PHONE_REGEX" if matches else "Output did not match US_PHONE_REGEX"
    status = EvalStatusEnum.PASS_ if matches else EvalStatusEnum.FAIL
    
    return Score(
        score=matches,
        status=status,
        details={'reasoning': reasoning},
    )
```

</LanguageContent>

#### Define the evaluator action

<LanguageContent lang="js">

```ts
import { Genkit } from 'genkit';
import { BaseEvalDataPoint, EvaluatorAction } from 'genkit/evaluator';

/**
 * Configures a regex evaluator to match a US phone number.
 */
export function createUSPhoneRegexEvaluator(ai: Genkit): EvaluatorAction {
  return ai.defineEvaluator(
    {
      name: `myCustomEvals/usPhoneRegexEvaluator`,
      displayName: 'Regex Match for US PHONE NUMBER',
      definition: 'Uses Regex to check if output matches a US phone number',
      isBilled: false,
    },
    async (datapoint: BaseEvalDataPoint) => {
      const score = await usPhoneRegexScore(datapoint);
      return {
        testCaseId: datapoint.testCaseId,
        evaluation: score,
      };
    },
  );
}
```

</LanguageContent>

<LanguageContent lang="go">

```go
// Configures a regex evaluator to match a US phone number
func createUSPhoneRegexEvaluator(r api.Registry) ai.Evaluator {
    return ai.DefineEvaluator(r, "myCustomEvals", "usPhoneRegexEvaluator",
        &ai.EvaluatorOptions{
            DisplayName: "Regex Match for US PHONE NUMBER",
            Definition:  "Uses Regex to check if output matches a US phone number",
            IsBilled:    false,
        },
        func(ctx context.Context, req *ai.EvaluatorCallbackRequest) (*ai.EvaluatorCallbackResponse, error) {
            score, err := usPhoneRegexScore(&req.Input)
            if err != nil {
                return nil, err
            }
            
            return &ai.EvaluatorCallbackResponse{
                TestCaseId: req.Input.TestCaseId,
                Evaluation: []ai.Score{*score},
            }, nil
        },
    )
}
```

</LanguageContent>

<LanguageContent lang="python">

```python
from genkit import GenkitRegistry
from genkit.types import BaseEvalDataPoint, EvalFnResponse
from typing import Any

# Assumes us_phone_regex_score is defined as in the previous step.

def create_us_phone_regex_evaluator(registry: GenkitRegistry):
    async def eval_fn(datapoint: BaseEvalDataPoint, options: Any | None) -> EvalFnResponse:
        score = await us_phone_regex_score(datapoint)
        return EvalFnResponse(
            test_case_id=datapoint.test_case_id,
            evaluation=score,
        )

    return registry.define_evaluator(
        name="myCustomEvals/usPhoneRegexEvaluator",
        display_name="Regex Match for US PHONE NUMBER",
        definition="Uses Regex to check if output matches a US phone number",
        is_billed=False,
        fn=eval_fn,
    )
```

</LanguageContent>

## Putting it together

### Plugin definition

Plugins are registered with the framework by installing them at the time of
initializing Genkit.

<LanguageContent lang="js">

To define a new plugin in JavaScript, use the `genkitPlugin` helper
method to instantiate all Genkit actions within the plugin context.

This code sample shows two evaluators: the LLM-based deliciousness evaluator,
and the regex-based US phone number evaluator. Instantiating these
evaluators within the plugin context registers them with the plugin.

```ts
import { GenkitPlugin, genkitPlugin } from 'genkit/plugin';

export function myCustomEvals<ModelCustomOptions extends z.ZodTypeAny>(options: {
  judge: ModelArgument<ModelCustomOptions>;
  judgeConfig?: ModelCustomOptions;
}): GenkitPlugin {
  // Define the new plugin
  return genkitPlugin('myCustomEvals', async (ai: Genkit) => {
    const { judge, judgeConfig } = options;

    // The plugin instatiates our custom evaluators within the context
    // of the `ai` object, making them available
    // throughout our Genkit application.
    createDeliciousnessEvaluator(ai, judge, judgeConfig);
    createUSPhoneRegexEvaluator(ai);
  });
}
export default myCustomEvals;
```

</LanguageContent>

<LanguageContent lang="go">

To define a new plugin in Go, create a struct that implements the `Plugin` interface
with an `Init` method that registers your evaluators.

This code sample shows a plugin with both the LLM-based deliciousness evaluator
and the regex-based US phone number evaluator:

```go
import (
    "context"
    "sync"
    
    "github.com/firebase/genkit/go/ai"
    "github.com/firebase/genkit/go/core/api"
)

// MyCustomEvals is a Genkit plugin that provides custom evaluators
type MyCustomEvals struct {
    JudgeLLM ai.Model // The LLM to use for evaluation
    mu       sync.Mutex
    initted  bool
}

func (m *MyCustomEvals) Name() string {
    return "myCustomEvals"
}

// Init initializes the plugin and registers evaluators
func (m *MyCustomEvals) Init(ctx context.Context) []api.Action {
    m.mu.Lock()
    defer m.mu.Unlock()
    
    if m.initted {
        panic("myCustomEvals.Init already called")
    }
    if m.JudgeLLM == nil {
        panic("myCustomEvals: judge LLM must be provided")
    }
    m.initted = true
    
    r := api.LookupRegistry()
    
    var actions []api.Action
    // Register the deliciousness evaluator
    actions = append(actions, createDeliciousnessEvaluator(r, m.JudgeLLM).(api.Action))
    // Register the US phone regex evaluator
    actions = append(actions, createUSPhoneRegexEvaluator(r).(api.Action))
    
    return actions
}
```

</LanguageContent>

<LanguageContent lang="python">

To define a new plugin in Python, create a class that inherits from `Plugin`
and implements an `initialize` method that registers your evaluators.

This code sample shows a plugin with both the LLM-based deliciousness evaluator
and the regex-based US phone number evaluator:

```python
from genkit import GenkitRegistry, Plugin, ai

# Assumes create_deliciousness_evaluator and create_us_phone_regex_evaluator are defined.

class MyCustomEvals(Plugin):
    def __init__(self, judge_llm: ai.Model):
        self.judge_llm = judge_llm

    def initialize(self, registry: GenkitRegistry):
        create_deliciousness_evaluator(registry, self.judge_llm)
        create_us_phone_regex_evaluator(registry)

    @property
    def name(self) -> str:
        return "myCustomEvals"
```

</LanguageContent>

### Configure Genkit

Add the `myCustomEvals` plugin to your Genkit configuration.

<LanguageContent lang="js">

For evaluation with Gemini, disable safety settings so that the evaluator can
accept, detect, and score potentially harmful content.

```ts
import { googleAI } from '@genkit-ai/google-genai';

const ai = genkit({
  plugins: [
    vertexAI(),
    ...
    myCustomEvals({
      judge: googleAI.model("gemini-2.5-flash"),
    }),
  ],
  ...
});
```

</LanguageContent>

<LanguageContent lang="go">

```go
import (
    "context"
    
    "github.com/firebase/genkit/go/ai"
    "github.com/firebase/genkit/go/plugins/googleai"
)

func main() {
    ctx := context.Background()
    
    // Initialize the Google AI plugin
    googleai.Init(ctx, &googleai.Config{
        APIKey: "your-api-key", // Or use environment variable
    })
    
    // Get the judge LLM
    judgeLLM := googleai.Model("gemini-2.0-flash-latest")
    
    // Initialize Genkit with the custom evaluator plugin
    genkit.Init(ctx,
        &MyCustomEvals{JudgeLLM: judgeLLM},
        // ... other plugins
    )
}
```

</LanguageContent>

<LanguageContent lang="python">

```python
from genkit import genkit
from genkit.plugins import google_genai
from my_custom_evals_plugin import MyCustomEvals # Your plugin file

ai = genkit.Genkit(
    plugins=[
        google_genai.GoogleAI(),
        # ... other plugins
        MyCustomEvals(judge_llm=google_genai.Model("gemini-1.5-flash")),
    ],
    # ...
)
```

</LanguageContent>

## Using your custom evaluators

Once you instantiate your custom evaluators within the Genkit app context
(either through a plugin or directly), they are ready to be used. The following
example illustrates how to try out the deliciousness evaluator with a few sample
inputs and outputs.

1.  Create a json file `deliciousness_dataset.json` with the following content:

```json
[
  {
    "testCaseId": "delicous_mango",
    "input": "What is a super delicious fruit",
    "output": "A perfectly ripe mango â€“ sweet, juicy, and with a hint of tropical sunshine."
  },
  {
    "testCaseId": "disgusting_soggy_cereal",
    "input": "What is something that is tasty when fresh but less tasty after some time?",
    "output": "Stale, flavorless cereal that's been sitting in the box too long."
  }
]
```

2.  Use the Genkit CLI to run the evaluator against these test cases.

```bash
# Start your genkit runtime
genkit start -- <command to start your app>

genkit eval:run deliciousness_dataset.json --evaluators=myCustomEvals/deliciousnessEvaluator
```

3.  Navigate to `localhost:4000/evaluate` to view your results in the Genkit UI.

It is important to note that confidence in custom evaluators increases as
you benchmark them with standard datasets or approaches. Iterate on the
results of such benchmarks to improve your evaluators' performance until it
reaches the targeted level of quality.

</LanguageContent>
