---
title: Managing prompts with Dotprompt
description: Learn how to use Dotprompt to manage prompts, models, and parameters for generative AI models across JavaScript, Go, and Python, with a streamlined approach to prompt engineering and iteration.
---

import LanguageSelector from '../../../components/LanguageSelector.astro';
import CopyMarkdownButton from '../../../components/CopyMarkdownButton.astro';
import LanguageContent from '../../../components/LanguageContent.astro';

<div style="display: flex; justify-content: space-between; align-items: center; gap: 1rem; margin: 1rem 0 1rem 0;">
  <LanguageSelector supportedLanguages="js go python" />
  <CopyMarkdownButton />
</div>

<LanguageContent lang="js go python">
Prompt engineering is the primary way that you, as an app developer, influence
the output of generative AI models. For example, when using LLMs, you can craft
prompts that influence the tone, format, length, and other characteristics of
the models' responses.

The way you write these prompts will depend on the model you're using; a prompt
written for one model might not perform well when used with another model.
Similarly, the model parameters you set (temperature, top-k, and so on) will
also affect output differently depending on the model.

Getting all three of these factors&mdash;the model, the model parameters, and
the prompt&mdash;working together to produce the output you want is rarely a
trivial process and often involves substantial iteration and experimentation.
Genkit provides a library and file format called Dotprompt, that aims to make
this iteration faster and more convenient.

[Dotprompt](https://github.com/google/dotprompt) is designed around the premise
that **prompts are code**. You define your prompts along with the models and
model parameters they're intended for separately from your application code.
Then, you (or, perhaps someone not even involved with writing application code)
can rapidly iterate on the prompts and model parameters using the Genkit
Developer UI. Once your prompts are working the way you want, you can import
them into your application and run them using Genkit.

Your prompt definitions each go in a file with a `.prompt` extension. Here's an
example of what these files look like:

```dotprompt
---
model: googleai/gemini-2.5-flash
config:
  temperature: 0.9
input:
  schema:
    location: string
    style?: string
    name?: string
  default:
    location: a restaurant
---

You are the world's most welcoming AI assistant and are currently working at {{location}}.

Greet a guest{{#if name}} named {{name}}{{/if}}{{#if style}} in the style of {{style}}{{/if}}.
```

The portion in the triple-dashes is YAML front matter, similar to the front
matter format used by GitHub Markdown and Jekyll; the rest of the file is the
prompt, which can optionally use <a href="https://handlebarsjs.com/guide/" target="_blank" >Handlebars</a> templates. The following sections will go into more detail about each of
the parts that make a `.prompt` file and how to use them.

## Before you begin

Before reading this page, you should be familiar with the content covered on the
[Generating content with AI models](/docs/models) page.

If you want to run the code examples on this page, first complete the steps in
the Getting started guide for your language:

<LanguageContent lang="js">
  Complete the [Get started](/docs/get-started) guide. All examples assume you have already installed Genkit as a
  dependency in your project.
</LanguageContent>

<LanguageContent lang="go">
  Complete the [Get started](/docs/get-started) guide. All examples assume you have already installed Genkit as a
  dependency in your project.
</LanguageContent>

<LanguageContent lang="python">
  Complete the [Get started](/docs/get-started) guide. All examples assume you have already installed Genkit as a
  dependency in your project.
</LanguageContent>

## Creating prompt files

Although Dotprompt provides several [different ways](#defining-prompts-in-code) to create
and load prompts, it's optimized for projects that organize their prompts as
`.prompt` files within a single directory (or subdirectories thereof). This
section shows you how to create and load prompts using this recommended setup.

### Creating a prompt directory

<LanguageContent lang="js go python">
  The Dotprompt library expects to find your prompts in a directory at your project root and automatically loads any
  prompts it finds there. By default, this directory is named `prompts`. For example, using the default directory name,
  your project structure might look something like this:
</LanguageContent>

<LanguageContent lang="js">
  ```
  your-project/
  ├── lib/
  ├── node_modules/
  ├── prompts/
  │   └── hello.prompt
  ├── src/
  ├── package-lock.json
  ├── package.json
  └── tsconfig.json
  ```
</LanguageContent>

<LanguageContent lang="go">
  ```
  your-project/
  ├── prompts/
  │ └── hello.prompt
  ├── main.go
  ├── go.mod
  └── go.sum
  ```
</LanguageContent>

<LanguageContent lang="python">
  ```
  your-project/
  ├── prompts/
  │   └── hello.prompt
  ├── src/
  │   └── main.py
  ├── pyproject.toml
  └── requirements.txt
  ```
</LanguageContent>



<LanguageContent lang="js go python">
  If you want to use a different directory, you can specify it when you configure Genkit:
</LanguageContent>

<LanguageContent lang="js">
```ts
    const ai = genkit({
      promptDir: './llm_prompts',
      // (Other settings...)
    });
    ```
</LanguageContent>

<LanguageContent lang="go">
  ```go 
  g := genkit.Init(context.Background(), genkit.WithPromptDir("./llm_prompts"))
  ```
</LanguageContent>

<LanguageContent lang="python">
```python
ai = Genkit(
    plugins=[...],
    prompt_dir='./llm_prompts',
)
```
</LanguageContent>



### Creating a prompt file

There are two ways to create a `.prompt` file: using a text editor, or with the
developer UI.

#### Using a text editor

If you want to create a prompt file using a text editor, create a text file with
the `.prompt` extension in your prompts directory: for example,
`prompts/hello.prompt`.

Here is a minimal example of a prompt file:

```dotprompt
---
model: googleai/gemini-2.5-flash
---
You are the world's most welcoming AI assistant. Greet the user and offer your assistance.
```

The portion in the dashes is YAML front matter, similar to the front matter
format used by GitHub markdown and Jekyll; the rest of the file is the prompt,
which can optionally use Handlebars templates. The front matter section is
optional, but most prompt files will at least contain metadata specifying a
model. The remainder of this page shows you how to go beyond this, and make use
of Dotprompt's features in your prompt files.

#### Using the developer UI

<LanguageContent lang="js go python">
  You can also create a prompt file using the model runner in the developer UI. Start with application code that imports
  the Genkit library and configures it to use the model plugin you're interested in:
</LanguageContent>

<LanguageContent lang="js">
```ts
    import { genkit } from 'genkit';

    // Import the model plugins you want to use.
    import { googleAI } from '@genkit-ai/google-genai';

    const ai = genkit({
      // Initialize and configure the model plugins.
      plugins: [
        googleAI({
          apiKey: 'your-api-key', // Or (preferred): export GEMINI_API_KEY=...
        }),
      ],
    });
    ```

    It's okay if the file contains other code, but the above is all that's required.

    Load the developer UI in the same project:

    ```bash
    genkit start -- tsx --watch src/your-code.ts
    ```

</LanguageContent>

<LanguageContent lang="go">
```go
package main

import (
	"context"
	"log"

	"github.com/firebase/genkit/go/ai"
	"github.com/firebase/genkit/go/genkit"
	"github.com/firebase/genkit/go/plugins/googlegenai"
)

func main() {
	g := genkit.Init(context.Background(), genkit.WithPlugins(&googlegenai.GoogleAI{}))

	// Blocks end of program execution to use the developer UI.
	select {}
}
```

    Load the developer UI in the same project:

    ```bash
    genkit start -- go run .
    ```

</LanguageContent>

<LanguageContent lang="python">
```python
from genkit import Genkit
from genkit.plugins.google_genai import GoogleAI

ai = Genkit(
    plugins=[GoogleAI()],
)
```

    It's okay if the file contains other code, but the above is all that's required.

    Load the developer UI in the same project:

    ```bash
    genkit start -- uv run src/main.py
    ```

</LanguageContent>

In the Models section, choose the model you want to use from the list of models
provided by the plugin.

Then, experiment with the prompt and configuration until you get results you're
happy with. When you're ready, press the Export button and save the file to your
prompts directory.

## Running prompts

After you've created prompt files, you can run them from your application code,
or using the tooling provided by Genkit. Regardless of how you want to run your
prompts, first start with application code that imports the Genkit library and
the model plugins you're interested in.

If you're storing your prompts in a directory other than the default, be sure to
specify it when you configure Genkit.

### Run prompts from code

<LanguageContent lang="js">
To use a prompt, first load it using the `prompt('file_name')` method:

    ```ts
    const helloPrompt = ai.prompt('hello');
    ```

    Once loaded, you can call the prompt like a function:

    ```ts
    const response = await helloPrompt();

    // Alternatively, use destructuring assignments to get only the properties
    // you're interested in:
    const { text } = await helloPrompt();
    ```

    Or you can also run the prompt in streaming mode:

    ```ts
    const { response, stream } = helloPrompt.stream();

    for await (const chunk of stream) {
      console.log(chunk.text);
    }
    // optional final (aggregated) response
    console.log((await response).text);
    ```

    A callable prompt takes two optional parameters: the input to the prompt (see
    the section below on [specifying input schemas](#input-and-output-schemas)), and a configuration
    object, similar to that of the `generate()` method. For example:

    ```ts
    const response2 = await helloPrompt(
      // Prompt input:
      { name: 'Ted' },

      // Generation options:
      {
        config: {
          temperature: 0.4,
        },
      },
    );
    ```

    Similarly for streaming:

    ```ts
    const { stream } = helloPrompt.stream(input, options);
    ```

    Any parameters you pass to the prompt call will override the same parameters
    specified in the prompt file.

    See [Generate content with AI models](/docs/models) for descriptions of the available
    options.

</LanguageContent>

<LanguageContent lang="go">
To use a prompt, first load it using the `genkit.LookupPrompt()` function:

    ```go
    helloPrompt := genkit.LookupPrompt(g, "hello")
    ```

    An executable prompt has similar options to that of `genkit.Generate()` and many
    of them are overridable at execution time, including things like input (see the
    section about [specifying input schemas](#input-and-output-schemas)), configuration, and more:

    ```go
    resp, err := helloPrompt.Execute(context.Background(),
    	ai.WithModelName("googleai/gemini-2.5-flash"),
    	ai.WithInput(map[string]any{"name": "John"}),
    	ai.WithConfig(&genai.GenerateContentConfig{Temperature: genai.Ptr[float32](0.5)})
    )
    ```

    #### Streaming prompt execution

    You can stream prompt output using `ExecuteStream()`, which returns an iterator:

    ```go
    helloPrompt := genkit.LookupPrompt(g, "hello")
    stream := helloPrompt.ExecuteStream(ctx, ai.WithInput(map[string]any{"name": "John"}))

    for result, err := range stream {
    	if err != nil {
    		return "", err
    	}
    	if result.Done {
    		return result.Response.Text(), nil
    	}
    	// Process each chunk
    	fmt.Print(result.Chunk.Text())
    }
    ```

    #### Typed prompts with DataPrompt

    For strongly-typed input and output, use `genkit.LookupDataPrompt[In, Out]()` to
    wrap your prompt with Go type information. This provides compile-time type safety
    and a more idiomatic Go experience:

    ```go
    type GreetingInput struct {
    	Name string `json:"name"`
    }

    type Greeting struct {
    	Message string `json:"message"`
    	Tone    string `json:"tone"`
    }

    // Look up a .prompt file with type information
    helloPrompt := genkit.LookupDataPrompt[GreetingInput, *Greeting](g, "hello")

    // Execute with strongly-typed input, get strongly-typed output
    greeting, resp, err := helloPrompt.Execute(ctx, GreetingInput{Name: "John"})
    if err != nil {
    	log.Fatal(err)
    }
    log.Printf("Message: %s, Tone: %s\n", greeting.Message, greeting.Tone)
    ```

    The `.prompt` file should define an output schema that matches your Go type:

    ```dotprompt
    ---
    model: googleai/gemini-2.5-flash
    input:
      schema:
        name: string
    output:
      schema:
        message: string
        tone: string
    ---
    Greet {{name}} warmly.
    ```

    For streaming with typed prompts, use `ExecuteStream()` which provides typed chunks:

    ```go
    helloPrompt := genkit.LookupDataPrompt[GreetingInput, *Greeting](g, "hello")

    for result, err := range helloPrompt.ExecuteStream(ctx, GreetingInput{Name: "John"}) {
    	if err != nil {
    		return nil, err
    	}
    	if result.Done {
    		// result.Output is *Greeting
    		return result.Output, nil
    	}
    	// result.Chunk is also *Greeting (partial data as it streams)
    	if result.Chunk.Message != "" {
    		fmt.Println("Partial message:", result.Chunk.Message)
    	}
    }
    ```

    Any parameters you pass to the prompt call will override the same parameters
    specified in the prompt file.

    See [Generate content with AI models](/docs/models) for descriptions of the available
    options.

</LanguageContent>

<LanguageContent lang="python">
To use a prompt, first load it using the `prompt('file_name')` method:

    ```python
    hello_prompt = ai.prompt('hello')
    ```

    Once loaded, you can call the prompt like a function:

    ```python
    response = await hello_prompt()

    # Access the text output
    print(response.text)
    ```

    Or you can run the prompt in streaming mode:

    ```python
    result = hello_prompt.stream()

    async for chunk in result.stream:
        print(chunk.text)

    # optional final (aggregated) response
    final_response = await result.response
    print(final_response.text)
    ```

    A callable prompt takes optional parameters: the input to the prompt (see
    the section below on [specifying input schemas](#input-and-output-schemas)), and keyword arguments
    for generation options. For example:

    ```python
    response = await hello_prompt(
        # Prompt input:
        {"name": "Ted"},
        # Generation options:
        opts={"config": {"temperature": 0.4}},
    )
    ```

    Similarly for streaming:

    ```python
    result = hello_prompt.stream(
        {"name": "Ted"},
        opts={"config": {"temperature": 0.4}},
    )
    ```

    Any parameters you pass to the prompt call will override the same parameters
    specified in the prompt file.

    See [Generate content with AI models](/docs/models) for descriptions of the available
    options.

</LanguageContent>

### Using the developer UI

As you're refining your app's prompts, you can run them in the Genkit developer
UI to quickly iterate on prompts and model configurations, independently from
your application code.

<LanguageContent lang="js">
Load the developer UI from your project directory:

    ```bash
    genkit start -- tsx --watch src/your-code.ts
    ```

</LanguageContent>

<LanguageContent lang="go">
Load the developer UI from your project directory:

    ```bash
    genkit start -- go run .
    ```

</LanguageContent>

<LanguageContent lang="python">
Load the developer UI from your project directory:

    ```bash
    genkit start -- uv run src/main.py
    ```

</LanguageContent>

Once you've loaded prompts into the developer UI, you can run them with
different input values, and experiment with how changes to the prompt wording or
the configuration parameters affect the model output. When you're happy with the
result, you can click the **Export prompt** button to save the modified prompt
back into your project directory.

## Model configuration

In the front matter block of your prompt files, you can optionally specify model
configuration values for your prompt:

```dotprompt
---
model: googleai/gemini-2.5-flash
config:
  temperature: 1.4
  topK: 50
  topP: 0.4
  maxOutputTokens: 400
  stopSequences:
    - "<end>"
    - "<fin>"
---
```

These values map directly to the configuration parameters:

<LanguageContent lang="js">
```ts
    const response3 = await helloPrompt(
      {},
      {
        config: {
          temperature: 1.4,
          topK: 50,
          topP: 0.4,
          maxOutputTokens: 400,
          stopSequences: ['<end>', '<fin>'],
        },
      },
    );
    ```
</LanguageContent>

<LanguageContent lang="go">
```go
    resp, err := helloPrompt.Execute(context.Background(),
        ai.WithConfig(&genai.GenerateContentConfig{
            Temperature:     genai.Ptr[float32](1.4),
            TopK:            genai.Ptr[int32](50),
            TopP:            genai.Ptr[float32](0.4),
            MaxOutputTokens: genai.Ptr[int32](400),
            StopSequences:   []string{"<end>", "<fin>"},
        }))
    ```
</LanguageContent>

<LanguageContent lang="python">
```python
    response = await hello_prompt(
        {},
        opts={"config": {
            "temperature": 1.4,
            "top_k": 50,
            "top_p": 0.4,
            "max_output_tokens": 400,
            "stop_sequences": ["<end>", "<fin>"],
        }},
    )
    ```
</LanguageContent>

See [Generate content with AI models](/docs/models) for descriptions of the available
options.

## Input and output schemas

You can specify input and output schemas for your prompt by defining them in the
front matter section:

```dotprompt
---
model: googleai/gemini-2.5-flash
input:
  schema:
    theme?: string
  default:
    theme: "pirate"
output:
  schema:
    dishname: string
    description: string
    calories: integer
    allergens(array): string
---
Invent a menu item for a {{theme}} themed restaurant.
```

These schemas are used in much the same way as those passed to a `generate()`
request or a flow definition. For example, the prompt defined above produces
structured output:

<LanguageContent lang="js">
```ts
    const menuPrompt = ai.prompt('menu');
    const { output } = await menuPrompt({ theme: 'medieval' });

    const dishName = output['dishname'];
    const description = output['description'];
    ```

</LanguageContent>

<LanguageContent lang="go">
```go
    menuPrompt := genkit.LookupPrompt(g, "menu")
    if menuPrompt == nil {
        log.Fatal("no prompt named 'menu' found")
    }

    resp, err := menuPrompt.Execute(ctx,
        ai.WithInput(map[string]any{"theme": "medieval"}),
    )
    if err != nil {
        log.Fatal(err)
    }

    var output map[string]any
    if err := resp.Output(&output); err != nil {
        log.Fatal(err)
    }

    log.Println(output["dishname"])
    log.Println(output["description"])
    ```

</LanguageContent>

<LanguageContent lang="python">
```python
    menu_prompt = ai.prompt('menu')
    response = await menu_prompt({"theme": "medieval"})

    dish_name = response.output["dishname"]
    description = response.output["description"]
    ```

</LanguageContent>

<LanguageContent lang="python">
:::note[Accessing structured output in Python]
`response.output` can be either a dict or a typed Pydantic model:

- Use `response.output["field"]` when output comes from `.prompt` schema (Picoschema/JSON Schema).
- Use `response.output.field` when you pass `opts={'output': {'schema': YourPydanticModel}}`.
:::
</LanguageContent>

You have several options for defining schemas in a `.prompt` file: Dotprompt's
own schema definition format, Picoschema; standard JSON Schema; or, as
references to schemas defined in your application code. The following sections
describe each of these options in more detail.

### Picoschema

The schemas in the example above are defined in a format called Picoschema.
Picoschema is a compact, YAML-optimized schema definition format that makes it
easy to define the most important attributes of a schema for LLM usage. Here's a
longer example of a schema, which specifies the information an app might store
about an article:

```yaml
schema:
  title: string # string, number, and boolean types are defined like this
  subtitle?: string # optional fields are marked with a `?`
  draft?: boolean, true when in draft state
  status?(enum, approval status): [PENDING, APPROVED]
  date: string, the date of publication e.g. '2024-04-09' # descriptions follow a comma
  tags(array, relevant tags for article): string # arrays are denoted via parentheses
  authors(array):
    name: string
    email?: string
  metadata?(object): # objects are also denoted via parentheses
    updatedAt?: string, ISO timestamp of last update
    approvedBy?: integer, id of approver
  extra?: any, arbitrary extra data
  (*): string, wildcard field
```

The above schema is equivalent to the following type definitions:

<LanguageContent lang="js">
```ts
    interface Article {
      title: string;
      subtitle?: string | null;
      /** true when in draft state */
      draft?: boolean | null;
      /** approval status */
      status?: 'PENDING' | 'APPROVED' | null;
      /** the date of publication e.g. '2024-04-09' */
      date: string;
      /** relevant tags for article */
      tags: string[];
      authors: {
        name: string;
        email?: string | null;
      }[];
      metadata?: {
        /** ISO timestamp of last update */
        updatedAt?: string | null;
        /** id of approver */
        approvedBy?: number | null;
      } | null;
      /** arbitrary extra data */
      extra?: any;
      /** wildcard field */
      [key: string]: any;
    }
    ```
</LanguageContent>

<LanguageContent lang="go">
```go
    type Article struct {
        Title    string   `json:"title"`
        Subtitle string   `json:"subtitle,omitempty" jsonschema:"required=false"`
        Draft    bool     `json:"draft,omitempty"`  // True when in draft state
        Status   string   `json:"status,omitempty" jsonschema:"enum=PENDING,enum=APPROVED"` // Approval status
        Date     string   `json:"date"`   // The date of publication e.g. '2025-04-07'
        Tags     []string `json:"tags"`   // Relevant tags for article
        Authors  []struct {
            Name  string `json:"name"`
            Email string `json:"email,omitempty"`
        } `json:"authors"`
        Metadata struct {
            UpdatedAt  string `json:"updatedAt,omitempty"`  // ISO timestamp of last update
            ApprovedBy int    `json:"approvedBy,omitempty"` // ID of approver
        } `json:"metadata,omitempty"`
        Extra any `json:"extra"` // Arbitrary extra data
    }
    ```
</LanguageContent>

<LanguageContent lang="python">
```python
    from typing import Any, Literal, Optional
    from pydantic import BaseModel, Field

    class Author(BaseModel):
        name: str
        email: Optional[str] = None

    class Metadata(BaseModel):
        updated_at: Optional[str] = Field(None, description="ISO timestamp of last update")
        approved_by: Optional[int] = Field(None, description="id of approver")

    class Article(BaseModel):
        title: str
        subtitle: Optional[str] = None
        draft: Optional[bool] = Field(None, description="true when in draft state")
        status: Optional[Literal["PENDING", "APPROVED"]] = Field(None, description="approval status")
        date: str = Field(description="the date of publication e.g. '2024-04-09'")
        tags: list[str] = Field(description="relevant tags for article")
        authors: list[Author]
        metadata: Optional[Metadata] = None
        extra: Optional[Any] = Field(None, description="arbitrary extra data")
    ```
</LanguageContent>

Picoschema supports scalar types `string`, `integer`, `number`, `boolean`, and
`any`. Objects, arrays, and enums are denoted by a parenthetical after the field
name.

Objects defined by Picoschema have all properties required unless denoted
optional by `?`, and do not allow additional properties. When a property is
marked as optional, it is also made nullable to provide more leniency for LLMs
to return null instead of omitting a field.

In an object definition, the special key `(*)` can be used to declare a
"wildcard" field definition. This will match any additional properties not
supplied by an explicit key.

### JSON Schema

Picoschema does not support many of the capabilities of full JSON schema. If you
require more robust schemas, you may supply a JSON Schema instead:

```yaml
output:
  schema:
    type: object
    properties:
      field1:
        type: number
        minimum: 20
```

### Schema references defined in code

<LanguageContent lang="js">
In addition to directly defining schemas in the `.prompt` file, you can
    reference a schema registered with `defineSchema()` by name. If you're using
    TypeScript, this approach will let you take advantage of the language's static
    type checking features when you work with prompts.

    To register a schema using Zod:

    ```ts
    import { z } from 'genkit';

    const MenuItemSchema = ai.defineSchema(
      'MenuItemSchema',
      z.object({
        dishname: z.string(),
        description: z.string(),
        calories: z.coerce.number(),
        allergens: z.array(z.string()),
      }),
    );
    ```

    Within your prompt, provide the name of the registered schema:

    ```dotprompt
    ---
    model: googleai/gemini-2.5-flash-latest
    output:
      schema: MenuItemSchema
    ---
    ```

    The Dotprompt library will automatically resolve the name to the underlying
    registered schema. You can then utilize the schema to strongly type the
    output of a Dotprompt:

    ```ts
    const menuPrompt = ai.prompt<
      z.ZodTypeAny, // Input schema
      typeof MenuItemSchema, // Output schema
      z.ZodTypeAny // Custom options schema
    >('menu');
    const { output } = await menuPrompt({ theme: 'medieval' });

    // Now data is strongly typed as MenuItemSchema:
    const dishName = output?.dishname;
    const description = output?.description;
    ```

</LanguageContent>

<LanguageContent lang="go">
In addition to directly defining schemas in the `.prompt` file, you can
    reference a schema registered with `genkit.DefineSchemaFor[T]()` by name. This
    approach lets you define your types once in Go and reference them in prompt
    files, giving you compile-time type safety without duplicating schema definitions.

    To register a schema from a Go type:

    ```go
    type MenuItem struct {
    	Dishname    string   `json:"dishname" jsonschema:"description=The name of the menu item"`
    	Description string   `json:"description" jsonschema:"description=A description of the menu item"`
    	Calories    int      `json:"calories" jsonschema:"description=The estimated number of calories"`
    	Allergens   []string `json:"allergens" jsonschema:"description=Any known allergens in the menu item"`
    }

    // Register the schema - the name is inferred from the type name ("MenuItem")
    genkit.DefineSchemaFor[MenuItem](g)
    ```

    Within your prompt, reference the registered schema by name:

    ```dotprompt
    ---
    model: googleai/gemini-2.5-flash
    output:
      schema: MenuItem
    ---
    Invent a menu item for a pirate themed restaurant.
    ```

    The Dotprompt library will automatically resolve the name to the underlying
    registered schema. You can then use `LookupDataPrompt` to get strongly-typed
    access to the prompt:

    ```go
    // Input can be any type; here we use a simple struct
    type MenuRequest struct {
    	Theme string `json:"theme"`
    }

    // Register the input schema too
    genkit.DefineSchemaFor[MenuRequest](g)

    // Look up the prompt with type information
    menuPrompt := genkit.LookupDataPrompt[MenuRequest, *MenuItem](g, "menu")

    // Execute returns strongly-typed output
    item, resp, err := menuPrompt.Execute(ctx, MenuRequest{Theme: "pirate"})
    if err != nil {
    	log.Fatal(err)
    }

    // item is *MenuItem - fully typed
    log.Printf("Dish: %s, Calories: %d", item.Dishname, item.Calories)
    ```

    You can also reference registered schemas programmatically using `ai.WithOutputSchemaName()`:

    ```go
    genkit.DefineSchemaFor[MenuItem](g)

    resp, err := genkit.Generate(ctx, g,
    	ai.WithPrompt("Invent a menu item for a pirate themed restaurant."),
    	ai.WithOutputSchemaName("MenuItem"),
    )
    ```

    :::tip[Avoiding duplicate definitions]
    Registering schemas with `DefineSchemaFor` solves the common pain point of
    defining schemas twice—once in your `.prompt` file and again in your Go code
    for type safety. Define your types once in Go, register them, and reference
    them by name in your prompts.
    :::

</LanguageContent>

<LanguageContent lang="python">
In addition to directly defining schemas in the `.prompt` file, you can
    reference a Pydantic model using the `output` parameter when defining
    or calling the prompt. This approach lets you take advantage of Python's type
    checking features when you work with prompts.

    Define your schema as a Pydantic model:

    ```python
    from pydantic import BaseModel

    class MenuItem(BaseModel):
        dishname: str
        description: str
        calories: int
        allergens: list[str]
    ```

    Within your prompt, you can use Picoschema or JSON Schema as usual:

    ```dotprompt
    ---
    model: googleai/gemini-2.5-flash
    output:
      schema:
        dishname: string
        description: string
        calories: integer
        allergens(array): string
    ---
    Invent a menu item for a pirate themed restaurant.
    ```

    Then load and call the prompt, specifying the output schema for type safety:

    ```python
    menu_prompt = ai.prompt('menu')
    response = await menu_prompt(
        {"theme": "medieval"},
        opts={'output': {'schema': MenuItem}},
    )

    # output is now typed as MenuItem
    dish_name = response.output.dishname
    description = response.output.description
    ```

    You can also use `ai.define_prompt()` to define prompts programmatically with
    Pydantic schemas using the `Output` helper:

    ```python
    from pydantic import BaseModel
    from genkit import Input, Output

    class MenuInput(BaseModel):
        theme: str

    class MenuItem(BaseModel):
        dishname: str
        description: str
        calories: int
        allergens: list[str]

    menu_prompt = ai.define_prompt(
        name="menu",
        model="googleai/gemini-2.5-flash",
        input=Input(schema=MenuInput),
        output=Output(schema=MenuItem),
        prompt="Invent a menu item for a {{theme}} themed restaurant.",
    )

    response = await menu_prompt(MenuInput(theme="pirate"))
    # response.output is MenuItem
    ```

</LanguageContent>

## Tool calling

The Dotprompt frontmatter configuration also allows you to select which tools to enable at generate time. Tools are
supplied as a list of tool names that must correspond to tools that have been registered with the Genkit instance
executing the prompt:

```dotprompt
---
model: googleai/gemini-2.5-pro
tools: [search_flights, search_hotels]
input:
  schema:
    destination: string
---

Plan a trip to {{destination}}, using the available tools to find flights and hotels.
```

Tools can also be passed when calling a prompt programmatically:

<LanguageContent lang="js">
```ts
const myTool = ai.defineTool(...);
const myPrompt = ai.prompt('my_prompt');
myPrompt({inputArgs: 'go here'}, {tools: [myTool]})
```
</LanguageContent>

<LanguageContent lang="go">
```go
// outside runtime code
var myTool = genkit.DefineTool(g, ...)
var myPrompt = genkit.LookupPrompt("my_prompt")

// within runtime code
resp, err := myPrompt.Execute(ctx, ai.WithInput(...), ai.WithTools(myTool))
```
</LanguageContent>

<LanguageContent lang="python">
```python
@ai.tool()
def my_tool(...):
    ...

my_prompt = ai.prompt('my_prompt')
response = await my_prompt({"input_args": "go here"}, opts={"tools": ["my_tool"]})
```
</LanguageContent>

## Prompt templates

The portion of a `.prompt` file that follows the front matter (if present) is
the prompt itself, which will be passed to the model. While this prompt could be
a simple text string, very often you will want to incorporate user input into
the prompt. To do so, you can specify your prompt using the

<a href="https://handlebarsjs.com/guide/" target="_blank">
  Handlebars
</a>
templating language. Prompt templates can include placeholders that refer to the values defined by your prompt's input
schema.

You already saw this in action in the section on input and output schemas:

```dotprompt
---
model: googleai/gemini-2.5-flash
input:
  schema:
    theme?: string
  default:
    theme: "pirate"
output:
  schema:
    dishname: string
    description: string
    calories: integer
    allergens(array): string
---
Invent a menu item for a {{theme}} themed restaurant.
```

In this example, the Handlebars expression, `{{theme}}`,
resolves to the value of the input's `theme` property when you run the
prompt. To pass input to the prompt:

<LanguageContent lang="js">
```ts
    const menuPrompt = ai.prompt('menu');
    const { output } = await menuPrompt({ theme: 'medieval' });
    ```
</LanguageContent>

<LanguageContent lang="go">
```go
    menuPrompt := genkit.LookupPrompt(g, "menu")

    resp, err := menuPrompt.Execute(context.Background(),
        ai.WithInput(map[string]any{"theme": "medieval"}),
    )
    ```

</LanguageContent>

<LanguageContent lang="python">
```python
    menu_prompt = ai.prompt('menu')
    response = await menu_prompt({"theme": "medieval"})
    ```

</LanguageContent>

Note that because the input schema declared the `theme` property to be optional
and provided a default, you could have omitted the property,
and the prompt would have resolved using the default value.

Handlebars templates also support some limited logical constructs. For example,
as an alternative to providing a default, you could define the prompt using
Handlebars's `#if` helper:

```dotprompt
---
model: googleai/gemini-2.5-flash
input:
  schema:
    theme?: string
---
Invent a menu item for a {{#if theme}}{{theme}} themed{{/if}} restaurant.
```

In this example, the prompt renders as "Invent a menu item for a restaurant"
when the `theme` property is unspecified.

See the <a href="https://handlebarsjs.com/guide/builtin-helpers.html" target="_blank">Handlebars
documentation</a> for information on all of the built-in logical helpers.

In addition to properties defined by your input schema, your templates can also
refer to values automatically defined by Genkit. The next few sections describe
these automatically-defined values and how you can use them.

### Multi-message prompts

By default, Dotprompt constructs a single message with a "user" role.
However, some prompts are best expressed as a combination of multiple
messages, such as a system prompt.

The `{{role}}` helper provides a simple way to
construct multi-message prompts:

```dotprompt
---
model: googleai/gemini-2.5-flash
input:
  schema:
    userQuestion: string
---
{{role "system"}}
You are a helpful AI assistant that really loves to talk about food. Try to work
food items into all of your conversations.
{{role "user"}}
{{userQuestion}}
```

Note that your final prompt must contain at least one `user` role.

### Multi-modal prompts

For models that support multimodal input, such as images alongside text, you can
use the `{{media}}` helper:

```dotprompt
---
model: googleai/gemini-2.5-flash
input:
  schema:
    photoUrl: string
---
Describe this image in a detailed paragraph:

{{media url=photoUrl}}
```

The URL can be `https:` or base64-encoded `data:` URIs for "inline" image usage.
In code, this would be:

<LanguageContent lang="js">
```ts
    const multimodalPrompt = ai.prompt('multimodal');
    const { text } = await multimodalPrompt({
      photoUrl: 'https://example.com/photo.jpg',
    });
    ```
</LanguageContent>

<LanguageContent lang="go">
```go
    multimodalPrompt := genkit.LookupPrompt(g, "multimodal")

    resp, err := multimodalPrompt.Execute(context.Background(),
        ai.WithInput(map[string]any{"photoUrl": "https://example.com/photo.jpg"}),
    )
    ```

</LanguageContent>

<LanguageContent lang="python">
```python
    multimodal_prompt = ai.prompt('multimodal')
    response = await multimodal_prompt({
        "photoUrl": "https://example.com/photo.jpg",
    })
    print(response.text)
    ```

</LanguageContent>

See also [Multimodal input](/docs/models#multimodal-input), on the Generating content
page, for an example of constructing a `data:` URL.

### Partials

Partials are reusable templates that can be included inside any prompt. Partials
can be especially helpful for related prompts that share common behavior.

When loading a prompt directory, any file prefixed with an underscore (`_`) is
considered a partial. So a file `_personality.prompt` might contain:

```dotprompt
You should speak like a {{#if style}}{{style}}{{else}}helpful assistant.{{/if}}.
```

This can then be included in other prompts:

```dotprompt
---
model: googleai/gemini-2.5-flash
input:
  schema:
    name: string
    style?: string
---

{{role "system"}}
{{>personality style=style}}

{{role "user"}}
Give the user a friendly greeting.

User's Name: {{name}}
```

Partials are inserted using the
`{{>NAME_OF_PARTIAL args...}}`
syntax. If no arguments are provided to the partial, it executes with the same
context as the parent prompt.

Partials accept both named arguments as above or a single positional argument
representing the context. This can be helpful for tasks such as rendering
members of a list.

**\_destination.prompt**

```dotprompt
- {{name}} ({{country}})
```

**chooseDestination.prompt**

```dotprompt
---
model: googleai/gemini-2.5-flash
input:
  schema:
    destinations(array):
      name: string
      country: string
---
Help the user decide between these vacation destinations:

{{#each destinations}}
{{>destination this}}
{{/each}}
```

#### Defining partials in code

You can also define partials in code:

<LanguageContent lang="js">
```ts
    ai.definePartial('personality', 'Talk like a {{#if style}}{{style}}{{else}}helpful assistant{{/if}}.');
    ```

    Code-defined partials are available in all prompts.

</LanguageContent>

<LanguageContent lang="go">
```go
    genkit.DefinePartial(g, "personality", "Talk like a {{#if style}}{{style}}{{else}}helpful assistant{{/if}}.")
    ```

    Code-defined partials are available in all prompts.

</LanguageContent>

<LanguageContent lang="python">
```python
    ai.define_partial('personality', 'Talk like a {{#if style}}{{style}}{{else}}helpful assistant{{/if}}.')
    ```

    Code-defined partials are available in all prompts.

</LanguageContent>

### Defining Custom Helpers

You can define custom helpers to process and manage data inside of a prompt.
Helpers are registered globally:

<LanguageContent lang="js">
```ts
    ai.defineHelper('shout', (text: string) => text.toUpperCase());
    ```

    Once a helper is defined you can use it in any prompt:

    ```dotprompt
    ---
    model: googleai/gemini-2.5-flash
    input:
      schema:
        name: string
    ---

    HELLO, {{shout name}}!!!
    ```

</LanguageContent>

<LanguageContent lang="go">
```go
    genkit.DefineHelper(g, "shout", func(input string) string {
        return strings.ToUpper(input)
    })
    ```

    Once a helper is defined you can use it in any prompt:

    ```dotprompt
    ---
    model: googleai/gemini-2.5-flash
    input:
      schema:
        name: string
    ---

    HELLO, {{shout name}}!!!
    ```

</LanguageContent>

<LanguageContent lang="python">
```python
    ai.define_helper('shout', lambda text: text.upper())
    ```

    Once a helper is defined you can use it in any prompt:

    ```dotprompt
    ---
    model: googleai/gemini-2.5-flash
    input:
      schema:
        name: string
    ---

    HELLO, {{shout name}}!!!
    ```

</LanguageContent>

## Prompt variants

Because prompt files are just text, you can (and should!) commit them to your
version control system, allowing you to compare changes over time easily. Often,
tweaked versions of prompts can only be fully tested in a production environment
side-by-side with existing versions. Dotprompt supports this through its
variants feature.

To create a variant, create a `[name].[variant].prompt` file. For instance, if
you were using Gemini 2.0 Flash in your prompt but wanted to see if Gemini 2.5
Pro would perform better, you might create two files:

- `my_prompt.prompt`: the "baseline" prompt
- `my_prompt.gemini25pro.prompt`: a variant named `gemini25pro`

To use a prompt variant:

<LanguageContent lang="js">
Specify the variant option when loading:

    ```ts
    const myPrompt = ai.prompt('my_prompt', { variant: 'gemini25pro' });
    ```

</LanguageContent>

<LanguageContent lang="go">
Specify the variant in the prompt name when loading:

    ```go
    myPrompt := genkit.LookupPrompt(g, "my_prompt.gemini25pro")
    ```

</LanguageContent>

<LanguageContent lang="python">
Specify the variant option when loading:

    ```python
    my_prompt = ai.prompt('my_prompt', variant='gemini25pro')
    ```

</LanguageContent>

The name of the variant is included in the metadata of generation traces, so you
can compare and contrast actual performance between variants in the Genkit trace
inspector.

## Defining prompts in code

All of the examples discussed so far have assumed that your prompts are defined
in individual `.prompt` files in a single directory (or subdirectories thereof),
accessible to your app at runtime. Dotprompt is designed around this setup, and
its authors consider it to be the best developer experience overall.

However, if you have use cases that are not well supported by this setup,
you can also define prompts in code:

<LanguageContent lang="js">
Use the `definePrompt()` function. The first parameter is analogous to the front matter block of a
    `.prompt` file; the second parameter can either be a Handlebars template string,
    as in a prompt file, or a function that returns a `GenerateRequest`:

    ```ts
    const myPrompt = ai.definePrompt({
      name: 'myPrompt',
      model: 'googleai/gemini-2.5-flash',
      input: {
        schema: z.object({
          name: z.string(),
        }),
      },
      prompt: 'Hello, {{name}}. How are you today?',
    });
    ```

    ```ts
    const myPrompt = ai.definePrompt({
      name: 'myPrompt',
      model: 'googleai/gemini-2.5-flash',
      input: {
        schema: z.object({
          name: z.string(),
        }),
      },
      messages: async (input) => {
        return [
          {
            role: 'user',
            content: [{ text: `Hello, ${input.name}. How are you today?` }],
          },
        ];
      },
    });
    ```

</LanguageContent>

<LanguageContent lang="go">
For strongly-typed prompts defined in code, use `genkit.DefineDataPrompt[In, Out]()`.
    This is the recommended approach as it provides compile-time type safety for both
    input and output, making your code more idiomatic and less error-prone:

    ```go
    type GeoQuery struct {
        CountryCount int `json:"countryCount" jsonschema:"default=10"`
    }

    type CountryList struct {
        Countries []string `json:"countries" jsonschema:"description=List of country names"`
    }

    geographyPrompt := genkit.DefineDataPrompt[GeoQuery, *CountryList](
        g, "GeographyPrompt",
        ai.WithModel(googlegenai.GoogleAIModelRef("gemini-2.5-flash", nil)),
        ai.WithSystem("You are a geography teacher. Respond only when the user asks about geography."),
        ai.WithPrompt("Give me the {{countryCount}} biggest countries in the world by inhabitants."),
    )

    // Execute returns strongly-typed output directly
    list, resp, err := geographyPrompt.Execute(ctx, GeoQuery{CountryCount: 15})
    if err != nil {
        log.Fatal(err)
    }

    log.Printf("Countries: %v", list.Countries)
    ```

    With `DefineDataPrompt`, the input and output schemas are automatically inferred
    from the Go type parameters, and the output is returned directly as the typed value.

    For streaming with typed prompts, use `ExecuteStream()`:

    ```go
    for result, err := range geographyPrompt.ExecuteStream(ctx, GeoQuery{CountryCount: 15}) {
        if err != nil {
            log.Fatal(err)
        }
        if result.Done {
            log.Printf("Final countries: %v", result.Output.Countries)
            break
        }
        // Stream partial results as they arrive
        if len(result.Chunk.Countries) > 0 {
            log.Printf("Got %d countries so far...", len(result.Chunk.Countries))
        }
    }
    ```

    #### Using DefinePrompt (untyped)

    For cases where you need more flexibility or dynamic typing, you can use the
    untyped `genkit.DefinePrompt()` function:

    ```go
    geographyPrompt := genkit.DefinePrompt(
        g, "GeographyPrompt",
        ai.WithSystem("You are a geography teacher. Respond only when the user asks about geography."),
        ai.WithPrompt("Give me the {{countryCount}} biggest countries in the world by inhabitants."),
        ai.WithConfig(&genai.GenerateContentConfig{Temperature: genai.Ptr[float32](0.5)}),
        ai.WithInputType(GeoQuery{CountryCount: 10}), // Defaults to 10.
        ai.WithOutputType(CountryList{}),
    )

    resp, err := geographyPrompt.Execute(context.Background(), ai.WithInput(GeoQuery{CountryCount: 15}))
    if err != nil {
        log.Fatal(err)
    }

    var list CountryList
    if err := resp.Output(&list); err != nil {
        log.Fatal(err)
    }

    log.Printf("Countries: %v", list.Countries)
    ```

    #### Rendering prompts

    Prompts may also be rendered into a `GenerateActionOptions` which may then be
    processed and passed into `genkit.GenerateWithRequest()`:

    ```go
    actionOpts, err := geographyPrompt.Render(ctx, GeoQuery{CountryCount: 15})
    if err != nil {
        log.Fatal(err)
    }

    // Do something with the value...
    actionOpts.Config = &genai.GenerateContentConfig{Temperature: genai.Ptr[float32](0.8)}

    resp, err := genkit.GenerateWithRequest(ctx, g, actionOpts, nil, nil) // No middleware or streaming
    ```

    Note that all prompt options carry over to `GenerateActionOptions` with the
    exception of `WithMiddleware()`, which must be passed separately if using
    `Prompt.Render()` instead of `Prompt.Execute()`.

</LanguageContent>

<LanguageContent lang="python">
Use the `define_prompt()` function. You can specify a Handlebars template string
    with the `prompt` parameter, or use structured messages:

    ```python
    from pydantic import BaseModel
    from genkit import Input

    class GreetingInput(BaseModel):
        name: str

    my_prompt = ai.define_prompt(
        name='myPrompt',
        model='googleai/gemini-2.5-flash',
        input=Input(schema=GreetingInput),
        prompt='Hello, {{name}}. How are you today?',
    )

    response = await my_prompt(GreetingInput(name='Alice'))
    print(response.text)
    ```

    You can also define prompts with structured output using Pydantic models:

    ```python
    from pydantic import BaseModel
    from genkit import Input, Output

    class CountryList(BaseModel):
        countries: list[str]

    class GeoQuery(BaseModel):
        country_count: int = 10

    geography_prompt = ai.define_prompt(
        name='GeographyPrompt',
        model='googleai/gemini-2.5-flash',
        input=Input(schema=GeoQuery),
        output=Output(schema=CountryList),
        system='You are a geography teacher. Respond only when the user asks about geography.',
        prompt='Give me the {{country_count}} biggest countries in the world by inhabitants.',
    )

    response = await geography_prompt(GeoQuery(country_count=15))
    # response.output is CountryList
    print(response.output.countries)
    ```

    For streaming:

    ```python
    result = geography_prompt.stream(GeoQuery(country_count=15))

    async for chunk in result.stream:
        print(chunk.text)

    final_response = await result.response
    print(final_response.output.countries)
    ```

    You can also define prompts with a system message and multi-turn setup:

    ```python
    my_prompt = ai.define_prompt(
        name='chatPrompt',
        model='googleai/gemini-2.5-flash',
        input=Input(schema=GreetingInput),
        system='You are a helpful assistant.',
        prompt='Hello, {{name}}!',
    )
    ```

</LanguageContent>

## Next steps

- Learn about [tool calling](/docs/tool-calling) to give your prompts access to external functions and APIs
- Explore [retrieval-augmented generation (RAG)](/docs/rag) to incorporate external knowledge into your prompts
- See [creating flows](/docs/flows) to build complex AI workflows using your prompts
- Check out the [evaluation guide](/docs/evaluation) for testing and improving your prompt performance

</LanguageContent>
