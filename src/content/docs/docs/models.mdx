---
title: Generating content with AI models
description: Learn how to generate content with AI models using Genkit's unified interface across JavaScript, Go, and Python, covering basic usage, configuration, structured output, streaming, and multimodal input/output.
---

import LLMSummary from '@/components/llm-summary.astro';
import ExampleLink from '@/components/ExampleLink.astro';
import LanguageSelector from '../../../components/LanguageSelector.astro';
import CopyMarkdownButton from '../../../components/CopyMarkdownButton.astro';
import LanguageContent from '../../../components/LanguageContent.astro';

<div style="display: flex; justify-content: space-between; align-items: center; gap: 1rem; margin: 1rem 0 1rem 0;">
  <LanguageSelector />
  <CopyMarkdownButton />
</div>

<LanguageContent lang="js">

<LLMSummary>
Genkit provides a unified interface to interact with various generative AI models (LLMs, image generation).

**Core Function:** `ai.generate()`

**Basic Usage:**

```typescript
import { googleAI } from '@genkit-ai/google-genai';
import { genkit } from 'genkit';

const ai = genkit({
  plugins: [googleAI()],
  model: googleAI.model('gemini-2.5-flash'), // Default model
});

// Generate with default model
const response1 = await ai.generate('prompt text');
console.log(response1.text);

// Generate with specific model reference
import { googleAI } from '@genkit-ai/google-genai';
const response2 = await ai.generate({
  model: googleAI.model('gemini-2.5-flash'),
  prompt: 'prompt text',
});
console.log(response2.text);

// Generate with model string ID
const response3 = await ai.generate({
  model: 'googleai/gemini-2.5-flash',
  prompt: 'prompt text',
});
console.log(response3.text);
```

**Configuration:**

- **System Prompt:** `system: "Instruction for the model"`
- **Model Parameters:** `config: { maxOutputTokens: 512, temperature: 1.0, topP: 0.95, topK: 40, stopSequences: ["\n"] }`

**Structured Output (using Zod):**

```typescript
import { z } from 'genkit';

const MenuItemSchema = z.object({
  name: z.string().describe('The name of the menu item.'),
  description: z.string().describe('A description of the menu item.'),
  calories: z.number().describe('The estimated number of calories.'),
  allergens: z.array(z.string()).describe('Any known allergens in the menu item.'),
});

const response = await ai.generate({
  prompt: 'Suggest a menu item.',
  output: { schema: MenuItemSchema },
});

const menuItem = response.output; // Typed output, might be null if validation fails
if (menuItem) {
  console.log(menuItem.name);
}
```

**Streaming:**

```typescript
const { stream, response } = ai.generateStream({
  prompt: 'Tell a story.',
  // Can also include output schema for streaming structured data
  // output: { schema: z.array(MenuItemSchema) },
});

// Stream text chunks
for await (const chunk of stream) {
  console.log(chunk.text); // For structured: chunk.output (accumulated)
}

// Get final complete response
const finalResponse = await response;
console.log(finalResponse.text); // For structured: finalResponse.output
```

**Multimodal Input:**

```typescript
import { readFile } from 'node:fs/promises';

// From URL
const response1 = await ai.generate({
  prompt: [{ media: { url: 'https://.../image.jpg' } }, { text: 'Describe this image.' }],
});

// From local file (data URL)
const data = await readFile('image.jpg');
const response2 = await ai.generate({
  prompt: [{ media: { url: `data:image/jpeg;base64,${data.toString('base64')}` } }, { text: 'Describe this image.' }],
});
```

**Media Generation (e.g., Images):**

```typescript
import { vertexAI } from '@genkit-ai/vertexai'; // Example image model
import parseDataURL from 'data-urls';
import { writeFile } from 'node:fs/promises';

const response = await ai.generate({
  model: vertexAI.model('imagen-3.0-fast-generate-001'),
  prompt: 'Image description',
  output: { format: 'media' }, // Request media output
});

if (response?.media?.url) {
  // URL is typically a data: URL
  const parsed = parseDataURL(response.media.url);
  if (parsed) {
    await writeFile('output.png', parsed.body);
  }
}
```

**Supported Model Plugins (Examples):**

- Vertex AI (`@genkit-ai/vertexai`): Gemini, Imagen, Claude on Vertex
- Google AI (`@genkit-ai/google-genai`): Gemini
- OpenAI (`@genkit-ai/compat-oai/openai`): GPT, Dall-E, Whisper on OpenAI
- xAI (`@genkit-ai/compat-oai/xai`): Grok on xAI
- DeepSeek (`@genkit-ai/compat-oai/deepseek`): DeepSeek Chat, Reasoner on DeepSeek
- Ollama (`@genkit-ai/ollama`): Llama 3, Gemma 2, etc. (self-hosted)
- Community: Anthropic, Azure OpenAI, Cohere, Mistral, Groq

**Key Concepts:**

- **Flexibility:** Easily swap models (`model` parameter).
- **Zod:** For defining and validating structured output schemas.
- **Streaming:** For real-time output using `generateStream`.
- **Multimodality:** Handle text, image, video, audio inputs (model-dependent).
- **Media Generation:** Create images, etc. (model-dependent).

</LLMSummary>

At the heart of generative AI are AI _models_. Currently, the two most prominent
examples of generative models are large language models (LLMs) and image
generation models. These models take input, called a _prompt_ (most commonly
text, an image, or a combination of both), and from it produce as output text,
an image, or even audio or video.

The output of these models can be surprisingly convincing: LLMs generate text
that appears as though it could have been written by a human being, and image
generation models can produce images that are very close to real photographs or
artwork created by humans.

In addition, LLMs have proven capable of tasks beyond simple text generation:

- Writing computer programs
- Planning subtasks that are required to complete a larger task
- Organizing unorganized data
- Understanding and extracting information data from a corpus of text
- Following and performing automated activities based on a text description of
  the activity

There are many models available to you, from several different providers. Each
model has its own strengths and weaknesses and one model might excel at one task
but perform less well at others. Apps making use of generative AI can often
benefit from using multiple different models depending on the task at hand.

As an app developer, you typically don't interact with generative AI
models directly, but rather through services available as web APIs.
Although these services often have similar functionality, they all provide them
through different and incompatible APIs. If you want to make use of multiple
model services, you have to use each of their proprietary SDKs, potentially
incompatible with each other. And if you want to upgrade from one model to the
newest and most capable one, you might have to build that integration all over
again.

Genkit addresses this challenge by providing a single interface that abstracts
away the details of accessing potentially any generative AI model service, with
several pre-built implementations already available. Building your AI-powered
app around Genkit simplifies the process of making your first generative AI call
and makes it equally easy to combine multiple models or swap one model for
another as new models emerge.

### Before you begin

If you want to run the code examples on this page, first complete the steps in
the [Getting started](/docs/get-started) guide. All of the examples assume that you
have already installed Genkit as a dependency in your project.

### Models supported by Genkit

Genkit is designed to be flexible enough to use potentially any generative AI
model service. Its core libraries define the common interface for working with
models, and model plugins define the implementation details for working with a
specific model and its API.

The Genkit team maintains plugins for working with models provided by Vertex AI,
Google Generative AI, and Ollama:

- Gemini family of LLMs, through the
  [Google Cloud Vertex AI plugin](/docs/integrations/vertex-ai)
- Gemini family of LLMs, through the [Google AI plugin](/docs/integrations/google-genai)
- Imagen2 and Imagen3 image generation models, through Google Cloud Vertex AI
- Anthropic's Claude 3 family of LLMs, through Google Cloud Vertex AI's model
  garden
- Gemma 2, Llama 3, and many more open models, through the [Ollama
  plugin](/docs/integrations/ollama) (you must host the Ollama server yourself)
- GPT, Dall-E and Whisper family of models, through the [OpenAI plugin](/docs/integrations/openai)
- Grok family of models, through the [xAI plugin](/docs/integrations/xai)
- DeepSeek Chat and DeepSeek Reasoner models, through the [DeepSeek plugin](/docs/integrations/deepseek)

In addition, there are also several community-supported plugins that provide
interfaces to these models:

- Claude 3 family of LLMs, through the [Anthropic plugin](https://bloomlabsinc.github.io/genkit-plugins/js/plugins/genkitx-anthropic)
- GPT family of LLMs through the [Azure OpenAI plugin](https://bloomlabsinc.github.io/genkit-plugins/js/plugins/genkitx-azure-openai)
- Command R family of LLMs through the [Cohere plugin](https://bloomlabsinc.github.io/genkit-plugins/js/plugins/genkitx-cohere)
- Mistral family of LLMs through the [Mistral plugin](https://bloomlabsinc.github.io/genkit-plugins/js/plugins/genkitx-mistral)
- Gemma 2, Llama 3, and many more open models hosted on Groq, through the
  [Groq plugin](https://bloomlabsinc.github.io/genkit-plugins/js/plugins/genkitx-groq)

You can discover more by searching for [packages tagged with `genkit-model` on
npmjs.org](https://www.npmjs.com/search?q=keywords%3Agenkit-model).

### Loading and configuring model plugins

Before you can use Genkit to start generating content, you need to load and
configure a model plugin. If you're coming from the Getting Started guide,
you've already done this. Otherwise, see the [Getting Started](/docs/get-started)
guide or the individual plugin's documentation and follow the steps there before
continuing.

### The generate() method

In Genkit, the primary interface through which you interact with generative AI
models is the `generate()` method.

The simplest `generate()` call specifies the model you want to use and a text
prompt:

```ts
import { googleAI } from '@genkit-ai/google-genai';
import { genkit } from 'genkit';

const ai = genkit({
  plugins: [googleAI()],
  // Optional. Specify a default model.
  model: googleAI.model('gemini-2.5-flash'),
});

async function run() {
  const response = await ai.generate('Invent a menu item for a restaurant with a pirate theme.');
  console.log(response.text);
}

run();
```

When you run this brief example, it will print out some debugging information
followed by the output of the `generate()` call, which will usually be Markdown
text as in the following example:

```md
## The Blackheart's Bounty

**A hearty stew of slow-cooked beef, spiced with rum and molasses, served in a
hollowed-out cannonball with a side of crusty bread and a dollop of tangy
pineapple salsa.**

**Description:** This dish is a tribute to the hearty meals enjoyed by pirates
on the high seas. The beef is tender and flavorful, infused with the warm spices
of rum and molasses. The pineapple salsa adds a touch of sweetness and acidity,
balancing the richness of the stew. The cannonball serving vessel adds a fun and
thematic touch, making this dish a perfect choice for any pirate-themed
adventure.
```

Run the script again and you'll get a different output.

The preceding code sample sent the generation request to the default model,
which you specified when you configured the Genkit instance.

You can also specify a model for a single `generate()` call:

```ts
import { googleAI } from '@genkit-ai/google-genai';

const response = await ai.generate({
  model: googleAI.model('gemini-2.5-flash'),
  prompt: 'Invent a menu item for a restaurant with a pirate theme.',
});
```

This example uses a model reference function provided by the model plugin. Model
references carry static type information about the model and its options which
can be useful for code completion in the IDE and at compile time. Many plugins
use this pattern, but not all, so in cases where they don't, refer to the plugin
documentation for their preferred way to create function references.

Sometimes you may see code samples where model references are imported as
constants:

```ts
import { googleAI, gemini20Flash } from '@genkit-ai/google-genai';

const ai = genkit({
  plugins: [googleAI()],
  model: gemini20Flash,
});
```

Some plugins may still use this pattern. For plugins that switched to the new
syntax those constants are still there and continue to work, but new constants
for new future models may not to be added in the future.

Another option is to specify the model using a string identifier. This way will
work for all plugins regardless of how they chose to handle typed model
references, however you won't have the help of static type checking:

```ts
const response = await ai.generate({
  model: 'googleai/gemini-2.5-flash-001',
  prompt: 'Invent a menu item for a restaurant with a pirate theme.',
});
```

A model string identifier looks like `providerid/modelid`, where the provider ID
(in this case, `googleai`) identifies the plugin, and the model ID is a
plugin-specific string identifier for a specific version of a model.

Some model plugins, such as the Ollama plugin, provide access to potentially
dozens of different models and therefore do not export individual model
references. In these cases, you can only specify a model to `generate()` using
its string identifier.

These examples also illustrate an important point: when you use
`generate()` to make generative AI model calls, changing the model you want to
use is simply a matter of passing a different value to the model parameter. By
using `generate()` instead of the native model SDKs, you give yourself the
flexibility to more easily use several different models in your app and change
models in the future.

So far you have only seen examples of the simplest `generate()` calls. However,
`generate()` also provides an interface for more advanced interactions with
generative models, which you will see in the sections that follow.

### System prompts

Some models support providing a _system prompt_, which gives the model
instructions as to how you want it to respond to messages from the user. You can
use the system prompt to specify a persona you want the model to adopt, the tone
of its responses, the format of its responses, and so on.

If the model you're using supports system prompts, you can provide one with the
`system` parameter:

```ts
const response = await ai.generate({
  prompt: 'What is your quest?',
  system: "You are a knight from Monty Python's Flying Circus.",
});
```

### Multi-turn conversations with messages

For multi-turn conversations, you can use the `messages` parameter instead of `prompt` to provide a conversation history. This is particularly useful when you need to maintain context across multiple interactions with the model.

The `messages` parameter accepts an array of message objects, where each message has a `role` (one of `'system'`, `'user'`, `'model'`, or `'tool'`) and `content`:

```ts
const response = await ai.generate({
  messages: [
    { role: 'user', content: 'Hello, can you help me plan a trip?' },
    { role: 'model', content: "Of course! I'd be happy to help you plan a trip. Where are you thinking of going?" },
    { role: 'user', content: 'I want to visit Japan for two weeks in spring.' },
  ],
});
```

You can also combine `messages` with other parameters like `system` prompts:

```ts
const response = await ai.generate({
  system: 'You are a helpful travel assistant.',
  messages: [{ role: 'user', content: 'What should I pack for Japan in spring?' }],
});
```

**When to use `messages` vs. Chat API:**

- Use the `messages` parameter for simple multi-turn conversations where you manually manage the conversation history
- For persistent chat sessions with automatic history management, use the [Chat API](/docs/chat) instead

### Model parameters

The `generate()` function takes a `config` parameter, through which you can
specify optional settings that control how the model generates content:

```ts
const response = await ai.generate({
  prompt: 'Invent a menu item for a restaurant with a pirate theme.',
  config: {
    maxOutputTokens: 512,
    stopSequences: ['\n'],
    temperature: 1.0,
    topP: 0.95,
    topK: 40,
  },
});
```

The exact parameters that are supported depend on the individual model and model
API. However, the parameters in the previous example are common to almost every
model. The following is an explanation of these parameters:

#### Parameters that control output length

**maxOutputTokens**

LLMs operate on units called _tokens_. A token usually, but does not
necessarily, map to a specific sequence of characters. When you pass a prompt to
a model, one of the first steps it takes is to _tokenize_ your prompt string
into a sequence of tokens. Then, the LLM generates a sequence of tokens from the
tokenized input. Finally, the sequence of tokens gets converted back into text,
which is your output.

The maximum output tokens parameter simply sets a limit on how many tokens to
generate using the LLM. Every model potentially uses a different tokenizer, but
a good rule of thumb is to consider a single English word to be made of 2 to 4
tokens.

As stated earlier, some tokens might not map to character sequences. One such
example is that there is often a token that indicates the end of the sequence:
when an LLM generates this token, it stops generating more. Therefore, it's
possible and often the case that an LLM generates fewer tokens than the maximum
because it generated the "stop" token.

**stopSequences**

You can use this parameter to set the tokens or token sequences that, when
generated, indicate the end of LLM output. The correct values to use here
generally depend on how the model was trained, and are usually set by the model
plugin. However, if you have prompted the model to generate another stop
sequence, you might specify it here.

Note that you are specifying character sequences, and not tokens per se. In most
cases, you will specify a character sequence that the model's tokenizer maps to
a single token.

#### Parameters that control "creativity"

The _temperature_, _top-p_, and _top-k_ parameters together control how
"creative" you want the model to be. Below are very brief explanations of what
these parameters mean, but the more important point to take away is this: these
parameters are used to adjust the character of an LLM's output. The optimal
values for them depend on your goals and preferences, and are likely to be found
only through experimentation.

**temperature**

LLMs are fundamentally token-predicting machines. For a given sequence of tokens
(such as the prompt) an LLM predicts, for each token in its vocabulary, the
likelihood that the token comes next in the sequence. The temperature is a
scaling factor by which these predictions are divided before being normalized to
a probability between 0 and 1.

Low temperature values&mdash;between 0.0 and 1.0&mdash;amplify the difference in
likelihoods between tokens, with the result that the model will be even less
likely to produce a token it already evaluated to be unlikely. This is often
perceived as output that is less creative. Although 0.0 is technically not a
valid value, many models treat it as indicating that the model should behave
deterministically, and to only consider the single most likely token.

High temperature values&mdash;those greater than 1.0&mdash;compress the
differences in likelihoods between tokens, with the result that the model
becomes more likely to produce tokens it had previously evaluated to be
unlikely. This is often perceived as output that is more creative. Some model
APIs impose a maximum temperature, often 2.0.

**topP**

_Top-p_ is a value between 0.0 and 1.0 that controls the number of possible
tokens you want the model to consider, by specifying the cumulative probability
of the tokens. For example, a value of 1.0 means to consider every possible
token (but still take into account the probability of each token). A value of
0.4 means to only consider the most likely tokens, whose probabilities add up to
0.4, and to exclude the remaining tokens from consideration.

**topK**

_Top-k_ is an integer value that also controls the number of possible tokens you
want the model to consider, but this time by explicitly specifying the maximum
number of tokens. Specifying a value of 1 means that the model should behave
deterministically.

#### Experiment with model parameters

You can experiment with the effect of these parameters on the output generated
by different model and prompt combinations by using the Developer UI. Start the
developer UI with the `genkit start` command and it will automatically load all
of the models defined by the plugins configured in your project. You can quickly
try different prompts and configuration values without having to repeatedly make
these changes in code.

### Structured output

<ExampleLink
  title="Structured Output"
  description="View a live example of using structured output to generate a D&D character sheet."
  example="structured-output"
/>

When using generative AI as a component in your application, you often want
output in a format other than plain text. Even if you're just generating content
to display to the user, you can benefit from structured output simply for the
purpose of presenting it more attractively to the user. But for more advanced
applications of generative AI, such as programmatic use of the model's output,
or feeding the output of one model into another, structured output is a must.

In Genkit, you can request structured output from a model by specifying a schema
when you call `generate()`:

```ts
import { z } from 'genkit';
```

```ts
const MenuItemSchema = z.object({
  name: z.string().describe('The name of the menu item.'),
  description: z.string().describe('A description of the menu item.'),
  calories: z.number().describe('The estimated number of calories.'),
  allergens: z.array(z.string()).describe('Any known allergens in the menu item.'),
});

const response = await ai.generate({
  prompt: 'Suggest a menu item for a pirate-themed restaurant.',
  output: { schema: MenuItemSchema },
});
```

Model output schemas are specified using the [Zod](https://zod.dev/)
library. In addition to a schema definition language, Zod also provides runtime
type checking, which bridges the gap between static TypeScript types and the
unpredictable output of generative AI models. Zod lets you write code that can
rely on the fact that a successful generate call will always return output that
conforms to your TypeScript types.

When you specify a schema in `generate()`, Genkit does several things behind the
scenes:

- Augments the prompt with additional guidance about the desired output format.
  This also has the side effect of specifying to the model what content exactly
  you want to generate (for example, not only suggest a menu item but also
  generate a description, a list of allergens, and so on).
- Parses the model output into a JavaScript object.
- Verifies that the output conforms with the schema.

To get structured output from a successful generate call, use the response
object's `output` property:

```ts
const menuItem = response.output; // Typed as z.infer<typeof MenuItemSchema>
console.log(menuItem?.name);
```

#### Handling errors

Note in the prior example that the `output` property can be `null`. This can
happen when the model fails to generate output that conforms to the schema.
The best strategy for dealing with such errors will depend on your exact use
case, but here are some general hints:

- **Try a different model**. For structured output to succeed, the model must be
  capable of generating output in JSON. The most powerful LLMs, like Gemini and
  Claude, are versatile enough to do this; however, smaller models, such as some
  of the local models you would use with Ollama, might not be able to generate
  structured output reliably unless they have been specifically trained to do
  so.

- **Make use of Zod's coercion abilities**: You can specify in your schemas that
  Zod should try to coerce non-conforming types into the type specified by the
  schema. If your schema includes primitive types other than strings, using Zod
  coercion can reduce the number of `generate()` failures you experience. The
  following version of `MenuItemSchema` uses type coercion to automatically
  correct situations where the model generates calorie information as a string
  instead of a number:

  ```ts
  const MenuItemSchema = z.object({
    name: z.string().describe('The name of the menu item.'),
    description: z.string().describe('A description of the menu item.'),
    calories: z.coerce.number().describe('The estimated number of calories.'),
    allergens: z.array(z.string()).describe('Any known allergens in the menu item.'),
  });
  ```

- **Retry the generate() call**. If the model you've chosen only rarely fails to
  generate conformant output, you can treat the error as you would treat a
  network error, and simply retry the request using some kind of incremental
  back-off strategy.

### Streaming

When generating large amounts of text, you can improve the experience for your
users by presenting the output as it's generated&mdash;streaming the output. A
familiar example of streaming in action can be seen in most LLM chat apps: users
can read the model's response to their message as it's being generated, which
improves the perceived responsiveness of the application and enhances the
illusion of chatting with an intelligent counterpart.

In Genkit, you can stream output using the `generateStream()` method. Its
syntax is similar to the `generate()` method:

```ts
const { stream, response } = ai.generateStream({
  prompt: 'Tell me a story about a boy and his dog.',
});
```

The response object has a `stream` property, which you can use to iterate over
the streaming output of the request as it's generated:

```ts
for await (const chunk of stream) {
  console.log(chunk.text);
}
```

You can also get the complete output of the request, as you can with a
non-streaming request:

```ts
const finalResponse = await response;
console.log(finalResponse.text);
```

Streaming also works with structured output:

```ts
const { stream, response } = ai.generateStream({
  prompt: 'Suggest three pirate-themed menu items.',
  output: { schema: z.array(MenuItemSchema) },
});

for await (const chunk of stream) {
  console.log(chunk.output);
}

const finalResponse = await response;
console.log(finalResponse.output);
```

Streaming structured output works a little differently from streaming text: the
`output` property of a response chunk is an object constructed from the
accumulation of the chunks that have been produced so far, rather than an object
representing a single chunk (which might not be valid on its own). **Every chunk
of structured output in a sense supersedes the chunk that came before it**.

For example, here's what the first five outputs from the prior example might
look like:

```js
null;

{
  starters: [{}];
}

{
  starters: [{ name: "Captain's Treasure Chest", description: 'A' }];
}

{
  starters: [
    {
      name: "Captain's Treasure Chest",
      description: 'A mix of spiced nuts, olives, and marinated cheese served in a treasure chest.',
      calories: 350,
    },
  ];
}

{
  starters: [
    {
      name: "Captain's Treasure Chest",
      description: 'A mix of spiced nuts, olives, and marinated cheese served in a treasure chest.',
      calories: 350,
      allergens: [Array],
    },
    { name: 'Shipwreck Salad', description: 'Fresh' },
  ];
}
```

### Multimodal input

<ExampleLink
  title="Image Analysis"
  description="See a live demo of how Genkit can enable image analysis using multimodal input."
  example="image-analysis"
/>

The examples you've seen so far have used text strings as model prompts. While
this remains the most common way to prompt generative AI models, many models can
also accept other media as prompts. Media prompts are most often used in
conjunction with text prompts that instruct the model to perform some operation
on the media, such as to caption an image or transcribe an audio recording.

The ability to accept media input and the types of media you can use are
completely dependent on the model and its API. For example, the Gemini 1.5
series of models can accept images, video, and audio as prompts.

To provide a media prompt to a model that supports it, instead of passing a
simple text prompt to `generate`, pass an array consisting of a media part and a
text part:

```ts
const response = await ai.generate({
  prompt: [{ media: { url: 'https://.../image.jpg' } }, { text: 'What is in this image?' }],
});
```

In the above example, you specified an image using a publicly-accessible HTTPS
URL. You can also pass media data directly by encoding it as a data URL. For
example:

```ts
import { readFile } from 'node:fs/promises';
```

```ts
const data = await readFile('image.jpg');
const response = await ai.generate({
  prompt: [{ media: { url: `data:image/jpeg;base64,${data.toString('base64')}` } }, { text: 'What is in this image?' }],
});
```

All models that support media input support both data URLs and HTTPS URLs. Some
model plugins add support for other media sources. For example, the Vertex AI
plugin also lets you use Cloud Storage (`gs://`) URLs.

### Generating Media

While most examples in this guide focus on generating text with LLMs, Genkit also supports generating other types of media, including **images** and **audio**. Thanks to its unified `generate()` interface, working with media models is just as straightforward as generating text.

:::note
Genkit returns generated media as a **data URL**, a widely supported format for handling binary media in both browsers and Node.js environments.
:::

#### Image Generation

To generate an image using a model like Imagen from Vertex AI, follow these steps:

1. **Install a data URL parser.** Genkit outputs media as data URLs, so you'll need to decode them before saving to disk. This example uses [`data-urls`](https://www.npmjs.com/package/data-urls):

   ```bash
   npm install data-urls
   npm install --save-dev @types/data-urls
   ```

2. **Generate the image and save it to a file:**

   ```ts
   import { vertexAI } from '@genkit-ai/vertexai';
   import parseDataURL from 'data-urls';
   import { writeFile } from 'node:fs/promises';

   const response = await ai.generate({
     model: vertexAI.model('imagen-3.0-fast-generate-001'),
     prompt: 'An illustration of a dog wearing a space suit, photorealistic',
     output: { format: 'media' },
   });

   if (response?.media?.url) {
     const parsed = parseDataURL(response.media.url);
     if (parsed) {
       await writeFile('dog.png', parsed.body);
     }
   }
   ```

This will generate an image and save it as a PNG file named `dog.png`.

#### Audio Generation

You can also use Genkit to generate audio with a text-to-speech (TTS) models. This is especially useful for voice features, narration, or accessibility support.

Hereâ€™s how to convert text into speech and save it as an audio file:

```ts
import { googleAI } from '@genkit-ai/google-genai';
import { writeFile } from 'node:fs/promises';
import { Buffer } from 'node:buffer';

const response = await ai.generate({
  model: googleAI.model('gemini-2.5-flash-preview-tts'),

  // Gemini-specific configuration for audio generation
  // Available configuration options will depend on model and provider
  config: {
    responseModalities: ['AUDIO'],
    speechConfig: {
      voiceConfig: {
        prebuiltVoiceConfig: { voiceName: 'Algenib' },
      },
    },
  },
  prompt: 'Say that Genkit is an amazing AI framework',
});

// Handle the audio data (returned as a data URL)
if (response.media?.url) {
  // Extract base64 data from the data URL
  const audioBuffer = Buffer.from(response.media.url.substring(response.media.url.indexOf(',') + 1), 'base64');

  // Save to a file
  await writeFile('output.wav', audioBuffer);
}
```

This code generates speech using the Gemini TTS model and saves the result to a file named `output.wav`.

<LanguageContent lang="js">

### Middleware

Genkit allows you to use middleware to modify the behavior of `generate()` calls. Middleware can be used for various purposes, such as retrying failed requests or falling back to different models.

#### Retry Middleware

The `retry` middleware automatically retries failed `generate()` calls based on configurable options. This is useful for handling transient errors like network issues or temporary service unavailability.

```ts
import { googleAI } from '@genkit-ai/google-genai';
import { genkit } from 'genkit';
import { retry } from 'genkit/model/middleware'; // Import retry middleware

const ai = genkit({
  plugins: [googleAI()],
  model: googleAI.model('gemini-2.5-pro'),
});

async function run() {
  const { text } = await ai.generate({
    model: googleAI.model('gemini-2.5-pro'),
    prompt: 'You are a helpful AI assistant named Walt, say hello',
    use: [
      retry({
        maxRetries: 2,
        initialDelayMs: 1000,
        backoffFactor: 2,
      }),
    ],
  });
  console.log(text);
}

run();
```

**Configuration Options:**

- `maxRetries`: The maximum number of times to retry a failed request (default: 3).
- `statuses`: An array of `StatusName` values that should trigger a retry (default: `['UNAVAILABLE', 'DEADLINE_EXCEEDED', 'RESOURCE_EXHAUSTED', 'ABORTED', 'INTERNAL']`).
- `initialDelayMs`: The initial delay between retries in milliseconds (default: 1000).
- `maxDelayMs`: The maximum delay between retries in milliseconds (default: 60000).
- `backoffFactor`: The factor by which the delay increases after each retry (exponential backoff, default: 2).
- `noJitter`: Whether to disable jitter on the delay (default: false).
- `onError`: A callback to be executed on each retry attempt.

#### Fallback Middleware

The `fallback` middleware allows you to specify a list of alternative models to try if the primary model fails with certain error statuses. This enhances the robustness of your application by providing graceful degradation.

```ts
import { googleAI } from '@genkit-ai/google-genai';
import { genkit } from 'genkit';
import { fallback } from 'genkit/model/middleware'; // Import fallback middleware

const ai = genkit({
  plugins: [googleAI()],
  model: googleAI.model('gemini-2.5-pro'),
});

async function run() {
  const { text } = await ai.generate({
    model: googleAI.model('gemini-2.5-pro'),
    prompt: 'You are a helpful AI assistant named Walt, say hello',
    use: [
      fallback(ai, {
        models: [googleAI.model('gemini-2.5-flash')],
        statuses: ['RESOURCE_EXHAUSTED'],
      }),
    ],
  });
  console.log(text);
}

run();
```

**Configuration Options:**

- `models`: An array of `ModelArgument` values representing the fallback models to try in order.
- `statuses`: An array of `StatusName` values that should trigger a fallback (default: `['UNAVAILABLE', 'DEADLINE_EXCEEDED', 'RESOURCE_EXHAUSTED', 'ABORTED', 'INTERNAL', 'NOT_FOUND', 'UNIMPLEMENTED']`).
- `onError`: A callback to be executed on each fallback attempt.

#### Model Safety

To protect your application from prompt injection, malicious inputs, and sensitive data leakage, you can use safety middleware.

For exampple, the [Google Cloud Plugin](/docs/integrations/google-cloud#model-armor) provides integration with **Model Armor**, which sanitizes both user prompts and model responses using configurable filters (e.g., for PII, jailbreak attempts, and malicious URIs).

#### Custom Middleware

You can also implement your own custom middleware to extend Genkit's functionality. A middleware function takes the current `GenerateRequest` and a `next` function as arguments. It can modify the request, call `next` to pass the request to the next middleware or the model, and then process the response.

```ts
import { ModelMiddleware } from 'genkit/model';

function customLoggerMiddleware(): ModelMiddleware {
  return async (req, next) => {
    console.log('Request:', JSON.stringify(req));
    const resp = await next(req);
    console.log('Response:', JSON.stringify(resp));
    return resp;
  };
}

// To use it:
const { text } = await ai.generate({
  model: googleAI.model('gemini-2.5-pro'),
  prompt: 'Hello',
  use: [customLoggerMiddleware()],
});
```

</LanguageContent>

### Next steps

#### Learn more about Genkit

- As an app developer, the primary way you influence the output of generative AI
  models is through prompting. Read [Prompt management](/docs/dotprompt) to learn how
  Genkit helps you develop effective prompts and manage them in your codebase.
- Although `generate()` is the nucleus of every generative AI powered
  application, real-world applications usually require additional work before
  and after invoking a generative AI model. To reflect this, Genkit introduces
  the concept of _flows_, which are defined like functions but add additional
  features such as observability and simplified deployment. To learn more, see
  [Defining workflows](/docs/flows).

#### Advanced LLM use

- Many of your users will have interacted with large language models for the first time through chatbots. Although LLMs are capable of much more than simulating conversations, it remains a familiar and useful style of interaction. Even when your users will not be interacting directly with the model in this way, the conversational style of prompting is a powerful way to influence the output generated by an AI model. Read [Multi-turn chats](/docs/chat) to learn how to use Genkit as part of an LLM chat implementation.
- One way to enhance the capabilities of LLMs is to prompt them with a list of
  ways they can request more information from you, or request you to perform
  some action. This is known as _tool calling_ or _function calling_. Models
  that are trained to support this capability can respond to a prompt with a
  specially-formatted response, which indicates to the calling application that
  it should perform some action and send the result back to the LLM along with
  the original prompt. Genkit has library functions that automate both the
  prompt generation and the call-response loop elements of a tool calling
  implementation. See [Tool calling](/docs/tool-calling) to learn more.
- Retrieval-augmented generation (RAG) is a technique used to introduce
  domain-specific information into a model's output. This is accomplished by
  inserting relevant information into a prompt before passing it on to the
  language model. A complete RAG implementation requires you to bring several
  technologies together: text embedding generation models, vector databases, and
  large language models. See [Retrieval-augmented generation (RAG)](/docs/rag) to
  learn how Genkit simplifies the process of coordinating these various
  elements.

#### Testing model output

As a software engineer, you're used to deterministic systems where the same
input always produces the same output. However, with AI models being
probabilistic, the output can vary based on subtle nuances in the input, the
model's training data, and even randomness deliberately introduced by parameters
like temperature.

Genkit's evaluators are structured ways to assess the quality of your LLM's
responses, using a variety of strategies. Read more on the
[Evaluation](/docs/evaluation) page.

</LanguageContent>

<LanguageContent lang="go">

At the heart of generative AI are AI _models_. The two most prominent
examples of generative models are large language models (LLMs) and image
generation models. These models take input, called a _prompt_ (most commonly
text, an image, or a combination of both), and from it produce as output text,
an image, or even audio or video.

The output of these models can be surprisingly convincing: LLMs generate text
that appears as though it could have been written by a human being, and image
generation models can produce images that are very close to real photographs or
artwork created by humans.

In addition, LLMs have proven capable of tasks beyond simple text generation:

- Writing computer programs.
- Planning subtasks that are required to complete a larger task.
- Organizing unorganized data.
- Understanding and extracting information data from a corpus of text.
- Following and performing automated activities based on a text description of
  the activity.

There are many models available to you, from several different providers. Each
model has its own strengths and weaknesses and one model might excel at one task
but perform less well at others. Apps making use of generative AI can often
benefit from using multiple different models depending on the task at hand.

As an app developer, you typically don't interact with generative AI models
directly, but rather through services available as web APIs. Although these
services often have similar functionality, they all provide them through
different and incompatible APIs. If you want to make use of multiple model
services, you have to use each of their proprietary SDKs, potentially
incompatible with each other. And if you want to upgrade from one model to the
newest and most capable one, you might have to build that integration all over
again.

Genkit addresses this challenge by providing a single interface that abstracts
away the details of accessing potentially any generative AI model service, with
several prebuilt implementations already available. Building your AI-powered
app around Genkit simplifies the process of making your first generative AI call
and makes it equally straightforward to combine multiple models or swap one
model for another as new models emerge.

### Before you begin

If you want to run the code examples on this page, first complete the steps in
the [Get started](/docs/get-started) guide. All of the examples assume that you
have already installed Genkit as a dependency in your project.

### Models supported by Genkit

Genkit is designed to be flexible enough to use potentially any generative AI
model service. Its core libraries define the common interface for working with
models, and model plugins define the implementation details for working with a
specific model and its API.

The Genkit team maintains plugins for working with models provided by Vertex AI,
Google Generative AI, and Ollama:

- Gemini family of LLMs, through the
  [Google GenAI plugin](/docs/integrations/google-genai).
- Gemma 3, Llama 4, and many more open models, through the
  [Ollama plugin](/docs/integrations/ollama)
  (you must host the Ollama server yourself).

### Loading and configuring model plugins

Before you can use Genkit to start generating content, you need to load and
configure a model plugin. If you're coming from the Get Started guide,
you've already done this. Otherwise, see the [Get Started](/docs/get-started)
guide or the individual plugin's documentation and follow the steps there before
continuing.

### The `genkit.Generate()` function

In Genkit, the primary interface through which you interact with generative AI
models is the `genkit.Generate()` function.

The simplest `genkit.Generate()` call specifies the model you want to use and a
text prompt:

```go
package main

import (
	"context"
	"log"

	"github.com/firebase/genkit/go/ai"
	"github.com/firebase/genkit/go/genkit"
	"github.com/firebase/genkit/go/plugins/googlegenai"
)

func main() {
	ctx := context.Background()

	g := genkit.Init(ctx,
		genkit.WithPlugins(&googlegenai.GoogleAI{}),
		genkit.WithDefaultModel("googleai/gemini-2.5-flash"),
	)

	resp, err := genkit.Generate(ctx, g,
		ai.WithPrompt("Invent a menu item for a pirate themed restaurant."),
	)
	if err != nil {
		log.Fatalf("could not generate model response: %v", err)
	}

	log.Println(resp.Text())
}
```

:::tip[Convenience function]
For simple text generation, you can use `genkit.GenerateText()` which returns
just the text string directly:

```go
text, err := genkit.GenerateText(ctx, g,
	ai.WithPrompt("Invent a menu item for a pirate themed restaurant."),
)
```
:::

When you run this brief example, it will print out some debugging information
followed by the output of the `genkit.Generate()` call, which will usually be
Markdown text as in the following example:

```md
## The Blackheart's Bounty

**A hearty stew of slow-cooked beef, spiced with rum and molasses, served in a
hollowed-out cannonball with a side of crusty bread and a dollop of tangy
pineapple salsa.**

**Description:** This dish is a tribute to the hearty meals enjoyed by pirates
on the high seas. The beef is tender and flavorful, infused with the warm spices
of rum and molasses. The pineapple salsa adds a touch of sweetness and acidity,
balancing the richness of the stew. The cannonball serving vessel adds a fun and
thematic touch, making this dish a perfect choice for any pirate-themed
adventure.
```

Run the script again and you'll get a different output.

The preceding code sample sent the generation request to the default model,
which you specified when you configured the Genkit instance.

You can also specify a model for a single `genkit.Generate()` call:

```go
resp, err := genkit.Generate(ctx, g,
	ai.WithModelName("googleai/gemini-2.5-pro"),
	ai.WithPrompt("Invent a menu item for a pirate themed restaurant."),
)
```

A model string identifier looks like `providerid/modelid`, where the provider ID
(in this case, `googleai`) identifies the plugin, and the model ID is a
plugin-specific string identifier for a specific version of a model.

These examples also illustrate an important point: when you use
`genkit.Generate()` to make generative AI model calls, changing the model you
want to use is a matter of passing a different value to the model
parameter. By using `genkit.Generate()` instead of the native model SDKs, you
give yourself the flexibility to more easily use several different models in
your app and change models in the future.

So far you have only seen examples of the simplest `genkit.Generate()` calls.
However, `genkit.Generate()` also provides an interface for more advanced
interactions with generative models, which you will see in the sections that
follow.

### System prompts

Some models support providing a _system prompt_, which gives the model
instructions as to how you want it to respond to messages from the user. You can
use the system prompt to specify characteristics such as a persona you want the
model to adopt, the tone of its responses, and the format of its responses.

If the model you're using supports system prompts, you can provide one with the
`ai.WithSystem()` option:

```go
resp, err := genkit.Generate(ctx, g,
	ai.WithSystem("You are a food industry marketing consultant."),
	ai.WithPrompt("Invent a menu item for a pirate themed restaurant."),
)
```

For models that don't support system prompts, `ai.WithSystem()` simulates it by
modifying the request to appear _like_ a system prompt.

### Model parameters

The `genkit.Generate()` function takes a `ai.WithConfig()` option, through which
you can specify optional settings that control how the model generates content:

```go
resp, err := genkit.Generate(ctx, g,
	ai.WithModelName("googleai/gemini-2.5-flash"),
	ai.WithPrompt("Invent a menu item for a pirate themed restaurant."),
	ai.WithConfig(&genai.GenerateContentConfig{
		MaxOutputTokens: genai.Ptr[int32](500),
		StopSequences:   []string{"<end>", "<fin>"},
		Temperature:     genai.Ptr[float32](0.5),
		TopP:            genai.Ptr[float32](0.4),
		TopK:            genai.Ptr[int32](50),
	}),
)
```

The exact parameters that are supported depend on the individual model and model
API. However, the parameters in the previous example are common to almost every
model. The following is an explanation of these parameters:

#### Parameters that control output length

**MaxOutputTokens**

LLMs operate on units called _tokens_. A token usually, but does not
necessarily, map to a specific sequence of characters. When you pass a prompt to
a model, one of the first steps it takes is to _tokenize_ your prompt string
into a sequence of tokens. Then, the LLM generates a sequence of tokens from the
tokenized input. Finally, the sequence of tokens gets converted back into text,
which is your output.

The maximum output tokens parameter sets a limit on how many tokens to
generate using the LLM. Every model potentially uses a different tokenizer, but
a good rule of thumb is to consider a single English word to be made of 2 to 4
tokens.

As stated earlier, some tokens might not map to character sequences. One such
example is that there is often a token that indicates the end of the sequence:
when an LLM generates this token, it stops generating more. Therefore, it's
possible and often the case that an LLM generates fewer tokens than the maximum
because it generated the "stop" token.

**StopSequences**

You can use this parameter to set the tokens or token sequences that, when
generated, indicate the end of LLM output. The correct values to use here
generally depend on how the model was trained, and are usually set by the model
plugin. However, if you have prompted the model to generate another stop
sequence, you might specify it here.

Note that you are specifying character sequences, and not tokens per se. In most
cases, you will specify a character sequence that the model's tokenizer maps to
a single token.

#### Parameters that control "creativity"

The _temperature_, _top-p_, and _top-k_ parameters together control how
"creative" you want the model to be. This section provides very brief
explanations of what these parameters mean, but the more important point is
this: these parameters are used to adjust the character of an LLM's output. The
optimal values for them depend on your goals and preferences, and are likely to
be found only through experimentation.

**Temperature**

LLMs are fundamentally token-predicting machines. For a given sequence of tokens
(such as the prompt) an LLM predicts, for each token in its vocabulary, the
likelihood that the token comes next in the sequence. The temperature is a
scaling factor by which these predictions are divided before being normalized to
a probability between 0 and 1.

Low temperature values&mdash;between 0.0 and 1.0&mdash;amplify the difference in
likelihoods between tokens, with the result that the model will be even less
likely to produce a token it already evaluated to be unlikely. This is often
perceived as output that is less creative. Although 0.0 is technically not a
valid value, many models treat it as indicating that the model should behave
deterministically, and to only consider the single most likely token.

High temperature values&mdash;those greater than 1.0&mdash;compress the
differences in likelihoods between tokens, with the result that the model
becomes more likely to produce tokens it had previously evaluated to be
unlikely. This is often perceived as output that is more creative. Some model
APIs impose a maximum temperature, often 2.0.

**TopP**

_Top-p_ is a value between 0.0 and 1.0 that controls the number of possible
tokens you want the model to consider, by specifying the cumulative probability
of the tokens. For example, a value of 1.0 means to consider every possible
token (but still take into account the probability of each token). A value of
0.4 means to only consider the most likely tokens, whose probabilities add up to
0.4, and to exclude the remaining tokens from consideration.

**TopK**

_Top-k_ is an integer value that also controls the number of possible tokens you
want the model to consider, but this time by explicitly specifying the maximum
number of tokens. Specifying a value of 1 means that the model should behave
deterministically.

#### Experiment with model parameters

You can experiment with the effect of these parameters on the output generated
by different model and prompt combinations by using the Developer UI. Start the
developer UI with the `genkit start` command and it will automatically load all
of the models defined by the plugins configured in your project. You can quickly
try different prompts and configuration values without having to repeatedly make
these changes in code.

#### Pair model with its config

Given that each provider or even a specific model may have its own configuration
schema or warrant certain settings, it may be error prone to set separate
options using `ai.WithModelName()` and `ai.WithConfig()` since the latter is not
strongly typed to the former.

To pair a model with its config, you can create a model reference that you can
pass into the generate call instead:

```go
model := googlegenai.GoogleAIModelRef("gemini-2.5-flash", &genai.GenerateContentConfig{
	MaxOutputTokens: genai.Ptr[int32](500),
	StopSequences:   []string{"<end>", "<fin>"},
	Temperature:     genai.Ptr[float32](0.5),
	TopP:            genai.Ptr[float32](0.4),
	TopK:            genai.Ptr[int32](50),
})

resp, err := genkit.Generate(ctx, g,
	ai.WithModel(model),
	ai.WithPrompt("Invent a menu item for a pirate themed restaurant."),
)
if err != nil {
	log.Fatal(err)
}
```

The constructor for the model reference will enforce that the correct config
type is provided which may reduce mismatches.

### Structured output

When using generative AI as a component in your application, you often want
output in a format other than plain text. Even if you're just generating content
to display to the user, you can benefit from structured output simply for the
purpose of presenting it more attractively to the user. But for more advanced
applications of generative AI, such as programmatic use of the model's output,
or feeding the output of one model into another, structured output is a must.

In Genkit, you can request structured output from a model by specifying an
output type when you call `genkit.Generate()`:

```go
type MenuItem struct {
	Name        string   `json:"name"`
	Description string   `json:"description"`
	Calories    int      `json:"calories"`
	Allergens   []string `json:"allergens"`
}

resp, err := genkit.Generate(ctx, g,
	ai.WithPrompt("Invent a menu item for a pirate themed restaurant."),
	ai.WithOutputType(MenuItem{}),
)
if err != nil {
	log.Fatal(err) // One possible error is that the response does not conform to the type.
}
```

Model output types are specified as JSON schema using the
[`invopop/jsonschema`](https://github.com/invopop/jsonschema) package. This
provides runtime type checking, which bridges the gap between static Go types
and the unpredictable output of generative AI models. This system lets you write
code that can rely on the fact that a successful generate call will always
return output that conforms to your Go types.

When you specify an output type in `genkit.Generate()`, Genkit does several
things behind the scenes:

- Augments the prompt with additional guidance about the selected output
  format. This also has the side effect of specifying to the model what
  content exactly you want to generate (for example, not only suggest a menu
  item but also generate a description, a list of allergens, and so on).
- Verifies that the output conforms to the schema.
- Marshals the model output into a Go type.

To get structured output from a successful generate call, call `Output()` on the
model response with an empty value of the type:

```go
var item MenuItem
if err := resp.Output(&item); err != nil {
	log.Fatal(err)
}

log.Printf("%s (%d calories, %d allergens): %s\n",
	item.Name, item.Calories, len(item.Allergens), item.Description)
```

Alternatively, you can use `genkit.GenerateData()` for a more succinct call:

```go
item, resp, err := genkit.GenerateData[MenuItem](ctx, g,
	ai.WithPrompt("Invent a menu item for a pirate themed restaurant."),
)
if err != nil {
	log.Fatal(err)
}

log.Printf("%s (%d calories, %d allergens): %s\n",
	item.Name, item.Calories, len(item.Allergens), item.Description)
```

This function requires the output type parameter but automatically sets the
`ai.WithOutputType()` option and calls `ModelResponse.Output()` before returning
the value.

#### Using registered schemas

For schemas that are shared across your application (such as those used in
`.prompt` files), you can register them with `genkit.DefineSchemaFor[T]()` and
reference them by name:

```go
// Register the schema once at startup
genkit.DefineSchemaFor[MenuItem](g)

// Reference by name in generate calls
resp, err := genkit.Generate(ctx, g,
	ai.WithPrompt("Invent a menu item for a pirate themed restaurant."),
	ai.WithOutputSchemaName("MenuItem"),
)
```

This is particularly useful when working with [Dotprompt](/docs/dotprompt), as
you can define your types once in Go and reference them in `.prompt` files by
name, avoiding duplicate schema definitions.

#### Handling errors

Note in the prior example that the `genkit.Generate()` call can result in an
error. One possible error can happen when the model fails to generate output
that conforms to the schema. The best strategy for dealing with such errors will
depend on your exact use case, but here are some general hints:

- **Try a different model**. For structured output to succeed, the model must
  be capable of generating output in JSON. The most powerful LLMs like Gemini
  are versatile enough to do this; however, smaller models, such as some of
  the local models you would use with Ollama, might not be able to generate
  structured output reliably unless they have been specifically trained to do
  so.

- **Simplify the schema**. LLMs may have trouble generating complex or deeply
  nested types. Try using clear names, fewer fields, or a flattened structure
  if you are not able to reliably generate structured data.

- **Retry the `genkit.Generate()` call**. If the model you've chosen only
  rarely fails to generate conformant output, you can treat the error as you
  would treat a network error, and retry the request using some kind of
  incremental back-off strategy.

### Streaming

When generating large amounts of text, you can improve the experience for your
users by presenting the output as it's generated&mdash;streaming the output. A
familiar example of streaming in action can be seen in most LLM chat apps: users
can read the model's response to their message as it's being generated, which
improves the perceived responsiveness of the application and enhances the
illusion of chatting with an intelligent counterpart.

#### Iterator-based streaming

The recommended way to stream output in Go is using `genkit.GenerateStream()`,
which returns an iterator that you can range over. This approach uses Go's
native iterator pattern, making the code more idiomatic and easier to follow:

```go
stream := genkit.GenerateStream(ctx, g,
	ai.WithPrompt("Suggest a complete menu for a pirate themed restaurant."),
)

for result, err := range stream {
	if err != nil {
		log.Fatal(err)
	}
	if result.Done {
		// Final response is available
		log.Println("Complete response:", result.Response.Text())
		break
	}
	// Process each chunk as it arrives
	log.Println(result.Chunk.Text())
}
```

The iterator yields `*ai.ModelStreamValue` values, where:
- `result.Chunk` contains the streamed chunk data
- `result.Done` indicates whether this is the final result
- `result.Response` contains the complete response (only available when `Done` is true)

#### Streaming structured output

For streaming structured output with strong typing, use `genkit.GenerateDataStream[T]()`:

```go
type MenuItem struct {
	Name        string `json:"name"`
	Description string `json:"description"`
}

stream := genkit.GenerateDataStream[MenuItem](ctx, g,
	ai.WithPrompt("Invent a menu item for a pirate themed restaurant."),
)

for result, err := range stream {
	if err != nil {
		log.Fatal(err)
	}
	if result.Done {
		// result.Output is strongly typed as MenuItem
		log.Printf("Final: %s - %s\n", result.Output.Name, result.Output.Description)
		break
	}
	// result.Chunk is also strongly typed as MenuItem (partial data)
	if result.Chunk.Name != "" {
		log.Printf("Got name: %s\n", result.Chunk.Name)
	}
}
```

With `GenerateDataStream[T]`, both the streamed chunks and the final output are
strongly typed, making your code safer and more predictable.

#### Callback-based streaming

Alternatively, you can stream output using the `ai.WithStreaming()` callback
option with `genkit.Generate()`. This approach is useful when you want to
process chunks inline or integrate with existing callback-based code:

```go
resp, err := genkit.Generate(ctx, g,
	ai.WithPrompt("Suggest a complete menu for a pirate themed restaurant."),
	ai.WithStreaming(func(ctx context.Context, chunk *ai.ModelResponseChunk) error {
		// Process each chunk as it arrives
		log.Println(chunk.Text())
		return nil
	}),
)
if err != nil {
	log.Fatal(err)
}

log.Println(resp.Text())
```

### Multimodal input

The examples you've seen so far have used text strings as model prompts. While
this remains the most common way to prompt generative AI models, many models can
also accept other media as prompts. Media prompts are most often used in
conjunction with text prompts that instruct the model to perform some operation
on the media, such as to caption an image or transcribe an audio recording.

The ability to accept media input and the types of media you can use are
completely dependent on the model and its API. For example, the Gemini 2.5
series of models can accept images, video, and audio as prompts.

To provide a media prompt to a model that supports it, instead of passing a
simple text prompt to `genkit.Generate()`, pass an array consisting of a media
part and a text part. This example specifies an image using a
publicly-accessible HTTPS URL.

```go
resp, err := genkit.Generate(ctx, g,
	ai.WithModelName("googleai/gemini-2.5-flash"),
	ai.WithMessages(
		ai.NewUserMessage(
			ai.NewMediaPart("image/jpeg", "https://example.com/photo.jpg"),
			ai.NewTextPart("Compose a poem about this image."),
		),
	),
)
```

You can also pass media data directly by encoding it as a data URL. For
example:

```go
image, err := os.ReadFile("photo.jpg")
if err != nil {
	log.Fatal(err)
}

resp, err := genkit.Generate(ctx, g,
	ai.WithModelName("googleai/gemini-2.5-flash"),
	ai.WithMessages(
		ai.NewUserMessage(
			ai.NewMediaPart("image/jpeg", "data:image/jpeg;base64,"+base64.StdEncoding.EncodeToString(image)),
			ai.NewTextPart("Compose a poem about this image."),
		),
	),
)
```

All models that support media input support both data URLs and HTTPS URLs. Some
model plugins add support for other media sources. For example, the Vertex AI
plugin also lets you use Cloud Storage (`gs://`) URLs.

### Next steps

#### Learn more about Genkit

- As an app developer, the primary way you influence the output of generative
  AI models is through prompting. Read
  [Managing prompts with Dotprompt](/docs/dotprompt) to learn
  how Genkit helps you develop effective prompts and manage them in your
  codebase.
- Although `genkit.Generate()` is the nucleus of every generative AI powered
  application, real-world applications usually require additional work before
  and after invoking a generative AI model. To reflect this, Genkit introduces
  the concept of _flows_, which are defined like functions but add additional
  features such as observability and simplified deployment. To learn more, see
  [Defining AI workflows](/docs/flows).

#### Advanced LLM use

There are techniques your app can use to reap even more benefit from LLMs.

- One way to enhance the capabilities of LLMs is to prompt them with a list of
  ways they can request more information from you, or request you to perform
  some action. This is known as _tool calling_ or _function calling_. Models
  that are trained to support this capability can respond to a prompt with a
  specially-formatted response, which indicates to the calling application
  that it should perform some action and send the result back to the LLM along
  with the original prompt. Genkit has library functions that automate both
  the prompt generation and the call-response loop elements of a tool calling
  implementation. See [Tool calling](/docs/tool-calling) to learn more.
- Retrieval-augmented generation (RAG) is a technique used to introduce
  domain-specific information into a model's output. This is accomplished by
  inserting relevant information into a prompt before passing it on to the
  language model. A complete RAG implementation requires you to bring several
  technologies together: text embedding generation models, vector databases, and
  large language models. See [Retrieval-augmented generation (RAG)](/docs/rag) to
  learn how Genkit simplifies the process of coordinating these various
  elements.

</LanguageContent>

<LanguageContent lang="python">

At the heart of generative AI are AI _models_. Currently, the two most prominent
examples of generative models are large language models (LLMs) and image
generation models. These models take input, called a _prompt_ (most commonly
text, an image, or a combination of both), and from it produce as output text,
an image, or even audio or video.

The output of these models can be surprisingly convincing: LLMs generate text
that appears as though it could have been written by a human being, and image
generation models can produce images that are very close to real photographs or
artwork created by humans.

In addition, LLMs have proven capable of tasks beyond simple text generation:

- Writing computer programs
- Planning subtasks that are required to complete a larger task
- Organizing unorganized data
- Understanding and extracting information data from a corpus of text
- Following and performing automated activities based on a text description of
  the activity

There are many models available to you, from several different providers. Each
model has its own strengths and weaknesses and one model might excel at one task
but perform less well at others. Apps making use of generative AI can often
benefit from using multiple different models depending on the task at hand.

As an app developer, you typically don't interact with generative AI
models directly, but rather through services available as web APIs.
Although these services often have similar functionality, they all provide them
through different and incompatible APIs. If you want to make use of multiple
model services, you have to use each of their proprietary SDKs, potentially
incompatible with each other. And if you want to upgrade from one model to the
newest and most capable one, you might have to build that integration all over
again.

Genkit addresses this challenge by providing a single interface that abstracts
away the details of accessing potentially any generative AI model service, with
several pre-built implementations already available. Building your AI-powered
app around Genkit simplifies the process of making your first generative AI call
and makes it equally easy to combine multiple models or swap one model for
another as new models emerge.

### Before you begin

If you want to run the code examples on this page, first complete the steps in
the [Get started](/docs/get-started) guide. All of the examples assume that you
have already installed Genkit as a dependency in your project.

### Models supported by Genkit

Genkit is designed to be flexible enough to use potentially any generative AI
model service. Its core libraries define the common interface for working with
models, and model plugins define the implementation details for working with a
specific model and its API.

The Genkit team maintains plugins for working with models provided by Vertex AI,
Google Generative AI, and Ollama:

- Gemini family of LLMs, through the
  [Google Cloud Vertex AI plugin](/docs/integrations/vertex-ai)
- Gemini family of LLMs, through the [Google AI plugin](/docs/integrations/google-genai)
- Imagen2 and Imagen3 image generation models, through Google Cloud Vertex AI
- Anthropic's Claude 3 family of LLMs, through Google Cloud Vertex AI's model
  garden
- Gemma 2, Llama 3, and many more open models, through the [Ollama
  plugin](/docs/integrations/ollama) (you must host the Ollama server yourself)
- GPT, Dall-E and Whisper family of models, through the [OpenAI plugin](/docs/integrations/openai)
- Grok family of models, through the [xAI plugin](/docs/integrations/xai)
- DeepSeek Chat and DeepSeek Reasoner models, through the [DeepSeek plugin](/docs/integrations/deepseek)

### Loading and configuring model plugins

Before you can use Genkit to start generating content, you need to load and
configure a model plugin. If you're coming from the Getting Started guide,
you've already done this. Otherwise, see the [Get started](/docs/get-started)
guide or the individual plugin's documentation and follow the steps there before
continuing.

### The generate() method

In Genkit, the primary interface through which you interact with generative AI
models is the `generate()` method.

The simplest `generate()` call specifies the model you want to use and a text
prompt:

```python
from genkit import Genkit
from genkit.plugins.google_genai import GoogleAI

ai = Genkit(
    plugins=[GoogleAI()],
    model='googleai/gemini-2.5-flash',
)

async def main() -> None:
    result = await ai.generate(
        prompt='Invent a menu item for a pirate themed restaurant.',
    )
    print(result.text)

ai.run_main(main())
```

When you run this brief example it will print out the output of the `generate()`
call, which will usually be Markdown text as in the following example:

```md
## The Blackheart's Bounty

**A hearty stew of slow-cooked beef, spiced with rum and molasses, served in a
hollowed-out cannonball with a side of crusty bread and a dollop of tangy
pineapple salsa.**

**Description:** This dish is a tribute to the hearty meals enjoyed by pirates
on the high seas. The beef is tender and flavorful, infused with the warm spices
of rum and molasses. The pineapple salsa adds a touch of sweetness and acidity,
balancing the richness of the stew. The cannonball serving vessel adds a fun and
thematic touch, making this dish a perfect choice for any pirate-themed
adventure.
```

Run the script again and you'll get a different output.

The preceding code sample sent the generation request to the default model,
which you specified when you configured the Genkit instance.

You can also specify a model for a single `generate()` call:

```python
result = await ai.generate(
    prompt='Invent a menu item for a pirate themed restaurant.',
    model='googleai/gemini-2.5-pro',
)
```

A model string identifier looks like `providerid/modelid`, where the provider ID
(in this case, `googleai`) identifies the plugin, and the model ID is a
plugin-specific string identifier for a specific version of a model.

These examples also illustrate an important point: when you use
`generate()` to make generative AI model calls, changing the model you want to
use is simply a matter of passing a different value to the model parameter. By
using `generate()` instead of the native model SDKs, you give yourself the
flexibility to more easily use several different models in your app and change
models in the future.

So far you have only seen examples of the simplest `generate()` calls. However,
`generate()` also provides an interface for more advanced interactions with
generative models, which you will see in the sections that follow.

### System prompts

Some models support providing a _system prompt_, which gives the model
instructions as to how you want it to respond to messages from the user. You can
use the system prompt to specify a persona you want the model to adopt, the tone
of its responses, the format of its responses, and so on.

If the model you're using supports system prompts, you can provide one with the
`system` parameter:

```python
result = await ai.generate(
    system='You are a food industry marketing consultant.',
    prompt='Invent a menu item for a pirate themed restaurant.',
)
```

### Model parameters

The `generate()` function takes a `config` parameter, through which you can
specify optional settings that control how the model generates content:

```python
result = await ai.generate(
    prompt='Invent a menu item for a pirate themed restaurant.',
    config={
      'max_output_tokens': 400,
      'stop_sequences': ['<end>', '<fin>'],
      'temperature': 1.2,
      'top_p': 0.4,
      'top_k': 50,
    },
)
```

The exact parameters that are supported depend on the individual model and model
API. However, the parameters in the previous example are common to almost every
model. The following is an explanation of these parameters:

#### Parameters that control output length

**max_output_tokens**

LLMs operate on units called _tokens_. A token usually, but does not
necessarily, map to a specific sequence of characters. When you pass a prompt to
a model, one of the first steps it takes is to _tokenize_ your prompt string
into a sequence of tokens. Then, the LLM generates a sequence of tokens from the
tokenized input. Finally, the sequence of tokens gets converted back into text,
which is your output.

The maximum output tokens parameter simply sets a limit on how many tokens to
generate using the LLM. Every model potentially uses a different tokenizer, but
a good rule of thumb is to consider a single English word to be made of 2 to 4
tokens.

As stated earlier, some tokens might not map to character sequences. One such
example is that there is often a token that indicates the end of the sequence:
when an LLM generates this token, it stops generating more. Therefore, it's
possible and often the case that an LLM generates fewer tokens than the maximum
because it generated the "stop" token.

**stop_sequences**

You can use this parameter to set the tokens or token sequences that, when
generated, indicate the end of LLM output. The correct values to use here
generally depend on how the model was trained, and are usually set by the model
plugin. However, if you have prompted the model to generate another stop
sequence, you might specify it here.

Note that you are specifying character sequences, and not tokens per se. In most
cases, you will specify a character sequence that the model's tokenizer maps to
a single token.

#### Parameters that control "creativity"

The _temperature_, _top-p_, and _top-k_ parameters together control how
"creative" you want the model to be. Below are very brief explanations of what
these parameters mean, but the more important point to take away is this: these
parameters are used to adjust the character of an LLM's output. The optimal
values for them depend on your goals and preferences, and are likely to be found
only through experimentation.

**temperature**

LLMs are fundamentally token-predicting machines. For a given sequence of tokens
(such as the prompt) an LLM predicts, for each token in its vocabulary, the
likelihood that the token comes next in the sequence. The temperature is a
scaling factor by which these predictions are divided before being normalized to
a probability between 0 and 1.

Low temperature values&mdash;between 0.0 and 1.0&mdash;amplify the difference in
likelihoods between tokens, with the result that the model will be even less
likely to produce a token it already evaluated to be unlikely. This is often
perceived as output that is less creative. Although 0.0 is technically not a
valid value, many models treat it as indicating that the model should behave
deterministically, and to only consider the single most likely token.

High temperature values&mdash;those greater than 1.0&mdash;compress the
differences in likelihoods between tokens, with the result that the model
becomes more likely to produce tokens it had previously evaluated to be
unlikely. This is often perceived as output that is more creative. Some model
APIs impose a maximum temperature, often 2.0.

**top_p**

_Top-p_ is a value between 0.0 and 1.0 that controls the number of possible
tokens you want the model to consider, by specifying the cumulative probability
of the tokens. For example, a value of 1.0 means to consider every possible
token (but still take into account the probability of each token). A value of
0.4 means to only consider the most likely tokens, whose probabilities add up to
0.4, and to exclude the remaining tokens from consideration.

**top_k**

_Top-k_ is an integer value that also controls the number of possible tokens you
want the model to consider, but this time by explicitly specifying the maximum
number of tokens. Specifying a value of 1 means that the model should behave
deterministically.

#### Experiment with model parameters

You can experiment with the effect of these parameters on the output generated
by different model and prompt combinations by using the Developer UI. Start the
developer UI with the `genkit start` command and it will automatically load all
of the models defined by the plugins configured in your project. You can quickly
try different prompts and configuration values without having to repeatedly make
these changes in code.

### Structured output

When using generative AI as a component in your application, you often want
output in a format other than plain text. Even if you're just generating content
to display to the user, you can benefit from structured output simply for the
purpose of presenting it more attractively to the user. But for more advanced
applications of generative AI, such as programmatic use of the model's output,
or feeding the output of one model into another, structured output is a must.

In Genkit, you can request structured output from a model by specifying a schema
when you call `generate()`:

```python
from pydantic import BaseModel
from genkit import Output

class MenuItemSchema(BaseModel):
    name: str
    description: str
    calories: int
    allergens: list[str]

result = await ai.generate(
    prompt='Invent a menu item for a pirate themed restaurant.',
    output=Output(schema=MenuItemSchema),
)
```

Model output schemas are specified using the [Pydantic Models](https://docs.pydantic.dev/latest/concepts/models/). In addition to a schema definition language, Pydantic also provides runtime
type checking, which bridges the gap between static Python types and the
unpredictable output of generative AI models. Pydantic lets you write code that can
rely on the fact that a successful generate call will always return output that
conforms to your Python types.

When you specify a schema in `generate()`, Genkit does several things behind the
scenes:

- Augments the prompt with additional guidance about the desired output format.
  This also has the side effect of specifying to the model what content exactly
  you want to generate (for example, not only suggest a menu item but also
  generate a description, a list of allergens, and so on).
- Parses the model output into a Pydantic object.
- Verifies that the output conforms with the schema.

To get structured output from a successful generate call, use the response
object's `output` property:

```python
output = result.output
```

#### Handling errors

Note in the prior example that the `output` property can be `None`. This can
happen when the model fails to generate output that conforms to the schema.
The best strategy for dealing with such errors will depend on your exact use
case, but here are some general hints:

- **Try a different model**. For structured output to succeed, the model must be
  capable of generating output in JSON. The most powerful LLMs, like Gemini and
  Claude, are versatile enough to do this; however, smaller models, such as some
  of the local models you would use with Ollama, might not be able to generate
  structured output reliably unless they have been specifically trained to do
  so.

- **Retry the generate() call**. If the model you've chosen only rarely fails to
  generate conformant output, you can treat the error as you would treat a
  network error, and simply retry the request using some kind of incremental
  back-off strategy.

### Streaming

When generating large amounts of text, you can improve the experience for your
users by presenting the output as it's generated&mdash;streaming the output. A
familiar example of streaming in action can be seen in most LLM chat apps: users
can read the model's response to their message as it's being generated, which
improves the perceived responsiveness of the application and enhances the
illusion of chatting with an intelligent counterpart.

In Genkit, you can stream output using the `generate_stream()` method. It
returns a tuple of an async iterator (for streaming chunks) and a future (for
the final complete response):

```python
stream, response = ai.generate_stream(
  prompt='Suggest a complete menu for a pirate themed restaurant.',
)
```

You can use the `stream` iterator to iterate over the streaming output of the
request as it's generated:

```python
async for chunk in stream:
    print(chunk.text)
```

You can also get the complete output of the request, as you can with a
non-streaming request:

```python
complete_text = (await response).text
```

Streaming also works with structured output:

```python
from pydantic import BaseModel
from genkit import Output

class MenuItemSchema(BaseModel):
    name: str
    description: str
    calories: int
    allergens: list[str]

class MenuSchema(BaseModel):
    starters: list[MenuItemSchema]
    mains: list[MenuItemSchema]
    desserts: list[MenuItemSchema]

stream, response = ai.generate_stream(
    prompt='Invent a menu item for a pirate themed restaurant.',
    output=Output(schema=MenuSchema),
)

async for chunk in stream:
    print(chunk.output)

print((await response).output)
```

Streaming structured output works a little differently from streaming text: the
`output` property of a response chunk is an object constructed from the
accumulation of the chunks that have been produced so far, rather than an object
representing a single chunk (which might not be valid on its own). **Every chunk
of structured output in a sense supersedes the chunk that came before it**.

For example, here's what the first five outputs from the prior example might
look like:

```json
null

{ "starters": [ {} ] }

{
  "starters": [ { "name": "Captain's Treasure Chest", "description": "A" } ]
}

{
  "starters": [
    {
      "name": "Captain's Treasure Chest",
      "description": "A mix of spiced nuts, olives, and marinated cheese served in a treasure chest.",
      "calories": 350
    }
  ]
}

{
  "starters": [
    {
      "name": "Captain's Treasure Chest",
      "description": "A mix of spiced nuts, olives, and marinated cheese served in a treasure chest.",
      "calories": 350,
      "allergens": []
    },
    { "name": "Shipwreck Salad", "description": "Fresh" }
  ]
}
```

### Multimodal input

The examples you've seen so far have used text strings as model prompts. While
this remains the most common way to prompt generative AI models, many models can
also accept other media as prompts. Media prompts are most often used in
conjunction with text prompts that instruct the model to perform some operation
on the media, such as to caption an image or transcribe an audio recording.

The ability to accept media input and the types of media you can use are
completely dependent on the model and its API. For example, the Gemini 2.5
series of models can accept images, video, and audio as prompts.

To provide a media prompt to a model that supports it, instead of passing a
simple text prompt to `generate`, pass a list consisting of a media part and a
text part:

```python
from genkit import Part, MediaPart, Media, TextPart

result = await ai.generate(
    prompt=[
      Part(root=MediaPart(media=Media(url='https://example.com/photo.jpg', content_type='image/jpeg'))),
      Part(root=TextPart(text='Compose a poem about this image.')),
    ],
)
```

In the above example, you specified an image using a publicly-accessible HTTPS
URL. You can also pass media data directly by encoding it as a data URL. For
example:

```python
import base64
from genkit import Part, MediaPart, Media, TextPart

# Assume read_file is defined elsewhere to read image bytes
# def read_file(path):
#     with open(path, 'rb') as f:
#         return f.read()

image_bytes = read_file('image.jpg')
base64_encoded_image = base64.b64encode(image_bytes).decode('utf-8') # Decode bytes to string

result = await ai.generate(
    prompt=[
      Part(root=MediaPart(media=Media(url=f'data:image/jpeg;base64,{base64_encoded_image}', content_type='image/jpeg'))),
      Part(root=TextPart(text='Compose a poem about this image.')),
    ],
)
```

All models that support media input support both data URLs and HTTPS URLs. Some
model plugins add support for other media sources. For example, the Vertex AI
plugin also lets you use Cloud Storage (`gs://`) URLs.

### Next steps

#### Advanced LLM use

- Many of your users will have interacted with large language models for the first time through chatbots. Although LLMs are capable of much more than simulating conversations, it remains a familiar and useful style of interaction. Even when your users will not be interacting directly with the model in this way, the conversational style of prompting is a powerful way to influence the output generated by an AI model. Read [Multi-turn chats](/docs/chat) to learn how to use Genkit as part of an LLM chat implementation.
- One way to enhance the capabilities of LLMs is to prompt them with a list of ways they can request more information from you, or request you to perform some action on their behalf. This is called _tool calling_ or _function calling_. Models trained to support this feature can respond with specially-formatted responses, which indicate to the calling application that it should take some action and send the result back to the LLM. Read more at the [Tool calling](/docs/tool-calling) page.
- _Retrieval-augmented generation_ (RAG) is a technique used to introduce domain-specific information into a model's output. It's accomplished by inserting relevant information into a model's prompt before passing it to the model. A complete RAG implementation requires you to bring together several technologies: text embedding generation models, vector databases, and large language models. See the [Retrieval-augmented generation (RAG)](/docs/rag) page to learn how Genkit streamlines the process of coordinating these various elements.

#### Testing model output

As a software engineer, you're probably accustomed to deterministic systems, where a given input always produces the same output. However, with AI models being probabilistic, the output can vary based on subtle nuances in the input, the model's training data, and even deliberately-introduced randomness via parameters like temperature.

Genkit's _evaluators_ are structured ways to assess the quality of your LLM's responses using a variety of strategies. Read more on the [Evaluation](/docs/evaluation) page.

</LanguageContent>
