---
title: xAI (Grok) Plugin
description: Learn how to use xAI's Grok models with Genkit across JavaScript, Go, and Python, including text generation, image generation, and advanced configuration.
---

import LanguageSelector from '../../../../components/LanguageSelector.astro';
import CopyMarkdownButton from '../../../../components/CopyMarkdownButton.astro';
import LanguageContent from '../../../../components/LanguageContent.astro';

<div style="display: flex; justify-content: space-between; align-items: center; gap: 1rem; margin: 1rem 0 1rem 0;">
  <LanguageSelector />
  <CopyMarkdownButton />
</div>

The xAI plugin provides access to xAI's powerful Grok family of models, including advanced text generation and image generation capabilities. Grok models are known for their real-time information access and conversational abilities.

## Installation and Setup

<LanguageContent lang="js">
Install the xAI plugin:

    ```bash
    npm install @genkit-ai/compat-oai
    ```

    Configure the plugin when initializing Genkit:

    ```ts
    import { genkit } from 'genkit';
    import { xAI } from '@genkit-ai/compat-oai/xai';

    const ai = genkit({
      plugins: [xAI()],
    });
    ```

    ### API Key Configuration

    Set your xAI API key using one of these methods:

    ```bash
    # Environment variable (recommended)
    export XAI_API_KEY=your_xai_api_key
    ```

    ```ts
    // Or pass directly to plugin (not recommended for production)
    const ai = genkit({
      plugins: [xAI({ apiKey: 'your_xai_api_key' })],
    });
    ```

    Get your API key from [xAI Console](https://console.x.ai/).
</LanguageContent>

<LanguageContent lang="go">
For Go applications, use the OpenAI-compatible client with xAI endpoints:

    ```go
    package main

    import (
        "context"
        "github.com/firebase/genkit/go/genkit"
        "github.com/firebase/genkit/go/plugins/openai"
    )

    func main() {
        ctx := context.Background()
        g, err := genkit.Init(ctx,
            genkit.WithPlugins(&openai.OpenAI{
                APIKey:  os.Getenv("XAI_API_KEY"),
                BaseURL: "https://api.x.ai/v1",
                Models: []openai.ModelConfig{
                    {Name: "grok-3-mini", Type: "chat"},
                    {Name: "grok-3", Type: "chat"},
                    {Name: "grok-image", Type: "generate"},
                },
            }),
        )
        if err != nil {
            log.Fatal(err)
        }
    }
    ```

    ### Environment Configuration

    ```bash
    export XAI_API_KEY=your_xai_api_key
    ```
</LanguageContent>

<LanguageContent lang="python">
For Python applications, use the OpenAI-compatible client:

    ```bash
    pip install genkit-plugin-openai
    ```

    ```python
    from genkit.ai import Genkit
    from genkit.plugins.openai import OpenAI

    ai = Genkit(
        plugins=[OpenAI(
            api_key=os.getenv("XAI_API_KEY"),
            base_url="https://api.x.ai/v1",
            models=[
                {"name": "grok-3-mini", "type": "chat"},
                {"name": "grok-3", "type": "chat"},
                {"name": "grok-image", "type": "generate"},
            ],
        )],
    )
    ```

    ### Environment Configuration

    ```bash
    export XAI_API_KEY=your_xai_api_key
    ```
</LanguageContent>

## Basic Usage

### Text Generation

<LanguageContent lang="js">
Use Grok models for text generation:

    ```ts
    import { genkit, z } from 'genkit';
    import { xAI } from '@genkit-ai/compat-oai/xai';

    const ai = genkit({
      plugins: [xAI()],
    });

    // Basic text generation
    const response = await ai.generate({
      model: xAI.model('grok-3-mini'),
      prompt: 'Explain quantum computing in simple terms',
    });

    console.log(response.text);

    // Flow with Grok
    export const grokFlow = ai.defineFlow(
      {
        name: 'grokFlow',
        inputSchema: z.object({ subject: z.string() }),
        outputSchema: z.object({ fact: z.string() }),
      },
      async ({ subject }) => {
        const llmResponse = await ai.generate({
          model: xAI.model('grok-3-mini'),
          prompt: `Tell me a fun fact about ${subject}`,
        });
        return { fact: llmResponse.text };
      },
    );

    // Real-time information queries
    const newsResponse = await ai.generate({
      model: xAI.model('grok-3'),
      prompt: 'What are the latest developments in AI this week?',
      config: {
        temperature: 0.7,
        maxTokens: 500,
      },
    });
    ```
</LanguageContent>

<LanguageContent lang="go">
Use Grok models with the generation API:

    ```go
    import (
        "context"
        "github.com/firebase/genkit/go/ai"
        "github.com/firebase/genkit/go/genkit"
    )

    func main() {
        ctx := context.Background()
        
        // Basic text generation
        resp, err := genkit.Generate(ctx, g,
            ai.WithModel("grok-3-mini"),
            ai.WithPrompt("Explain quantum computing in simple terms"),
        )
        if err != nil {
            log.Fatal(err)
        }

        fmt.Println(resp.Text())

        // Real-time information queries
        newsResp, err := genkit.Generate(ctx, g,
            ai.WithModel("grok-3"),
            ai.WithPrompt("What are the latest developments in AI this week?"),
            ai.WithConfig(map[string]interface{}{
                "temperature": 0.7,
                "max_tokens":  500,
            }),
        )
        if err != nil {
            log.Fatal(err)
        }

        fmt.Println(newsResp.Text())
    }
    ```
</LanguageContent>

<LanguageContent lang="python">
Use Grok models with the generation API:

    ```python
    from genkit.ai import Genkit
    from genkit.plugins.openai import OpenAI, openai_name

    ai = Genkit(
        plugins=[OpenAI(
            api_key=os.getenv("XAI_API_KEY"),
            base_url="https://api.x.ai/v1",
            models=[
                {"name": "grok-3-mini", "type": "chat"},
                {"name": "grok-3", "type": "chat"},
            ],
        )],
    )

    # Basic text generation
    response = await ai.generate(
        model=openai_name('grok-3-mini'),
        prompt='Explain quantum computing in simple terms'
    )
    print(response.text)

    # Real-time information queries
    news_response = await ai.generate(
        model=openai_name('grok-3'),
        prompt='What are the latest developments in AI this week?',
        config={
            'temperature': 0.7,
            'max_tokens': 500,
        }
    )
    print(news_response.text)
    ```
</LanguageContent>

### Image Generation

<LanguageContent lang="js">
Use Grok for image generation:

    ```ts
    // Image generation with Grok
    const imageResponse = await ai.generate({
      model: xAI.model('grok-image'),
      prompt: 'A futuristic cityscape with flying cars and neon lights',
      config: {
        size: '1024x1024',
        quality: 'hd',
        style: 'vivid',
      },
    });

    // Image generation flow
    export const imageFlow = ai.defineFlow(
      {
        name: 'imageFlow',
        inputSchema: z.object({ 
          description: z.string(),
          style: z.string().optional(),
        }),
        outputSchema: z.object({ imageUrl: z.string() }),
      },
      async ({ description, style }) => {
        const prompt = style 
          ? `${description} in ${style} style`
          : description;

        const response = await ai.generate({
          model: xAI.model('grok-image'),
          prompt,
          config: {
            size: '1024x1024',
            quality: 'hd',
          },
        });

        return { imageUrl: response.media?.url || '' };
      },
    );
    ```
</LanguageContent>

<LanguageContent lang="go">
Use Grok for image generation:

    ```go
    // Image generation
    imageResp, err := genkit.Generate(ctx, g,
        ai.WithModel("grok-image"),
        ai.WithPrompt("A futuristic cityscape with flying cars and neon lights"),
        ai.WithConfig(map[string]interface{}{
            "size":    "1024x1024",
            "quality": "hd",
            "style":   "vivid",
        }),
    )
    if err != nil {
        log.Fatal(err)
    }

    // Access generated image
    if imageResp.Media() != nil {
        fmt.Printf("Generated image URL: %s\n", imageResp.Media().URL)
    }
    ```
</LanguageContent>

<LanguageContent lang="python">
Use Grok for image generation:

    ```python
    # Image generation
    image_response = await ai.generate(
        model=openai_name('grok-image'),
        prompt='A futuristic cityscape with flying cars and neon lights',
        config={
            'size': '1024x1024',
            'quality': 'hd',
            'style': 'vivid',
        }
    )

    # Access generated image
    if image_response.media:
        print(f"Generated image URL: {image_response.media.url}")
    ```
</LanguageContent>

## Advanced Features

### Real-time Information Access

<LanguageContent lang="js">
Leverage Grok's real-time information capabilities:

    ```ts
    // Current events and news
    const newsFlow = ai.defineFlow(
      {
        name: 'newsFlow',
        inputSchema: z.object({ topic: z.string() }),
        outputSchema: z.object({ summary: z.string() }),
      },
      async ({ topic }) => {
        const response = await ai.generate({
          model: xAI.model('grok-3'),
          prompt: `Provide a current summary of recent news about ${topic}. Include the latest developments and key information.`,
          config: {
            temperature: 0.3, // Lower temperature for factual content
            maxTokens: 800,
          },
        });
        return { summary: response.text };
      },
    );

    // Market data and trends
    const marketFlow = ai.defineFlow(
      {
        name: 'marketFlow',
        inputSchema: z.object({ symbol: z.string() }),
        outputSchema: z.object({ analysis: z.string() }),
      },
      async ({ symbol }) => {
        const response = await ai.generate({
          model: xAI.model('grok-3'),
          prompt: `Analyze the current market situation for ${symbol}. Include recent price movements, news, and relevant factors.`,
          config: {
            temperature: 0.4,
            maxTokens: 600,
          },
        });
        return { analysis: response.text };
      },
    );
    ```
</LanguageContent>

<LanguageContent lang="go">
Leverage Grok's real-time information capabilities:

    ```go
    // Current events and news
    func getNewsAnalysis(ctx context.Context, topic string) (string, error) {
        resp, err := genkit.Generate(ctx, g,
            ai.WithModel("grok-3"),
            ai.WithPrompt(fmt.Sprintf(
                "Provide a current summary of recent news about %s. Include the latest developments and key information.",
                topic,
            )),
            ai.WithConfig(map[string]interface{}{
                "temperature": 0.3,
                "max_tokens":  800,
            }),
        )
        if err != nil {
            return "", err
        }
        return resp.Text(), nil
    }

    // Market data and trends
    func getMarketAnalysis(ctx context.Context, symbol string) (string, error) {
        resp, err := genkit.Generate(ctx, g,
            ai.WithModel("grok-3"),
            ai.WithPrompt(fmt.Sprintf(
                "Analyze the current market situation for %s. Include recent price movements, news, and relevant factors.",
                symbol,
            )),
            ai.WithConfig(map[string]interface{}{
                "temperature": 0.4,
                "max_tokens":  600,
            }),
        )
        if err != nil {
            return "", err
        }
        return resp.Text(), nil
    }
    ```
</LanguageContent>

<LanguageContent lang="python">
Leverage Grok's real-time information capabilities:

    ```python
    # Current events and news
    async def get_news_analysis(topic: str) -> str:
        response = await ai.generate(
            model=openai_name('grok-3'),
            prompt=f"Provide a current summary of recent news about {topic}. Include the latest developments and key information.",
            config={
                'temperature': 0.3,
                'max_tokens': 800,
            }
        )
        return response.text

    # Market data and trends
    async def get_market_analysis(symbol: str) -> str:
        response = await ai.generate(
            model=openai_name('grok-3'),
            prompt=f"Analyze the current market situation for {symbol}. Include recent price movements, news, and relevant factors.",
            config={
                'temperature': 0.4,
                'max_tokens': 600,
            }
        )
        return response.text
    ```
</LanguageContent>

### Conversational AI

<LanguageContent lang="js">
Build conversational applications with Grok:

    ```ts
    // Conversational chat flow
    export const chatFlow = ai.defineFlow(
      {
        name: 'chatFlow',
        inputSchema: z.object({
          message: z.string(),
          history: z.array(z.object({
            role: z.enum(['user', 'assistant']),
            content: z.string(),
          })).optional(),
        }),
        outputSchema: z.object({ response: z.string() }),
      },
      async ({ message, history = [] }) => {
        // Build conversation context
        const messages = [
          { role: 'system', content: 'You are Grok, a helpful and witty AI assistant with access to real-time information.' },
          ...history,
          { role: 'user', content: message },
        ];

        const response = await ai.generate({
          model: xAI.model('grok-3'),
          messages,
          config: {
            temperature: 0.8,
            maxTokens: 1000,
          },
        });

        return { response: response.text };
      },
    );

    // Personality-driven responses
    export const personalityFlow = ai.defineFlow(
      {
        name: 'personalityFlow',
        inputSchema: z.object({ 
          query: z.string(),
          personality: z.enum(['witty', 'professional', 'casual', 'technical']),
        }),
        outputSchema: z.object({ response: z.string() }),
      },
      async ({ query, personality }) => {
        const personalityPrompts = {
          witty: 'Respond with humor and wit, making clever observations.',
          professional: 'Respond in a professional, formal tone.',
          casual: 'Respond in a casual, friendly manner.',
          technical: 'Respond with technical depth and precision.',
        };

        const response = await ai.generate({
          model: xAI.model('grok-3-mini'),
          prompt: `${personalityPrompts[personality]} Query: ${query}`,
          config: {
            temperature: personality === 'witty' ? 0.9 : 0.6,
            maxTokens: 600,
          },
        });

        return { response: response.text };
      },
    );
    ```
</LanguageContent>

<LanguageContent lang="go">
Build conversational applications with Grok:

    ```go
    type ChatMessage struct {
        Role    string `json:"role"`
        Content string `json:"content"`
    }

    func handleChat(ctx context.Context, message string, history []ChatMessage) (string, error) {
        // Build conversation context
        messages := []ChatMessage{
            {Role: "system", Content: "You are Grok, a helpful and witty AI assistant with access to real-time information."},
        }
        messages = append(messages, history...)
        messages = append(messages, ChatMessage{Role: "user", Content: message})

        resp, err := genkit.Generate(ctx, g,
            ai.WithModel("grok-3"),
            ai.WithMessages(messages),
            ai.WithConfig(map[string]interface{}{
                "temperature": 0.8,
                "max_tokens":  1000,
            }),
        )
        if err != nil {
            return "", err
        }

        return resp.Text(), nil
    }
    ```
</LanguageContent>

<LanguageContent lang="python">
Build conversational applications with Grok:

    ```python
    from typing import List, Dict

    async def handle_chat(message: str, history: List[Dict[str, str]] = None) -> str:
        if history is None:
            history = []
        
        # Build conversation context
        messages = [
            {"role": "system", "content": "You are Grok, a helpful and witty AI assistant with access to real-time information."},
            *history,
            {"role": "user", "content": message},
        ]

        response = await ai.generate(
            model=openai_name('grok-3'),
            messages=messages,
            config={
                'temperature': 0.8,
                'max_tokens': 1000,
            }
        )

        return response.text
    ```
</LanguageContent>

## Model Comparison

### Available Models

| Model | Capabilities | Best For | Context Window |
|-------|-------------|----------|----------------|
| **grok-3-mini** | Fast text generation | Quick responses, simple tasks | 128K tokens |
| **grok-3** | Advanced reasoning, real-time data | Complex analysis, current events | 128K tokens |
| **grok-image** | Image generation | Creative visuals, concept art | N/A |

### Performance Characteristics

<LanguageContent lang="js">
```ts
    // Performance comparison example
    const performanceTest = async () => {
      const prompt = "Explain the impact of AI on modern society";
      
      // Fast response with grok-3-mini
      const startMini = Date.now();
      const miniResponse = await ai.generate({
        model: xAI.model('grok-3-mini'),
        prompt,
      });
      const miniTime = Date.now() - startMini;
      
      // Detailed response with grok-3
      const startFull = Date.now();
      const fullResponse = await ai.generate({
        model: xAI.model('grok-3'),
        prompt,
      });
      const fullTime = Date.now() - startFull;
      
      console.log(`Mini: ${miniTime}ms, Full: ${fullTime}ms`);
      console.log(`Mini length: ${miniResponse.text.length}, Full length: ${fullResponse.text.length}`);
    };
    ```
</LanguageContent>

<LanguageContent lang="go">
```go
    func performanceTest(ctx context.Context) {
        prompt := "Explain the impact of AI on modern society"
        
        // Fast response with grok-3-mini
        startMini := time.Now()
        miniResp, _ := genkit.Generate(ctx, g,
            ai.WithModel("grok-3-mini"),
            ai.WithPrompt(prompt),
        )
        miniTime := time.Since(startMini)
        
        // Detailed response with grok-3
        startFull := time.Now()
        fullResp, _ := genkit.Generate(ctx, g,
            ai.WithModel("grok-3"),
            ai.WithPrompt(prompt),
        )
        fullTime := time.Since(startFull)
        
        fmt.Printf("Mini: %v, Full: %v\n", miniTime, fullTime)
        fmt.Printf("Mini length: %d, Full length: %d\n", 
            len(miniResp.Text()), len(fullResp.Text()))
    }
    ```
</LanguageContent>

<LanguageContent lang="python">
```python
    import time

    async def performance_test():
        prompt = "Explain the impact of AI on modern society"
        
        # Fast response with grok-3-mini
        start_mini = time.time()
        mini_response = await ai.generate(
            model=openai_name('grok-3-mini'),
            prompt=prompt
        )
        mini_time = time.time() - start_mini
        
        # Detailed response with grok-3
        start_full = time.time()
        full_response = await ai.generate(
            model=openai_name('grok-3'),
            prompt=prompt
        )
        full_time = time.time() - start_full
        
        print(f"Mini: {mini_time:.2f}s, Full: {full_time:.2f}s")
        print(f"Mini length: {len(mini_response.text)}, Full length: {len(full_response.text)}")
    ```
</LanguageContent>

## Advanced Configuration

### Custom Model Configuration

<LanguageContent lang="js">
```ts
    // Advanced configuration with passthrough options
    const response = await ai.generate({
      model: xAI.model('grok-3'),
      prompt: 'Analyze the latest tech trends',
      config: {
        temperature: 0.7,
        maxTokens: 1000,
        topP: 0.9,
        frequencyPenalty: 0.1,
        presencePenalty: 0.1,
        // Passthrough configuration for new features
        stream: true,
        logprobs: true,
        top_logprobs: 5,
      },
    });

    // Environment-specific configuration
    const environmentConfig = {
      development: {
        model: xAI.model('grok-3-mini'),
        temperature: 0.8,
        maxTokens: 500,
      },
      production: {
        model: xAI.model('grok-3'),
        temperature: 0.6,
        maxTokens: 1000,
      },
    };

    const config = environmentConfig[process.env.NODE_ENV || 'development'];
    ```
</LanguageContent>

<LanguageContent lang="go">
```go
    // Advanced configuration
    resp, err := genkit.Generate(ctx, g,
        ai.WithModel("grok-3"),
        ai.WithPrompt("Analyze the latest tech trends"),
        ai.WithConfig(map[string]interface{}{
            "temperature":        0.7,
            "max_tokens":        1000,
            "top_p":             0.9,
            "frequency_penalty": 0.1,
            "presence_penalty":  0.1,
            "stream":            true,
            "logprobs":          true,
            "top_logprobs":      5,
        }),
    )
    ```
</LanguageContent>

<LanguageContent lang="python">
```python
    # Advanced configuration
    response = await ai.generate(
        model=openai_name('grok-3'),
        prompt='Analyze the latest tech trends',
        config={
            'temperature': 0.7,
            'max_tokens': 1000,
            'top_p': 0.9,
            'frequency_penalty': 0.1,
            'presence_penalty': 0.1,
            'stream': True,
            'logprobs': True,
            'top_logprobs': 5,
        }
    )
    ```
</LanguageContent>

## Best Practices

### Optimizing for Real-time Information

1. **Use appropriate models**: Use `grok-3` for current events and real-time data
2. **Set proper temperature**: Lower temperature (0.3-0.5) for factual content
3. **Specify time context**: Include "current", "latest", or "recent" in prompts
4. **Verify information**: Cross-reference important facts when possible

### Cost Optimization

1. **Choose the right model**: Use `grok-3-mini` for simple tasks
2. **Optimize token usage**: Be concise in prompts and set appropriate `maxTokens`
3. **Cache responses**: Cache frequently requested information
4. **Batch requests**: Group similar requests when possible

### Error Handling

<LanguageContent lang="js">
```ts
    const robustGrokFlow = ai.defineFlow(
      {
        name: 'robustGrokFlow',
        inputSchema: z.object({ query: z.string() }),
        outputSchema: z.object({ response: z.string() }),
      },
      async ({ query }) => {
        try {
          const response = await ai.generate({
            model: xAI.model('grok-3'),
            prompt: query,
            config: {
              temperature: 0.7,
              maxTokens: 800,
            },
          });
          return { response: response.text };
        } catch (error) {
          if (error.message.includes('rate_limit')) {
            // Fallback to mini model
            const fallbackResponse = await ai.generate({
              model: xAI.model('grok-3-mini'),
              prompt: query,
            });
            return { response: fallbackResponse.text };
          }
          throw error;
        }
      },
    );
    ```
</LanguageContent>

<LanguageContent lang="go">
```go
    func robustGrokGenerate(ctx context.Context, query string) (string, error) {
        resp, err := genkit.Generate(ctx, g,
            ai.WithModel("grok-3"),
            ai.WithPrompt(query),
            ai.WithConfig(map[string]interface{}{
                "temperature": 0.7,
                "max_tokens":  800,
            }),
        )
        
        if err != nil {
            if strings.Contains(err.Error(), "rate_limit") {
                // Fallback to mini model
                fallbackResp, fallbackErr := genkit.Generate(ctx, g,
                    ai.WithModel("grok-3-mini"),
                    ai.WithPrompt(query),
                )
                if fallbackErr != nil {
                    return "", fallbackErr
                }
                return fallbackResp.Text(), nil
            }
            return "", err
        }
        
        return resp.Text(), nil
    }
    ```
</LanguageContent>

<LanguageContent lang="python">
```python
    async def robust_grok_generate(query: str) -> str:
        try:
            response = await ai.generate(
                model=openai_name('grok-3'),
                prompt=query,
                config={
                    'temperature': 0.7,
                    'max_tokens': 800,
                }
            )
            return response.text
        except Exception as error:
            if 'rate_limit' in str(error):
                # Fallback to mini model
                fallback_response = await ai.generate(
                    model=openai_name('grok-3-mini'),
                    prompt=query
                )
                return fallback_response.text
            raise error
    ```
</LanguageContent>

## Next Steps

- Learn about [generating content](/docs/models) to understand how to use these models effectively
- Explore [tool calling](/docs/tool-calling) to add interactive capabilities to your Grok applications
- See [creating flows](/docs/flows) to build structured AI workflows with real-time information
- Check out [deployment guides](/docs/deployment) for production deployment strategies
