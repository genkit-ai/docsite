---
title: Google AI plugin
description: Learn how to use Google's Gemini models with Genkit across JavaScript, Go, and Python through the Google AI Studio API, including text generation, embeddings, image generation, video generation, and text-to-speech.
---

import LanguageSelector from '../../../../components/LanguageSelector.astro';
import CopyMarkdownButton from '../../../../components/CopyMarkdownButton.astro';
import LanguageContent from '../../../../components/LanguageContent.astro';

<div style="display: flex; justify-content: space-between; align-items: center; gap: 1rem; margin: 1rem 0 1rem 0;">
  <LanguageSelector />
  <CopyMarkdownButton />
</div>

<LanguageContent lang="js">

The Google GenAI plugin provides a unified interface to connect with Google's generative AI models through the **Gemini Developer API** using API key authentication. It is a replacement for the previous `googleAI` plugin.

## Installation

```bash
npm i --save @genkit-ai/google-genai
```

## Configuration

```typescript
import { genkit } from 'genkit';
import { googleAI } from '@genkit-ai/google-genai';

const ai = genkit({
  plugins: [
    googleAI(),
    // Or with an explicit API key:
    // googleAI({ apiKey: 'your-api-key' }),
  ],
});
```

**Authentication:** Requires a Google AI API Key, which you can get from [Google AI Studio](https://aistudio.google.com/apikey). You can provide this key in several ways:

1. **Environment variables**: Set `GEMINI_API_KEY` or `GOOGLE_API_KEY`
2. **Plugin configuration**: Pass `apiKey` when initializing the plugin (shown above)
3. **Per-request**: Override the API key for specific requests in the config:

```typescript
const response = await ai.generate({
  model: googleAI.model('gemini-2.5-flash'),
  prompt: 'Your prompt here',
  config: {
    apiKey: 'different-api-key', // Use a different API key for this request
  },
});
```

This per-request API key option is useful for routing specific requests to different API keys, such as for multi-tenant applications or cost tracking.

## Usage Examples

### Text Generation

```typescript
import { genkit } from 'genkit';
import { googleAI } from '@genkit-ai/google-genai';

const ai = genkit({
  plugins: [googleAI()],
});

const response = await ai.generate({
  model: googleAI.model('gemini-2.5-flash'),
  prompt: 'Explain how neural networks learn in simple terms.',
});

console.log(response.text);
```

### Text Embedding

```typescript
const embeddings = await ai.embed({
  embedder: googleAI.embedder('gemini-embedding-001'),
  content: 'Machine learning models process data to make predictions.',
});

console.log(embeddings);
```

### Image Generation

```typescript
const response = await ai.generate({
  model: googleAI.model('imagen-4.0-generate-001'),
  prompt: 'A serene Japanese garden with cherry blossoms and a koi pond.',
});

const generatedImage = response.media;
console.log('Image generated:', generatedImage);
```

## Language Models

You can create models that call the Google Generative AI API. The models support tool calls and some have multi-modal capabilities.

### Model Capabilities

The following table shows the capabilities of popular Gemini models:

| Model | Image Input | Object Generation | Tool Usage | Tool Streaming | Thinking | Code Execution | Google Search |
|-------|-------------|-------------------|------------|----------------|----------|----------------|---------------|
| `gemini-2.5-pro` | ✓ | ✓ | ✓ | ✓ | ✓ | ✓ | ✓ |
| `gemini-2.5-flash` | ✓ | ✓ | ✓ | ✓ | ✓ | ✓ | ✓ |
| `gemini-2.5-flash-lite` | ✓ | ✓ | ✓ | ✓ | ✓ | ✓ | ✓ |

:::note
See the [Google Generative AI models documentation](https://ai.google.dev/gemini-api/docs/models) for a complete list of available models and their capabilities.
:::

## Available Models

The Google GenAI plugin provides access to various model types for different use cases:

### Text Generation Models

**Gemini 2.5 Series** - Latest models with advanced reasoning and multimodal capabilities:
- `gemini-2.5-pro` - Most capable model for complex tasks
- `gemini-2.5-flash` - Fast and efficient for most use cases
- `gemini-2.5-flash-lite` - Lightweight version for simple tasks
- `gemini-2.5-flash-image` - Supports image generation outputs

**Gemini 2.0 Series (Legacy)** - Previous generation models:
- `gemini-2.0-flash` - Fast model with good capabilities
- `gemini-2.0-flash-lite` - Lightweight version

**Gemma 3 Series** - Open models for various use cases:
- `gemma-3-27b-it` - Large instruction-tuned model
- `gemma-3-12b-it` - Medium instruction-tuned model
- `gemma-3-4b-it` - Small instruction-tuned model
- `gemma-3-1b-it` - Tiny instruction-tuned model
- `gemma-3n-e4b-it` - Efficient 4-bit model

### Embedding Models

- `gemini-embedding-001` - Latest Gemini embedding model (3072 dimensions, customizable)
- `text-embedding-004` - Text embedding model (768 dimensions, customizable)

### Image Generation Models

**Imagen 4 Series** - Latest generation with improved quality:
- `imagen-4.0-generate-001` - Standard quality image generation
- `imagen-4.0-ultra-generate-001` - Ultra-high quality image generation
- `imagen-4.0-fast-generate-001` - Fast image generation

**Imagen 3 Series** - Previous generation:
- `imagen-3.0-generate-002` - High-quality image generation

### Video Generation Models (Veo)

- `veo-3.0-generate-001` - Latest Veo model with improved quality
- `veo-3.0-fast-generate-001` - Fast video generation
- `veo-2.0-generate-001` - Previous generation video model

### Text-to-Speech Models

- `gemini-2.5-flash-preview-tts` - Flash model with TTS
- `gemini-2.5-pro-preview-tts` - Pro model with TTS

## Using Embedding Models

Create embedding models that call the Google Generative AI embeddings API:

```typescript
const embedder = googleAI.embedder('gemini-embedding-001');
```

## Using Image Generation Models

Create image generation models using Imagen:

```typescript
const model = googleAI.model('imagen-4.0-generate-001');
```

## Provider Options

### Safety Settings

You can configure safety settings to control content filtering for different harm categories:

```typescript
const response = await ai.generate({
  model: googleAI.model('gemini-2.5-flash'),
  prompt: 'Your prompt here',
  config: {
    safetySettings: [
      {
        category: 'HARM_CATEGORY_HATE_SPEECH',
        threshold: 'BLOCK_MEDIUM_AND_ABOVE',
      },
      {
        category: 'HARM_CATEGORY_DANGEROUS_CONTENT',
        threshold: 'BLOCK_MEDIUM_AND_ABOVE',
      },
    ],
  },
});
```

Available harm categories:
- `HARM_CATEGORY_HATE_SPEECH`
- `HARM_CATEGORY_DANGEROUS_CONTENT`
- `HARM_CATEGORY_HARASSMENT`
- `HARM_CATEGORY_SEXUALLY_EXPLICIT`

Available thresholds:
- `HARM_BLOCK_THRESHOLD_UNSPECIFIED`
- `BLOCK_LOW_AND_ABOVE`
- `BLOCK_MEDIUM_AND_ABOVE`
- `BLOCK_ONLY_HIGH`
- `BLOCK_NONE`

### Accessing Safety Ratings

You can access safety ratings from the Gemini API response through the provider-specific custom metadata:

```typescript
const response = await ai.generate({
  model: googleAI.model('gemini-2.5-flash'),
  prompt: 'Your prompt here',
  config: {
    safetySettings: [
      {
        category: 'HARM_CATEGORY_HATE_SPEECH',
        threshold: 'BLOCK_MEDIUM_AND_ABOVE',
      },
    ],
  },
});

// Access the full Gemini API response from custom metadata
const geminiResponse = response.custom as any;

// Safety ratings may be in candidates (per-response ratings)
const candidateSafetyRatings = geminiResponse?.candidates?.[0]?.safetyRatings;

// Or in promptFeedback (if the prompt itself was flagged)
const promptSafetyRatings = geminiResponse?.promptFeedback?.safetyRatings;
```

:::note
Safety ratings are not always present in responses - they are typically only included when content is flagged for safety concerns. See the [Google AI safety settings documentation](https://ai.google.dev/gemini-api/docs/safety-settings) for more details.
:::

### Thinking and Reasoning

Gemini 2.5 models use an internal thinking process that improves reasoning for complex tasks. You can control the thinking budget and access thought summaries:

```typescript
const response = await ai.generate({
  model: googleAI.model('gemini-2.5-flash'),
  prompt: 'What is the sum of the first 10 prime numbers?',
  config: {
    thinkingConfig: {
      thinkingBudget: 8192, // Number of thinking tokens
      includeThoughts: true, // Include thought summaries
    },
  },
});

console.log(response.text);
// Access reasoning summary if available
if (response.reasoning) {
  console.log('Reasoning:', response.reasoning);
}
```

For more information, see the [Google AI Thinking documentation](https://ai.google.dev/gemini-api/docs/thinking).

### File Inputs and YouTube URLs

Gemini models support various file types including PDFs and can process YouTube videos directly:

```typescript
import * as fs from 'fs';

// PDF file input
const pdfResponse = await ai.generate({
  model: googleAI.model('gemini-2.5-flash'),
  prompt: [
    { text: 'Summarize this document:' },
    {
      media: {
        contentType: 'application/pdf',
        url: `data:application/pdf;base64,${fs.readFileSync('./document.pdf', 'base64')}`,
      },
    },
  ],
});

// YouTube URL input
const videoResponse = await ai.generate({
  model: googleAI.model('gemini-2.5-flash'),
  prompt: [
    { text: 'Summarize this video:' },
    {
      media: {
        contentType: 'video/mp4',
        url: 'https://www.youtube.com/watch?v=VIDEO_ID',
      },
    },
  ],
});
```

:::note
YouTube URLs (public or unlisted videos) are supported directly. You can specify one YouTube video URL per request.
:::

### Video Understanding

Gemini models can process videos to describe content, answer questions, and refer to specific timestamps. This enables powerful use cases like video summarization, content analysis, and temporal question answering.

#### Referring to Timestamps

You can ask questions about specific points in time within a video using timestamps in `MM:SS` format:

```typescript
const response = await ai.generate({
  model: googleAI.model('gemini-2.5-flash'),
  prompt: [
    { text: 'What are the examples given at 00:05 and 00:10 supposed to show us?' },
    {
      media: {
        contentType: 'video/mp4',
        url: 'https://www.youtube.com/watch?v=VIDEO_ID',
      },
    },
  ],
});
```

#### Transcription with Visual Descriptions

Gemini can transcribe audio and provide visual descriptions by processing both the audio track and visual frames (sampled at 1 frame per second):

```typescript
// Using an uploaded file from the Files API
const response = await ai.generate({
  model: googleAI.model('gemini-2.5-flash'),
  prompt: [
    { text: 'Transcribe the audio from this video, giving timestamps for salient events in the video. Also provide visual descriptions.' },
    {
      media: {
        contentType: uploadedFile.mimeType,
        url: uploadedFile.uri,
      },
    },
  ],
});
```

#### Video Processing Details

**Supported Models:**
- All Gemini 2.0 and 2.5 models can process video data
- Gemini 2.5 series models provide the best quality for video understanding

**Context Limits:**
- 2M context models: Up to 2 hours (default resolution) or 6 hours (low resolution)
- 1M context models: Up to 1 hour (default resolution) or 3 hours (low resolution)

**Frame Sampling:**
- Videos are sampled at 1 frame per second by default
- This sampling rate affects the level of detail in visual descriptions

**Token Calculation:**
- Approximately 300 tokens per second at default media resolution
- Approximately 100 tokens per second at low media resolution
- Audio: 32 tokens per second

**Best Practices:**
- Use `MM:SS` format for timestamps (e.g., `01:15` for 1 minute 15 seconds)
- Place text prompts *after* video parts in the prompt array
- Use one video per request for optimal results
- For Gemini 2.5+, you can include up to 10 videos per request

:::note
For advanced video processing features like custom frame rates and clipping intervals, see the [Google AI Video Understanding documentation](https://ai.google.dev/gemini-api/docs/video-understanding).
:::

#### Supported Video Formats

Gemini supports the following video MIME types:
- `video/mp4`
- `video/mpeg`
- `video/mov`
- `video/avi`
- `video/x-flv`
- `video/mpg`
- `video/webm`
- `video/wmv`
- `video/3gpp`

### Context Caching

Gemini 2.5 models automatically cache common content prefixes, providing a 75% token discount on cached tokens:

```typescript
// Structure prompts with consistent content at the beginning
const baseContext = `You are a helpful cooking assistant with expertise in Italian cuisine.
Here is a comprehensive database of traditional Italian recipes and techniques...`.repeat(50);

// First request - content will be cached
const response1 = await ai.generate({
  model: googleAI.model('gemini-2.5-pro'),
  prompt: `${baseContext}\n\nCreate a vegetarian pasta recipe for 4 people.`,
});

// Second request with same prefix - eligible for cache hit and 75% discount
const response2 = await ai.generate({
  model: googleAI.model('gemini-2.5-pro'),
  prompt: `${baseContext}\n\nCreate a seafood pasta recipe for 6 people.`,
});
```

**Minimum token requirements for automatic caching:**
- Gemini 2.5 Flash: 1024 tokens
- Gemini 2.5 Pro: 2048 tokens

### Code Execution

Certain models can generate and execute Python code for calculations and problem-solving. Enable code execution in the model configuration:

```typescript
const response = await ai.generate({
  model: googleAI.model('gemini-2.5-pro'),
  prompt: 'Use Python to calculate the 20th Fibonacci number.',
  config: {
    codeExecution: true,
  },
});

console.log(response.text);
// The response will include code execution results
```

### Google Search Grounding

Enable Google Search to provide answers with current information and verifiable sources. When enabled, the model automatically determines if a search is needed and executes queries to ground its responses in real-time web content.

#### Basic Usage

```typescript
const response = await ai.generate({
  model: googleAI.model('gemini-2.5-flash'),
  prompt: 'What are the top tech news stories this week?',
  config: {
    googleSearchRetrieval: true,
  },
});

console.log(response.text);
```

#### How It Works

When Google Search grounding is enabled:

1. **Prompt Analysis**: The model analyzes your prompt and determines if a Google Search can improve the answer
2. **Automatic Search**: If needed, the model generates and executes one or more search queries
3. **Result Processing**: The model processes search results and synthesizes the information
4. **Grounded Response**: Returns a response grounded in search results with citations and metadata

#### Accessing Grounding Metadata

The response includes detailed metadata about search queries, web sources, and citation information:

```typescript
const response = await ai.generate({
  model: googleAI.model('gemini-2.5-flash'),
  prompt: 'What are the latest developments in AI?',
  config: {
    googleSearchRetrieval: true,
  },
});

// Access grounding metadata
const groundingMetadata = (response.custom as any)?.candidates?.[0]?.groundingMetadata;

if (groundingMetadata) {
  // Search queries used
  console.log('Search queries:', groundingMetadata.webSearchQueries);
  
  // Web sources (chunks)
  console.log('Sources:', groundingMetadata.groundingChunks);
  
  // Citation mappings (links text segments to sources)
  console.log('Citations:', groundingMetadata.groundingSupports);
}
```

**Metadata Structure:**

- **`webSearchQueries`**: Array of search queries the model used
- **`groundingChunks`**: Array of web sources with `uri` and `title`
- **`groundingSupports`**: Maps text segments to their supporting sources
  - `segment`: The grounded text with `startIndex`, `endIndex`, and `text`
  - `groundingChunkIndices`: References to supporting chunks
  - `confidenceScores`: Confidence scores (0-1) for each source
- **`searchEntryPoint`**: Contains rendered HTML/CSS for search suggestions widget

Example metadata:

```json
{
  "groundingMetadata": {
    "webSearchQueries": ["latest AI developments"],
    "searchEntryPoint": {
      "renderedContent": "..."
    },
    "groundingChunks": [
      {"web": {"uri": "https://...", "title": "Source 1"}},
      {"web": {"uri": "https://...", "title": "Source 2"}}
    ],
    "groundingSupports": [
      {
        "segment": {
          "startIndex": 0,
          "endIndex": 89,
          "text": "Recent AI developments include..."
        },
        "groundingChunkIndices": [0, 1],
        "confidenceScores": [0.97, 0.85]
      }
    ]
  }
}
```

#### Combining with Other Tools

You can combine Google Search grounding with other tools like URL context:

```typescript
const response = await ai.generate({
  model: googleAI.model('gemini-2.5-flash'),
  prompt: 'Based on https://example.com, tell me about this topic. Also provide latest news.',
  config: {
    googleSearchRetrieval: true,
    tools: [
      {
        urlContext: {},
      },
    ],
  },
});
```

:::note
For detailed information about grounding metadata structure and building citation experiences, see the [Google AI Search Grounding documentation](https://ai.google.dev/gemini-api/docs/grounding).
:::

### Google Maps Grounding

Enable Google Maps to provide location-aware responses with accurate, up-to-date information from Google's extensive database of places. When enabled, the model can access Google Maps data to answer geographical queries with factual information about locations, businesses, and points of interest.

#### Basic Usage

```typescript
const response = await ai.generate({
  model: googleAI.model('gemini-2.5-flash'),
  prompt: 'What are the best Italian restaurants within a 15-minute walk from here?',
  config: {
    tools: [
      {
        googleMaps: {},
      },
    ],
    toolConfig: {
      retrievalConfig: {
        // Optionally provide location context (this is in Los Angeles)
        latLng: {
          latitude: 34.050481,
          longitude: -118.248526,
        },
      },
    },
  },
});

console.log(response.text);
```

#### How It Works

When Google Maps grounding is enabled:

1. **Query Analysis**: The model analyzes your prompt for geographical context
2. **Maps Integration**: If relevant, the model queries Google Maps for location data
3. **Data Retrieval**: Information about places, reviews, addresses, and other Maps data is retrieved
4. **Grounded Response**: Returns a response grounded in Google Maps data with citations

#### Accessing Grounding Metadata

The response includes detailed metadata about Maps sources and citations:

```typescript
const response = await ai.generate({
  model: googleAI.model('gemini-2.5-flash'),
  prompt: 'Find coffee shops near Times Square',
  config: {
    tools: [
      {
        googleMaps: {},
      },
    ],
    toolConfig: {
      retrievalConfig: {
        latLng: {
          latitude: 40.758896,
          longitude: -73.985130,
        },
      },
    },
  },
});

// Access grounding metadata
const groundingMetadata = (response.custom as any)?.candidates?.[0]?.groundingMetadata;

if (groundingMetadata) {
  // Maps sources (places)
  console.log('Sources:', groundingMetadata.groundingChunks);
  
  // Citation mappings (links text segments to sources)
  console.log('Citations:', groundingMetadata.groundingSupports);
  
  // Optional: Widget context token for rendering Maps widget
  if (groundingMetadata.googleMapsWidgetContextToken) {
    console.log('Widget token:', groundingMetadata.googleMapsWidgetContextToken);
  }
}
```

**Metadata Structure:**

- **`groundingChunks`**: Array of Maps sources with `maps.uri`, `maps.title`, and `maps.placeId`
- **`groundingSupports`**: Maps text segments to their supporting sources
  - `segment`: The grounded text with `startIndex`, `endIndex`, and `text`
  - `groundingChunkIndices`: References to supporting chunks
  - `confidenceScores`: Confidence scores (0-1) for each source
- **`googleMapsWidgetContextToken`**: Token for rendering a contextual Maps widget (when enabled)

#### Enabling the Maps Widget

To receive a widget context token for rendering an interactive Google Maps widget:

```typescript
const response = await ai.generate({
  model: googleAI.model('gemini-2.5-flash'),
  prompt: 'Plan a day in San Francisco. I want to see the Golden Gate Bridge, visit a museum, and have dinner.',
  config: {
    tools: [
      {
        googleMaps: {
          enableWidget: true,
        },
      },
    ],
    toolConfig: {
      retrievalConfig: {
        latLng: {
          latitude: 37.78193,
          longitude: -122.40476,
        },
      },
    },
  },
});

const widgetToken = (response.custom as any)?.candidates?.[0]?.groundingMetadata?.googleMapsWidgetContextToken;
if (widgetToken) {
  // Use with Google Maps JavaScript API to render widget
  console.log(`<gmp-place-contextual context-token="${widgetToken}"></gmp-place-contextual>`);
}
```

#### Combining with Other Tools

You can combine Google Maps grounding with other tools:

```typescript
const response = await ai.generate({
  model: googleAI.model('gemini-2.5-flash'),
  prompt: 'Based on https://example.com/travel-guide, recommend restaurants nearby.',
  config: {
    tools: [
      {
        googleMaps: {},
      },
      {
        urlContext: {},
      },
    ],
    toolConfig: {
      retrievalConfig: {
        latLng: {
          latitude: 34.050481,
          longitude: -118.248526,
        },
      },
    },
  },
});
```

:::note
For detailed information about grounding metadata structure, widget rendering, and usage requirements, see the [Google AI Maps Grounding documentation](https://ai.google.dev/gemini-api/docs/grounding#maps).
:::

### URL Context

The URL Context tool lets you provide additional context to the model in the form of URLs. By including URLs in your request, the model will access the content from those pages to inform and enhance its response.

The URL Context tool is useful for tasks like:

- **Extract Data**: Pull specific information like prices, names, or key findings from multiple URLs
- **Compare Documents**: Analyze multiple reports, articles, or PDFs to identify differences and track trends
- **Synthesize & Create Content**: Combine information from several source URLs to generate accurate summaries, blog posts, or reports
- **Analyze Code & Docs**: Point to a GitHub repository or technical documentation to explain code, generate setup instructions, or answer questions

#### Basic Usage

```typescript
const url1 = 'https://www.foodnetwork.com/recipes/ina-garten/perfect-roast-chicken-recipe-1940592';
const url2 = 'https://www.allrecipes.com/recipe/21151/simple-whole-roast-chicken/';

const response = await ai.generate({
  model: googleAI.model('gemini-2.5-flash'),
  prompt: `Compare the ingredients and cooking times from the recipes at ${url1} and ${url2}`,
  config: {
    tools: [
      {
        urlContext: {},
      },
    ],
  },
});

console.log(response.text);

// For verification, you can inspect the metadata to see which URLs the model retrieved
const urlMetadata = (response.custom as any)?.candidates?.[0]?.urlContextMetadata;
console.log(urlMetadata);
```

#### How It Works

The URL Context tool uses a two-step retrieval process to balance speed, cost, and access to fresh data:

1. **Index Cache Lookup**: First attempts to fetch content from an internal index cache (highly optimized)
2. **Live Fetch Fallback**: If the URL is not in the index (e.g., very new pages), automatically performs a live fetch to retrieve content in real-time

#### Accessing URL Context Metadata

When the model uses the URL Context tool, the response includes a `urlContextMetadata` object that lists the URLs retrieved and their retrieval status:

```typescript
const response = await ai.generate({
  model: googleAI.model('gemini-2.5-flash'),
  prompt: 'Summarize the content from https://example.com/article',
  config: {
    tools: [
      {
        urlContext: {},
      },
    ],
  },
});

const urlMetadata = (response.custom as any)?.candidates?.[0]?.urlContextMetadata;

if (urlMetadata?.urlMetadata) {
  urlMetadata.urlMetadata.forEach((url: any) => {
    console.log('URL:', url.retrievedUrl);
    console.log('Status:', url.urlRetrievalStatus);
  });
}
```

**Metadata Structure:**

- **`urlMetadata`**: Array of URL retrieval information
  - `retrievedUrl`: The URL that was accessed
  - `urlRetrievalStatus`: Status of the retrieval (e.g., `URL_RETRIEVAL_STATUS_SUCCESS`, `URL_RETRIEVAL_STATUS_UNSAFE`)

#### Combining with Other Tools

You can combine URL Context with other tools like Google Search grounding or Google Maps to create more powerful workflows:

```typescript
const response = await ai.generate({
  model: googleAI.model('gemini-2.5-flash'),
  prompt: 'Give me a three day events schedule based on https://example.com/events. Also let me know what needs to be taken care of considering weather and commute.',
  config: {
    googleSearchRetrieval: true,
    tools: [
      {
        urlContext: {},
      },
    ],
  },
});

console.log(response.text);

// Get URLs retrieved for context
const urlMetadata = (response.custom as any)?.candidates?.[0]?.urlContextMetadata;
console.log(urlMetadata);
```

When both URL Context and Google Search grounding are enabled, the model can use its search capabilities to find relevant information online and then use the URL Context tool to get a more in-depth understanding of the pages it finds.

#### Token Count and Pricing

The content retrieved from URLs counts as input tokens. You can see the token count in the `usageMetadata` object:

```typescript
const response = await ai.generate({
  model: googleAI.model('gemini-2.5-flash'),
  prompt: 'Analyze https://example.com',
  config: {
    tools: [{ urlContext: {} }],
  },
});

console.log('Usage:', response.usage);
// toolUsePromptTokenCount will include tokens from URL content
```

#### Best Practices

- **Provide specific URLs**: For best results, provide direct URLs to the content you want the model to analyze. The model will only retrieve content from the URLs you provide, not from nested links
- **Check for accessibility**: Verify that URLs don't lead to pages requiring login or behind paywalls
- **Use complete URLs**: Provide the full URL including the protocol (e.g., `https://www.example.com` instead of just `example.com`)

#### Limitations

- **Request limit**: Up to 20 URLs per request
- **URL content size**: Maximum 34MB per URL
- **Safety checks**: URLs are checked for content moderation. Unsafe URLs will have status `URL_RETRIEVAL_STATUS_UNSAFE`

**Supported content types:**
- Text (HTML, JSON, plain text, XML, CSS, JavaScript, CSV, RTF)
- Images (PNG, JPEG, BMP, WebP)
- PDF documents

**Unsupported content types:**
- Paywalled content
- YouTube videos (use video understanding feature instead)
- Google Workspace files (Docs, Sheets, etc.)
- Video and audio files

:::note
This feature is supported for Gemini 2.5 Pro, Gemini 2.5 Flash, and Gemini 2.5 Flash Lite models. For more details, see the [Google AI URL Context documentation](https://ai.google.dev/gemini-api/docs/url-context).
:::

### Image Generation Outputs

Models with image generation capabilities can create images:

```typescript
const response = await ai.generate({
  model: googleAI.model('gemini-2.5-flash-image'),
  prompt: 'Create a picture of a futuristic city at sunset',
  config: {
    responseModalities: ['IMAGE'],
    imageConfig: {
      aspectRatio: '16:9', // Options: 1:1, 2:3, 3:2, 3:4, 4:3, 4:5, 5:4, 9:16, 16:9, 21:9
    },
  },
});

const generatedImage = response.media;
```

### Thinking Config

#### Thinking Level (Gemini 3.0)

```typescript
const response = await ai.generate({
  model: googleAI.model('gemini-3-pro-preview'),
  prompt: 'what is heavier, one kilo of steel or one kilo of feathers',
    config: {
      thinkingConfig: {
        thinkingLevel: 'HIGH',  // Or 'LOW' or 'MEDIUM'
    },
  },
});
```

#### Thinking Budget (Gemini 2.5)

```typescript
const { message } = await ai.generate({
  model: googleAI.model('gemini-2.5-pro'),
  prompt: 'what is heavier, one kilo of steel or one kilo of feathers',
  config: {
    thinkingConfig: {
      thinkingBudget: 1024,
      includeThoughts: true,
    },
  },
});
```

## Gemini API Features

The following features are available through the `googleAI` plugin.

### Gemini Files API

The Files API allows you to upload media files (images, audio, video, documents) larger than 20 MB to use with Gemini models. Files are automatically deleted after 48 hours.

#### Upload and Use a File

```ts
import { GoogleGenAI, createUserContent, createPartFromUri } from '@google/genai';
import { genkit } from 'genkit';
import { googleAI } from '@genkit-ai/google-genai';

const ai = genkit({
  plugins: [googleAI()],
});

// Initialize the Google GenAI client
const genaiClient = new GoogleGenAI({});

// Upload a file
const uploadedFile = await genaiClient.files.upload({
  file: 'path/to/sample.mp3',
  config: { mimeType: 'audio/mpeg' },
});

// Use the uploaded file with Genkit
const response = await ai.generate({
  model: googleAI.model('gemini-2.5-flash'),
  prompt: [
    { text: 'Describe this audio clip' },
    {
      media: {
        contentType: uploadedFile.mimeType,
        url: uploadedFile.uri,
      },
    },
  ],
});

console.log(response.text);
```

#### Get File Metadata

Verify file upload and retrieve metadata:

```ts
const fileName = uploadedFile.name;
const fileMetadata = await genaiClient.files.get({ name: fileName });
console.log(fileMetadata);
```

#### List Uploaded Files

Get a list of all uploaded files:

```ts
const listResponse = await genaiClient.files.list({ config: { pageSize: 10 } });
for await (const file of listResponse) {
  console.log(file.name);
}
```

#### Delete a File

Manually delete an uploaded file (files auto-delete after 48 hours):

```ts
await genaiClient.files.delete({ name: fileName });
```

:::note
**Usage Limits:**
- Maximum 20 GB of files per project
- Maximum 2 GB per individual file
- Files automatically deleted after 48 hours
- Use Files API when total request size (including files, text, system instructions) exceeds 20 MB

For more details, see the [Google AI Files API documentation](https://ai.google.dev/gemini-api/docs/files).
:::

### Video Generation (Veo) Models

The Google Generative AI plugin provides access to video generation capabilities through the Veo models. These models can generate videos from text prompts or manipulate existing images to create dynamic video content.

#### Basic Usage: Text-to-Video Generation

To generate a video from a text prompt using the Veo model:

```ts
import { googleAI } from '@genkit-ai/google-genai';
import * as fs from 'fs';
import { Readable } from 'stream';
import { MediaPart } from 'genkit';
import { genkit } from 'genkit';

const ai = genkit({
  plugins: [googleAI()],
});

ai.defineFlow('text-to-video-veo', async () => {
  let { operation } = await ai.generate({
    model: googleAI.model('veo-2.0-generate-001'),
    prompt: 'A majestic dragon soaring over a mystical forest at dawn.',
    config: {
      durationSeconds: 5,
      aspectRatio: '16:9',
    },
  });

  if (!operation) {
    throw new Error('Expected the model to return an operation');
  }

  // Wait until the operation completes.
  while (!operation.done) {
    operation = await ai.checkOperation(operation);
    // Sleep for 5 seconds before checking again.
    await new Promise((resolve) => setTimeout(resolve, 5000));
  }

  if (operation.error) {
    throw new Error('failed to generate video: ' + operation.error.message);
  }

  const video = operation.output?.message?.content.find((p) => !!p.media);
  if (!video) {
    throw new Error('Failed to find the generated video');
  }
  await downloadVideo(video, 'output.mp4');
});

async function downloadVideo(video: MediaPart, path: string) {
  const fetch = (await import('node-fetch')).default;
  // Add API key before fetching the video.
  const videoDownloadResponse = await fetch(`${video.media!.url}&key=${process.env.GEMINI_API_KEY}`);
  if (!videoDownloadResponse || videoDownloadResponse.status !== 200 || !videoDownloadResponse.body) {
    throw new Error('Failed to fetch video');
  }

  Readable.from(videoDownloadResponse.body).pipe(fs.createWriteStream(path));
}
```

Veo 3 uses the exact same API, just make sure you only use supported config options (see below).

To use the Veo 3 model, reference `veo-3.0-generate-001`:

```ts
let { operation } = await ai.generate({
  model: googleAI.model('veo-3.0-generate-001'),
  prompt: 'A cinematic shot of a an old car driving down a deserted road at sunset.',
});
```

For faster video generation, you can use the Veo 3 Fast model:

```ts
let { operation } = await ai.generate({
  model: googleAI.model('veo-3.0-fast-generate-001'),
  prompt: 'A cinematic shot of a an old car driving down a deserted road at sunset.',
});
```

#### Video Generation from Photo Reference

To use a photo as reference for the video using the Veo model (e.g. to make a static photo move), you can provide an image as part of the prompt.

```ts
const startingImage = fs.readFileSync('photo.jpg', { encoding: 'base64' });

let { operation } = await ai.generate({
  model: googleAI.model('veo-2.0-generate-001'),
  prompt: [
    {
      text: 'make the subject in the photo move',
    },
    {
      media: {
        contentType: 'image/jpeg',
        url: `data:image/jpeg;base64,${startingImage}`,
      },
    },
  ],
  config: {
    durationSeconds: 5,
    aspectRatio: '9:16',
    personGeneration: 'allow_adult',
  },
});
```

#### Configuration Options

The Veo models support various configuration options.

##### Veo Model Parameters

Full list of options can be found at https://ai.google.dev/gemini-api/docs/video#veo-model-parameters

- `negativePrompt`: Text string that describes anything you want to discourage the model from generating
- `aspectRatio`: Changes the aspect ratio of the generated video.
  - `"16:9"`: Supported in Veo 3 and Veo 2 (default for both).
  - `"9:16"`: Supported in Veo 2 only.
- `resolution`: Video resolution (Veo 3 only).
  - `"720p"`: 720p resolution (default, supported for both 16:9 and 9:16).
  - `"1080p"`: 1080p resolution (only supported for 16:9 aspect ratio).
- `personGeneration`: Allow the model to generate videos of people. The following values are supported:
  - **Text-to-video generation**:
    - `"allow_all"`: Generate videos that include adults and children. Currently the only available `personGeneration` value for Veo 3.
    - `"dont_allow"`: Veo 2 only. Don't allow the inclusion of people or faces.
    - `"allow_adult"`: Veo 2 only. Generate videos that include adults, but not children.
  - **Image-to-video generation**: Veo 2 only
    - `"dont_allow"`: Don't allow the inclusion of people or faces.
    - `"allow_adult"`: Generate videos that include adults, but not children.
- `numberOfVideos`: Output videos requested
  - `1`: Supported in Veo 3 and Veo 2
  - `2`: Supported in Veo 2 only.
- `durationSeconds`: Veo 2 only. Length of each output video in seconds, between 5 and 8. Not configurable for Veo 3, default setting is 8 seconds.
- `enhancePrompt`: Veo 2 only. Enable or disable the prompt rewriter. Enabled by default. Not configurable for Veo 3, default prompt enhancer is always on.

### Text-to-Speech (TTS) Models

The Google Genai plugin provides access to text-to-speech capabilities through Gemini TTS models. These models can convert text into natural-sounding speech for various applications.

#### Basic Usage

To generate audio using a TTS model:

```ts
import { googleAI } from '@genkit-ai/google-genai';
import { writeFile } from 'node:fs/promises';
import wav from 'wav'; // npm install wav && npm install -D @types/wav

const ai = genkit({
  plugins: [googleAI()],
});

const { media } = await ai.generate({
  model: googleAI.model('gemini-2.5-flash-preview-tts'),
  config: {
    responseModalities: ['AUDIO'],
    speechConfig: {
      voiceConfig: {
        prebuiltVoiceConfig: { voiceName: 'Algenib' },
      },
    },
  },
  prompt: 'Say that Genkit is an amazing Gen AI library',
});

if (!media) {
  throw new Error('no media returned');
}
const audioBuffer = Buffer.from(media.url.substring(media.url.indexOf(',') + 1), 'base64');
// The googleAI plugin returns raw PCM data, which we convert to WAV format.
await writeFile('output.wav', await toWav(audioBuffer));

async function toWav(pcmData: Buffer, channels = 1, rate = 24000, sampleWidth = 2): Promise<string> {
  return new Promise((resolve, reject) => {
    // This code depends on `wav` npm library.
    const writer = new wav.Writer({
      channels,
      sampleRate: rate,
      bitDepth: sampleWidth * 8,
    });

    let bufs = [] as any[];
    writer.on('error', reject);
    writer.on('data', function (d) {
      bufs.push(d);
    });
    writer.on('end', function () {
      resolve(Buffer.concat(bufs).toString('base64'));
    });

    writer.write(pcmData);
    writer.end();
  });
}
```

#### Multi-speaker Audio Generation

You can generate audio with multiple speakers, each with their own voice:

```ts
const response = await ai.generate({
  model: googleAI.model('gemini-2.5-flash-preview-tts'),
  config: {
    responseModalities: ['AUDIO'],
    speechConfig: {
      multiSpeakerVoiceConfig: {
        speakerVoiceConfigs: [
          {
            speaker: 'Speaker1',
            voiceConfig: {
              prebuiltVoiceConfig: { voiceName: 'Algenib' },
            },
          },
          {
            speaker: 'Speaker2',
            voiceConfig: {
              prebuiltVoiceConfig: { voiceName: 'Achernar' },
            },
          },
        ],
      },
    },
  },
  prompt: `Here's the dialog:
    Speaker1: "Genkit is an amazing Gen AI library!"
    Speaker2: "I thought it was a framework."`,
});
```

When using multi-speaker configuration, the model automatically detects speaker labels in the text (like "Speaker1:" and "Speaker2:") and applies the corresponding voice to each speaker's lines.

#### Configuration Options

The Gemini TTS models support various configuration options:

##### Voice Selection

You can choose from different pre-built voices with unique characteristics:

```ts
speechConfig: {
  voiceConfig: {
    prebuiltVoiceConfig: {
      voiceName: 'Algenib' // Other options: 'Achernar', 'Ankaa', etc.
    },
  },
}
```

##### Speech Emphasis

You can use markdown-style formatting in your prompt to add emphasis:

- Bold text (`**like this**`) for stronger emphasis
- Italic text (`*like this*`) for moderate emphasis

Example:

```ts
prompt: 'Genkit is an **amazing** Gen AI *library*!';
```

##### Advanced Speech Parameters

For more control over the generated speech:

```ts
speechConfig: {
  voiceConfig: {
    prebuiltVoiceConfig: {
      voiceName: 'Algenib',
      speakingRate: 1.0,  // Range: 0.25 to 4.0, default is 1.0
      pitch: 0.0,         // Range: -20.0 to 20.0, default is 0.0
      volumeGainDb: 0.0,  // Range: -96.0 to 16.0, default is 0.0
    },
  },
}
```

- `speakingRate`: Controls the speed of speech (higher values = faster speech)
- `pitch`: Adjusts the pitch of the voice (higher values = higher pitch)
- `volumeGainDb`: Controls the volume (higher values = louder)

For more detailed information about the Gemini TTS models and their configuration options, see the [Google AI Speech Generation documentation](https://ai.google.dev/gemini-api/docs/speech-generation).

## Troubleshooting

### Schema Limitations

The Google Generative AI API uses a subset of the OpenAPI 3.0 schema, which does not support certain features like unions. If you encounter schema-related errors, you may need to adjust your schema:

```typescript
// Error example:
// GenerateContentRequest.generation_config.response_schema.properties[field].type: must be specified

// Workaround: Simplify schema or avoid unsupported features
const response = await ai.generate({
  model: googleAI.model('gemini-2.5-flash'),
  output: {
    schema: z.object({
      name: z.string(),
      age: z.number(),
      // Avoid z.union, z.record, and other unsupported features
    }),
  },
  prompt: 'Generate a person profile',
});
```

Known Zod features that may not work:
- `z.union()` - Use alternatives like separate fields
- `z.record()` - Use explicit object properties instead

### Rate Limits

If you encounter rate limit errors, consider:
- Implementing exponential backoff retry logic
- Using batch operations where available
- Upgrading your API quota in Google AI Studio

### API Key Issues

Ensure your API key is properly configured:
- Set the `GEMINI_API_KEY` or `GOOGLE_API_KEY` environment variable
- Or pass the key directly in plugin configuration: `googleAI({ apiKey: 'your-key' })`
- Get your API key from [Google AI Studio](https://aistudio.google.com/apikey)

## Next Steps

- Learn about [generating content](/docs/models) to understand how to use these models effectively
- Explore [creating flows](/docs/flows) to build structured AI workflows
- To use the Gemini API at enterprise scale or leverage Vertex vector search and Model Garden, see the [Vertex AI plugin](/docs/integrations/vertex-ai)

</LanguageContent>

<LanguageContent lang="go">

The Google Generative AI plugin provides interfaces to Google's Gemini models through the Gemini API.

## Configuration

To use this plugin, import the `googlegenai` package and pass
`googlegenai.GoogleAI` to `WithPlugins()` in the Genkit initializer:

```go
import "github.com/firebase/genkit/go/plugins/googlegenai"
```

```go
g := genkit.Init(context.Background(), genkit.WithPlugins(&googlegenai.GoogleAI{}))
```

The plugin requires an API key for the Gemini API, which you can get from
[Google AI Studio](https://aistudio.google.com/app/apikey).

Configure the plugin to use your API key by doing one of the following:

- Set the `GEMINI_API_KEY` environment variable to your API key.

- Specify the API key when you initialize the plugin:

  ```go
  genkit.WithPlugins(&googlegenai.GoogleAI{APIKey: "YOUR_API_KEY"})
  ```

  However, don't embed your API key directly in code! Use this feature only
  in conjunction with a service like Cloud Secret Manager or similar.

## Usage

### Generative models

To get a reference to a supported model, specify its identifier to `googlegenai.GoogleAIModel`:

```go
model := googlegenai.GoogleAIModel(g, "gemini-2.5-flash")
```

Alternatively, you may create a `ModelRef` which pairs the model name with its config:

```go
modelRef := googlegenai.GoogleAIModelRef("gemini-2.5-flash", &genai.GenerateContentConfig{
	Temperature: genai.Ptr[float32](0.5),
	MaxOutputTokens: genai.Ptr[int32](500),
	// Other configuration...
})
```

Model references have a `Generate()` method that calls the Google API:

```go
resp, err := genkit.Generate(ctx, g, ai.WithModel(modelRef), ai.WithPrompt("Tell me a joke."))
if err != nil {
	return err
}

log.Println(resp.Text())
```

See [Generating content with AI models](/docs/models) for more information.

### Embedding models

To get a reference to a supported embedding model, specify its identifier to `googlegenai.GoogleAIEmbedder`:

```go
embeddingModel := googlegenai.GoogleAIEmbedder(g, "text-embedding-004")
```

Embedder references have an `Embed()` method that calls the Google AI API:

```go
resp, err := genkit.Embed(ctx, g, ai.WithEmbedder(embeddingModel), ai.WithTextDocs(userInput))
if err != nil {
	return err
}
```

See [Retrieval-augmented generation (RAG)](/docs/rag) for more information.

## Next Steps

- Learn about [generating content](/docs/models) to understand how to use these models effectively
- Explore [creating flows](/docs/flows) to build structured AI workflows
- To use the Gemini API at enterprise scale see the [Vertex AI plugin](/docs/integrations/vertex-ai)

</LanguageContent>

<LanguageContent lang="python">

The `genkit-plugin-google-genai` package provides the `GoogleAI` plugin for accessing Google's generative AI models via the Google Gemini API (requires an API key).

## Installation

```bash
pip3 install genkit-plugin-google-genai
```

## Configuration

To use the Google Gemini API, you need an API key.

```python
from genkit.ai import Genkit
from genkit.plugins.google_genai import GoogleAI

ai = Genkit(
  plugins=[GoogleAI()],
  model='googleai/gemini-2.5-flash',
)
```

You will need to set GEMINI_API_KEY environment variable or you can provide the API Key directly:

```python
ai = Genkit(
  plugins=[GoogleAI(api_key='...')]
)
```

## Usage

### Text Generation

```python
response = await ai.generate('What should I do when I visit Melbourne?')
print(response.text)
```

### Text Embedding

```python
embeddings = await ai.embed(
    embedder='googleai/text-embedding-004',
    content='How many widgets do you have in stock?',
)
```

### Image Generation

```python
response = await ai.generate(
    model='googleai/imagen-4.0-generate-001',
    prompt='a banana riding a bicycle',
)
```

## Next Steps

- Learn about [generating content](/docs/models) to understand how to use these models effectively
- Explore [creating flows](/docs/flows) to build structured AI workflows
- To use the Gemini API at enterprise scale see the [Vertex AI plugin](/docs/integrations/vertex-ai)

</LanguageContent>
