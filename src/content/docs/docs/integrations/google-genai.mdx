---
title: Google Generative AI plugin
description: Learn how to use Google's Generative AI models across JavaScript, Go, and Python through the Gemini Developer API, including text generation, embeddings, image generation, video generation, and text-to-speech.
---

import LanguageSelector from '../../../../components/LanguageSelector.astro';
import CopyMarkdownButton from '../../../../components/CopyMarkdownButton.astro';
import LanguageContent from '../../../../components/LanguageContent.astro';

<div style="display: flex; justify-content: space-between; align-items: center; gap: 1rem; margin: 1rem 0 1rem 0;">
  <LanguageSelector />
  <CopyMarkdownButton />
</div>

<LanguageContent lang="js">

The Google AI plugin provides a unified interface to connect with Google's generative AI models through the **Gemini Developer API** using API key authentication. The `@genkit-ai/google-genai` package is a drop-in replacement for the previous `@genkit-ai/googleai` package.

The plugin supports a wide range of capabilities:
- **Language Models**: Gemini models for text generation, reasoning, and multimodal tasks
- **Embedding Models**: Text and multimodal embeddings
- **Image Models**: Imagen for generation and Gemini for image analysis
- **Video Models**: Veo for video generation and Gemini for video understanding
- **Speech Models**: Polyglot text-to-speech generation

## Setup

### Installation

```bash
npm i --save @genkit-ai/google-genai
```

### Configuration

```typescript
import { genkit } from 'genkit';
import { googleAI } from '@genkit-ai/google-genai';

const ai = genkit({
  plugins: [
    googleAI(),
    // Or with an explicit API key:
    // googleAI({ apiKey: 'your-api-key' }),
  ],
});
```

### Authentication

Requires a Gemini API Key, which you can get from [Google AI Studio](https://aistudio.google.com/apikey). You can provide this key in several ways:

1. **Environment variables**: Set `GEMINI_API_KEY`
2. **Plugin configuration**: Pass `apiKey` when initializing the plugin (shown above)
3. **Per-request**: Override the API key for specific requests in the config:

```typescript
const response = await ai.generate({
  model: googleAI.model('gemini-2.5-flash'),
  prompt: 'Your prompt here',
  config: {
    apiKey: 'different-api-key', // Use a different API key for this request
  },
});
```

This per-request API key option is useful for routing specific requests to different API keys, such as for multi-tenant applications or cost tracking.

## Language Models

You can create models that call the Google Generative AI API. The models support tool calls and some have multi-modal capabilities.

### Available Models

**Gemini 3 Series** - Latest experimental models with state-of-the-art reasoning:
- `gemini-3-pro-preview` - Preview of the most capable model for complex tasks
- `gemini-3-flash-preview` - Fast and intelligent model for high-volume tasks
- `gemini-3-pro-image-preview` - Supports image generation outputs

**Gemini 2.5 Series** - Latest stable models with advanced reasoning and multimodal capabilities:
- `gemini-2.5-pro` - Most capable stable model for complex tasks
- `gemini-2.5-flash` - Fast and efficient for most use cases
- `gemini-2.5-flash-lite` - Lightweight version for simple tasks
- `gemini-2.5-flash-image` - Supports image generation outputs

**Gemma 3 Series** - Open models for various use cases:
- `gemma-3-27b-it` - Large instruction-tuned model
- `gemma-3-12b-it` - Medium instruction-tuned model
- `gemma-3-4b-it` - Small instruction-tuned model
- `gemma-3-1b-it` - Tiny instruction-tuned model
- `gemma-3n-e4b-it` - Efficient 4-bit model

:::note
See the [Google Generative AI models documentation](https://ai.google.dev/gemini-api/docs/models) for a complete list of available models and their capabilities.
:::

### Basic Usage

```typescript
import { genkit } from 'genkit';
import { googleAI } from '@genkit-ai/google-genai';

const ai = genkit({
  plugins: [googleAI()],
});

const response = await ai.generate({
  model: googleAI.model('gemini-2.5-flash'),
  prompt: 'Explain how neural networks learn in simple terms.',
});

console.log(response.text);
```

### Structured Output

Gemini models support structured output generation, which guarantees that the model output will conform to a specified JSON schema.

```typescript
import { z } from 'genkit';

const response = await ai.generate({
  model: googleAI.model('gemini-2.5-flash'),
  output: {
    schema: z.object({
      name: z.string(),
      bio: z.string(),
      age: z.number(),
    }),
  },
  prompt: 'Generate a profile for a fictional character',
});

console.log(response.output);
```

#### Schema Limitations

The Gemini API relies on a specific subset of the OpenAPI 3.0 standard. When defining Zod schemas for structured output, keep the following limitations in mind:

**Supported Features**
- **Objects & Arrays**: Standard object properties and array items.
- **Enums**: Fully supported (`z.enum`).
- **Nullable**: Supported via `z.nullable()` (mapped to `nullable: true`).

**Critical Limitations**
- **Unions (`z.union`)**: Complex unions are often problematic. The API has specific handling for `anyOf` but may reject ambiguous or complex `oneOf` structures. Prefer using a single object with optional fields or distinct tool definitions over complex unions.
- **Validation Keywords**: Keywords like `pattern`, `minLength`, `maxLength`, `minItems`, and `maxItems` are **not supported** by the Gemini API's constrained decoding. Including them may result in `400 InvalidArgument` errors or them being ignored.
- **Recursion**: Recursive schemas are generally not supported.
- **Complexity**: Deeply nested schemas or schemas with hundreds of properties may trigger complexity limits.

**Best Practices**
- Keep schemas simple and flat where possible.
- Use property descriptions (`.describe()`) to guide the model instead of complex validation rules (e.g., "String must be an email" instead of a regex pattern).
- If you need strict validation (e.g., regex), perform it in your application code *after* receiving the structured response.

### Thinking and Reasoning

Gemini 2.5 and newer models use an internal thinking process that improves reasoning for complex tasks.

**Thinking Level (Gemini 3.0):**

```typescript
const response = await ai.generate({
  model: googleAI.model('gemini-3-pro-preview'),
  prompt: 'what is heavier, one kilo of steel or one kilo of feathers',
    config: {
      thinkingConfig: {
        thinkingLevel: 'HIGH',  // Or 'LOW' or 'MEDIUM'
    },
  },
});
```

**Thinking Budget (Gemini 2.5):**

```typescript
const response = await ai.generate({
  model: googleAI.model('gemini-2.5-pro'),
  prompt: 'what is heavier, one kilo of steel or one kilo of feathers',
  config: {
    thinkingConfig: {
      thinkingBudget: 8192, // Number of thinking tokens
      includeThoughts: true, // Include thought summaries
    },
  },
});

if (response.reasoning) {
  console.log('Reasoning:', response.reasoning);
}
```

### Context Caching

Gemini 2.5 and newer models automatically cache common content prefixes (min 1024 tokens for Flash, 2048 for Pro), providing a 75% token discount on cached tokens.

```typescript
// Structure prompts with consistent content at the beginning
const baseContext = `You are a helpful cook... (large context) ...`.repeat(50);

// First request - content will be cached
await ai.generate({
  model: googleAI.model('gemini-2.5-pro'),
  prompt: `${baseContext}\n\nTask 1...`,
});

// Second request with same prefix - eligible for cache hit
await ai.generate({
  model: googleAI.model('gemini-2.5-pro'),
  prompt: `${baseContext}\n\nTask 2...`,
});
```

### Safety Settings

You can configure safety settings to control content filtering for different harm categories:

```typescript
const response = await ai.generate({
  model: googleAI.model('gemini-2.5-flash'),
  prompt: 'Your prompt here',
  config: {
    safetySettings: [
      {
        category: 'HARM_CATEGORY_HATE_SPEECH',
        threshold: 'BLOCK_MEDIUM_AND_ABOVE',
      },
      {
        category: 'HARM_CATEGORY_DANGEROUS_CONTENT',
        threshold: 'BLOCK_MEDIUM_AND_ABOVE',
      },
    ],
  },
});
```

Available harm categories:
- `HARM_CATEGORY_HATE_SPEECH`
- `HARM_CATEGORY_DANGEROUS_CONTENT`
- `HARM_CATEGORY_HARASSMENT`
- `HARM_CATEGORY_SEXUALLY_EXPLICIT`

Available thresholds:
- `HARM_BLOCK_THRESHOLD_UNSPECIFIED`
- `BLOCK_LOW_AND_ABOVE`
- `BLOCK_MEDIUM_AND_ABOVE`
- `BLOCK_ONLY_HIGH`
- `BLOCK_NONE`

**Accessing Safety Ratings:**

Safety ratings are typically only included when content is flagged. You can access them from the response custom metadata:

```typescript
const geminiResponse = response.custom as any;
const candidateSafetyRatings = geminiResponse?.candidates?.[0]?.safetyRatings;
const promptSafetyRatings = geminiResponse?.promptFeedback?.safetyRatings;
```

### Google Search Grounding

Enable Google Search to provide answers with current information and verifiable sources.

```typescript
const response = await ai.generate({
  model: googleAI.model('gemini-2.5-flash'),
  prompt: 'What are the top tech news stories this week?',
  config: {
    googleSearchRetrieval: true,
  },
});

// Access grounding metadata
const groundingMetadata = (response.custom as any)?.candidates?.[0]?.groundingMetadata;
if (groundingMetadata) {
  console.log('Sources:', groundingMetadata.groundingChunks);
}
```

The following configuration options are available for Google Search grounding:

- **googleSearchRetrieval** _object | boolean_

  Enables Google Search grounding. Can be a boolean (`true`) or a configuration object.
  Example: `{ dynamicRetrievalConfig: { mode: 'MODE_DYNAMIC', dynamicThreshold: 0.7 } }`

  - **dynamicRetrievalConfig** _object_
    - **mode** _string_
      The retrieval mode (e.g., `'MODE_DYNAMIC'`).
    - **dynamicThreshold** _number_
      The threshold for dynamic retrieval (e.g., `0.7`).

**Response Metadata:**

- **webSearchQueries** _string[]_

  Array of search queries used to retrieve information.
  Example: `["What's the weather in Chicago this weekend?"]`

- **searchEntryPoint** _object_

  Contains the main search result content formatted for display.

  - **renderedContent** _string_
    The HTML content of the search result.

- **groundingSupports** _object[]_

  Links specific response segments to supporting search result chunks.

  - **segment** _object_
    - **text** _string_
      The text of the segment.
  - **groundingChunkIndices** _number[]_
    Indices of the chunks that support this segment.
  - **confidenceScores** _number[]_
    Confidence scores for each supporting chunk.

### Google Maps Grounding

Enable Google Maps to provide location-aware responses.

```typescript
const response = await ai.generate({
  model: googleAI.model('gemini-2.5-flash'),
  prompt: 'Find coffee shops near Times Square',
  config: {
    tools: [{ googleMaps: {} }],
  },
});
```

You can also request a widget token to render an interactive map:

```typescript
const response = await ai.generate({
  model: googleAI.model('gemini-2.5-flash'),
  prompt: 'Show me a map of San Francisco',
  config: {
    tools: [{ googleMaps: { enableWidget: true } }],
  },
});
```

The following configuration options are available for Google Maps grounding:

- **googleMaps** _object_

  Enables Google Maps grounding.
  Example: `{ enableWidget: true }`

  - **enableWidget** _boolean_
    Whether to include a widget token in the response.

- **toolConfig** _object_

  Additional configuration for provider tools. Can improve relevance by providing location context for Google Maps.
  Example: `{ retrievalConfig: { latLng: { latitude: 37.7749, longitude: -122.4194 } } }`

  - **retrievalConfig** _object_
    - **latLng** _object_
      - **latitude** _number_
        The latitude in degrees.
      - **longitude** _number_
        The longitude in degrees.

```typescript
const response = await ai.generate({
  model: googleAI.model('gemini-2.5-flash'),
  prompt: 'Find coffee shops near my current location.',
  config: {
    tools: [{ googleMaps: {} }],
    toolConfig: {
      retrievalConfig: {
        latLng: {
          latitude: 37.7749,
          longitude: -122.4194,
        },
      },
    }
  },
});
```

### URL Context

Provide specific URLs for the model to analyze:

```typescript
const response = await ai.generate({
  model: googleAI.model('gemini-2.5-flash'),
  prompt: 'Summarize this page',
  config: {
    tools: [{ urlContext: {} }],
  },
});
```

When using `urlContext`, the model will fetch content from URLs found in your prompt.

### Code Execution

Enable the model to write and execute Python code for calculations and logic.

```typescript
const response = await ai.generate({
  model: googleAI.model('gemini-2.5-pro'),
  prompt: 'Calculate the 20th Fibonacci number',
  config: {
    codeExecution: true,
  },
});
```

The following configuration options are available for code execution:

- **codeExecution** _boolean_

  Enables code execution for reasoning and calculations.
  Example: `true`

### Generating Text and Images (Nano Banana)

Some Gemini models (like `gemini-2.5-flash-image`) can output images natively alongside text:

```typescript
const response = await ai.generate({
  model: googleAI.model('gemini-2.5-flash-image'),
  prompt: 'Create a picture of a futuristic city and describe it',
  config: {
    responseModalities: ['IMAGE', 'TEXT'],
  },
});

// Extract image
if (response.image) {
  console.log('Image:', response.image);
}

// Extract text
if (response.text) {
  console.log('Text:', response.text);
}

// Extract all messages including text and images
if (response.messages) {
  console.log('Messages:', response.messages);
}
```

The following configuration options are available for Gemini image generation:

- **responseModalities** _string[]_

  Specifies the output modalities.
  Options: `['TEXT', 'IMAGE']`, `['IMAGE']`
  Default: `['TEXT', 'IMAGE']`

- **imageConfig** _object_

  - **aspectRatio** _string_

    Aspect ratio of the generated images.
    Options: `'1:1'`, `'3:2'`, `'2:3'`, `'3:4'`, `'4:3'`, `'4:5'`, `'5:4'`, `'9:16'`, `'16:9'`, `'21:9'`
    Default: `'1:1'`

  - **imageSize** _string_

    Resolution of the generated image.
    Supported by `gemini-3-pro-image-preview` only.
    Options: `'1K'`, `'2K'`, `'4K'`
    Default: `'1K'`

### Multimodal Input Capabilities

#### Video Understanding

Gemini models can process videos to describe content, answer questions, and refer to timestamps (in `MM:SS` format).

```typescript
const response = await ai.generate({
  model: googleAI.model('gemini-2.5-flash'),
  prompt: [
    { text: 'What happens at 00:05?' },
    { media: { contentType: 'video/mp4', url: 'https://youtube.com/watch?v=...' } },
  ],
});
```

**Video Processing Details:**
- **Sampling**: 1 frame per second (default)
- **Context**: 2M context models can handle up to 2 hours of video.
- **Inputs**: Up to 10 videos per request (Gemini 2.5+).

#### Image Understanding

Gemini models can reason about images passed as inline data or URLs.

```typescript
const response = await ai.generate({
  model: googleAI.model('gemini-2.5-flash'),
  prompt: [
    { text: 'Describe what is in this image' },
    { media: { url: 'https://example.com/image.jpg' } },
  ],
});
```

#### Audio Understanding

Gemini models can process audio files to transcribe speech text, answer questions about the audio content, or summarize recordings.

```typescript
const response = await ai.generate({
  model: googleAI.model('gemini-2.5-flash'),
  prompt: [
    { text: 'Transcribe this audio clip' },
    { media: { contentType: 'audio/mp3', url: 'https://example.com/audio.mp3' } },
  ],
});
```

#### PDF Support

Gemini models can process PDF documents to extract information, summarize content, or answer questions based on the visual layout and text.

```typescript
const response = await ai.generate({
  model: googleAI.model('gemini-2.5-flash'),
  prompt: [
    { text: 'Summarize this document' },
    { media: { contentType: 'application/pdf', url: 'https://example.com/doc.pdf' } },
  ],
});
```

#### File Inputs and Gemini Files API

Gemini models support various file types. For small files, you can use inline data. For larger files (up to 2GB), use the Gemini Files API.

**Using Files API:**

To use large files, you must upload them using the [Google GenAI SDK](https://ai.google.dev/gemini-api/docs/files) or other supported methods. Genkit does not provide file management helpers, but you can pass the file URI to Genkit for generation:

```typescript
import { GoogleGenAI } from '@google/genai';
// ... init genaiClient ...

// Upload file
const uploadedFile = await genaiClient.files.upload({
  file: 'path/to/video.mp4',
  config: { mimeType: 'video/mp4' },
});

// Use in generation
const response = await ai.generate({
  model: googleAI.model('gemini-2.5-flash'),
  prompt: [
    { text: 'Describe this video' },
    {
      media: {
        contentType: uploadedFile.mimeType,
        url: uploadedFile.uri,
      },
    },
  ],
});
```

## Embedding Models

### Available Models

- `gemini-embedding-001` - Latest Gemini embedding model (3072 dimensions, customizable)
- `text-embedding-004` - Text embedding model (768 dimensions, customizable)

### Usage

```typescript
const embeddings = await ai.embed({
  embedder: googleAI.embedder('gemini-embedding-001'),
  content: 'Machine learning models process data to make predictions.',
});

console.log(embeddings);
```

## Image Models

### Available Models

**Imagen 4 Series** - Latest generation with improved quality:
- `imagen-4.0-generate-001` - Standard quality
- `imagen-4.0-ultra-generate-001` - Ultra-high quality
- `imagen-4.0-fast-generate-001` - Fast generation

**Imagen 3 Series**:
- `imagen-3.0-generate-002`

### Usage

```typescript
const response = await ai.generate({
  model: googleAI.model('imagen-4.0-generate-001'),
  prompt: 'A serene Japanese garden with cherry blossoms and a koi pond.',
  config: {
    numberOfImages: 4,
    aspectRatio: '16:9',
    personGeneration: 'allow_adult',
    addWatermark: true,
  },
});

const generatedImage = response.media;
```

**Configuration Options:**

- **numberOfImages** _number_

  Number of images to generate.
  Default: `4`

- **aspectRatio** _string_

  Aspect ratio of the generated images.
  Options: `'1:1'`, `'3:4'`, `'4:3'`, `'9:16'`, `'16:9'`
  Default: `'1:1'`

- **personGeneration** _string_

  Policy for generating people.
  Options: `'dont_allow'`, `'allow_adult'`, `'allow_all'`

- **addWatermark** _boolean_

  Adds invisible SynthID watermark.
  Default: `true`

- **enhancePrompt** _boolean_

  Enables LLM-based rewrite for better prompt adherence.
  Default: `true`

- **negativePrompt** _string_

  Text to exclude from the generated image.


## Video Models

The Google AI plugin provides access to video generation capabilities through the Veo models. These models can generate videos from text prompts or manipulate existing images to create dynamic video content.

### Available Models

**Veo 3.1 Series** - Latest generation with native audio and high fidelity:
- `veo-3.1-generate-preview` - High-quality video and audio generation
- `veo-3.1-fast-generate-preview` - Fast generation with high quality

**Veo 3.0 Series**:
- `veo-3.0-generate-001`
- `veo-3.0-fast-generate-001`

**Veo 2.0 Series**:
- `veo-2.0-generate-001`

### Usage

#### Text-to-Video

To generate a video from a text prompt using the Veo model:

```typescript
import { googleAI } from '@genkit-ai/google-genai';
import * as fs from 'fs';
import { Readable } from 'stream';
import { genkit, MediaPart } from 'genkit';

const ai = genkit({
  plugins: [googleAI()],
});

ai.defineFlow('text-to-video-veo', async () => {
  let { operation } = await ai.generate({
    model: googleAI.model('veo-3.0-fast-generate-001'),
    prompt: 'A majestic dragon soaring over a mystical forest at dawn.',
    config: {
      aspectRatio: '16:9',
    },
  });

  if (!operation) {
    throw new Error('Expected the model to return an operation');
  }

  // Wait until the operation completes.
  while (!operation.done) {
    operation = await ai.checkOperation(operation);
    // Sleep for 5 seconds before checking again.
    await new Promise((resolve) => setTimeout(resolve, 5000));
  }

  if (operation.error) {
    throw new Error('failed to generate video: ' + operation.error.message);
  }

  const video = operation.output?.message?.content.find((p) => !!p.media);
  if (!video) {
    throw new Error('Failed to find the generated video');
  }
  await downloadVideo(video, 'output.mp4');
});

async function downloadVideo(video: MediaPart, path: string) {
  const fetch = (await import('node-fetch')).default;
  // Add API key before fetching the video.
  const videoDownloadResponse = await fetch(`${video.media!.url}&key=${process.env.GEMINI_API_KEY}`);
  if (!videoDownloadResponse || videoDownloadResponse.status !== 200 || !videoDownloadResponse.body) {
    throw new Error('Failed to fetch video');
  }

  Readable.from(videoDownloadResponse.body).pipe(fs.createWriteStream(path));
}
```

#### Video Generation from Photo Reference

To use a photo as reference for the video using the Veo model (e.g. to make a static photo move), you can provide an image as part of the prompt.

```typescript
const startingImage = fs.readFileSync('photo.jpg', { encoding: 'base64' });

let { operation } = await ai.generate({
  model: googleAI.model('veo-2.0-generate-001'),
  prompt: [
    {
      text: 'make the subject in the photo move',
    },
    {
      media: {
        contentType: 'image/jpeg',
        url: `data:image/jpeg;base64,${startingImage}`,
      },
    },
  ],
  config: {
    durationSeconds: 5,
    aspectRatio: '9:16',
    personGeneration: 'allow_adult',
  },
});
```

The Veo models support various configuration options:

- **negativePrompt** _string_

  Text that describes anything you want to discourage the model from generating.

- **aspectRatio** _string_

  Changes the aspect ratio of the generated video.
  - `"16:9"`
  - `"9:16"`

- **personGeneration** _string_

  Allow the model to generate videos of people.
  - **Text-to-video generation**:
    - `"allow_all"`: Generate videos that include adults and children. Currently the only available value for Veo 3.
    - `"dont_allow"` (Veo 2 only): Don't allow people or faces.
    - `"allow_adult"` (Veo 2 only): Generate videos with adults, but not children.
  - **Image-to-video generation** (Veo 2 only):
    - `"dont_allow"`: Don't allow people or faces.
    - `"allow_adult"`: Generate videos with adults, but not children.

- **numberOfVideos** _number_

  Output videos requested.
  - `1`: Supported in Veo 3 and Veo 2.
  - `2`: Supported in Veo 2 only.

- **durationSeconds** _number_ (Veo 2 only)

  Length of each output video in seconds (5 to 8). Not configurable for Veo 3.1/3.0 (defaults to 8 seconds).

- **resolution** _string_ (Veo 3.1 only)

  Resolution of the generated video.
  - `"720p"` (default)
  - `"1080p"` (Available for 16:9 aspect ratio)

- **seed** _number_ (Veo 3.1/3.0 only)

  Sets the random seed for generation. Doesn't guarantee determinism but improves consistency.

- **referenceImages** _object[]_ (Veo 3.1 only)

  Provides up to 3 reference images to guide the video's content or style.

- **enhancePrompt** _boolean_ (Veo 2 only)

  Enable or disable the prompt rewriter. Enabled by default. For Veo 3.1/3.0, the prompt enhancer is always on.


## Speech Models

The Google GenAI plugin provides access to text-to-speech capabilities through Gemini TTS models. These models can convert text into natural-sounding speech for various applications.

### Available Models

- `gemini-2.5-flash-preview-tts` - Flash model with TTS
- `gemini-2.5-pro-preview-tts` - Pro model with TTS

### Usage

**Basic Usage**

To convert text to single-speaker audio, set the response modality to "AUDIO", and pass a `speechConfig` object with `voiceConfig` set. You'll need to choose a voice name from the prebuilt [output voices](https://ai.google.dev/gemini-api/docs/speech-generation#voices).

The plugin returns raw PCM data, which can then be converted to a standard format like WAV.

```typescript
import wav from 'wav';
import { Buffer } from 'node:buffer';

async function saveWavFile(filename: string, pcmData: Buffer, sampleRate = 24000) {
  return new Promise((resolve, reject) => {
    const writer = new wav.FileWriter(filename, {
      channels: 1,
      sampleRate,
      bitDepth: 16,
    });
    writer.on('finish', resolve);
    writer.on('error', reject);
    writer.write(pcmData);
    writer.end();
  });
}

const response = await ai.generate({
  model: googleAI.model('gemini-2.5-flash-preview-tts'),
  config: {
    responseModalities: ['AUDIO'],
    speechConfig: {
      voiceConfig: {
        prebuiltVoiceConfig: { voiceName: 'Algenib' },
      },
    },
  },
  prompt: 'Say that Genkit is an amazing AI framework',
});

if (response.media?.url) {
  const data = response.media.url.split(',')[1];
  if (data) {
    const pcmData = Buffer.from(data, 'base64');
    await saveWavFile('output.wav', pcmData);
  }
}
```

**Multi-Speaker**

You can generate audio with multiple speakers, each with their own voice. The model automatically detects speaker labels in the text (like "Speaker1:" and "Speaker2:") and applies the corresponding voice to each speaker's lines.

```typescript
const { media } = await ai.generate({
  model: googleAI.model('gemini-2.5-flash-preview-tts'),
  prompt: `
    Speaker A: Hello, how are you today?
    Speaker B: I am doing great, thanks for asking!
  `,
  config: {
    responseModalities: ['AUDIO'],
    speechConfig: {
      multiSpeakerVoiceConfig: {
        speakerVoiceConfigs: [
          {
            speaker: 'Speaker A',
            voiceConfig: { prebuiltVoiceConfig: { voiceName: 'Puck' } },
          },
          {
            speaker: 'Speaker B',
            voiceConfig: { prebuiltVoiceConfig: { voiceName: 'Kore' } },
          },
        ],
      },
    },
  },
});
```

The following configuration options are available for speech generation:

- **speechConfig** _object_

  - **voiceConfig** _object_

    Defines the voice configuration for a single speaker.

    - **prebuiltVoiceConfig** _object_

      - **voiceName** _string_

        The name of the voice to use.
        Options: `Puck`, `Charon`, `Kore`, `Fenrir`, `Aoede` (and [others](https://ai.google.dev/gemini-api/docs/speech-generation#voices)).

      - **speakingRate** _number_

        Controls the speed of speech. Range: `0.25` to `4.0`, default is `1.0`.

      - **pitch** _number_

        Adjusts the pitch of the voice. Range: `-20.0` to `20.0`, default is `0.0`.

      - **volumeGainDb** _number_

        Controls the volume. Range: `-96.0` to `16.0`, default is `0.0`.

  - **multiSpeakerVoiceConfig** _object_

    Defines the voice configuration for multiple speakers.

    - **speakerVoiceConfigs** _array_

      A list of voice configurations for each speaker.

      - **speaker** _string_

        The name of the speaker (e.g., "Speaker A") as used in the prompt.

      - **voiceConfig** _object_

        The voice configuration for this speaker. See `voiceConfig` above.

**Speech Emphasis**

You can use markdown-style formatting in your prompt to add emphasis:

- **Bold text** (`**like this**`) for stronger emphasis.
- _Italic text_ (`*like this*`) for moderate emphasis.

```typescript
prompt: 'Genkit is an **amazing** Gen AI *library*!';
```

TTS models automatically detect the input language. Supported languages include `en-US`, `fr-FR`, `de-DE`, `es-US`, `ja-JP`, `ko-KR`, `pt-BR`, `zh-CN`, and [more](https://ai.google.dev/gemini-api/docs/speech-generation#languages).

</LanguageContent>

<LanguageContent lang="go">

The Google AI plugin provides a unified interface to connect with Google's generative AI models through the **Gemini Developer API** or **Vertex AI** using API key authentication or Google Cloud credentials.

The plugin supports a wide range of capabilities:

- **Language Models**: Gemini models for text generation, reasoning, and multimodal tasks
- **Embedding Models**: Text and multimodal embeddings
- **Image Models**: Imagen for generation and Gemini for image analysis
- **Video Models**: Veo for video generation and Gemini for video understanding
- **Speech Models**: Polyglot text-to-speech generation

## Setup

### Installation

```bash
go get github.com/firebase/genkit/go/plugins/googlegenai
```

### Configuration

You can use either the Google AI (Gemini API) or Vertex AI backend.

**Using Google AI (Gemini API):**

```go
import (
	"context"
	"log"

	"github.com/firebase/genkit/go/genkit"
	"github.com/firebase/genkit/go/plugins/googlegenai"
)

func main() {
	ctx := context.Background()

	g := genkit.Init(ctx,
		genkit.WithPlugins(&googlegenai.GoogleAI{
			APIKey: "your-api-key", // Optional: defaults to GEMINI_API_KEY or GOOGLE_API_KEY env var
		}),
	)
}
```

**Using Vertex AI:**

```go
import (
	"context"
	"log"

	"github.com/firebase/genkit/go/genkit"
	"github.com/firebase/genkit/go/plugins/googlegenai"
)

func main() {
	ctx := context.Background()

	g := genkit.Init(ctx,
		genkit.WithPlugins(&googlegenai.VertexAI{
			ProjectID: "your-project-id", // Optional: defaults to GOOGLE_CLOUD_PROJECT
			Location:  "us-central1",     // Optional: defaults to GOOGLE_CLOUD_LOCATION
		}),
	)
}
```

### Authentication

**Google AI**: Requires a Gemini API Key, which you can get from [Google AI Studio](https://aistudio.google.com/apikey). Set the `GEMINI_API_KEY` environment variable or pass it to the plugin configuration.

**Vertex AI**: Requires Google Cloud credentials. Set the `GOOGLE_APPLICATION_CREDENTIALS` environment variable to your service account key file path, or use default credentials (e.g., `gcloud auth application-default login`).

## Language Models

You can create models that call the Google Generative AI API. The models support tool calls and some have multi-modal capabilities.

### Available Models

Genkit automatically discovers available models supported by the [Go GenAI SDK](https://github.com/google/go-genai). This ensures that recently released models are available immediately as they are added to the SDK, while deprecated models are automatically ignored and hidden from the list of actions.

**Gemini 3 Series** - Latest experimental models with state-of-the-art reasoning:
- `gemini-3-pro-preview` - Preview of the most capable model for complex tasks
- `gemini-3-flash-preview` - Fast and intelligent model for high-volume tasks
- `gemini-3-pro-image-preview` - Supports image generation outputs

**Gemini 2.5 Series** - Latest stable models with advanced reasoning and multimodal capabilities:
- `gemini-2.5-pro` - Most capable stable model for complex tasks
- `gemini-2.5-flash` - Fast and efficient for most use cases
- `gemini-2.5-flash-lite` - Lightweight version for simple tasks
- `gemini-2.5-flash-image` - Supports image generation outputs

**Gemma 3 Series** - Open models for various use cases:
- `gemma-3-27b-it` - Large instruction-tuned model
- `gemma-3-12b-it` - Medium instruction-tuned model
- `gemma-3-4b-it` - Small instruction-tuned model
- `gemma-3-1b-it` - Tiny instruction-tuned model
- `gemma-3n-e4b-it` - Efficient 4-bit model

:::note
You can use any model ID supported by the underlying SDK. For a complete and up-to-date list of models and their specific capabilities, refer to the [Google Generative AI models documentation](https://ai.google.dev/gemini-api/docs/models).
:::

### Basic Usage

```go
import (
	"context"
	"fmt"
	"log"

	"github.com/firebase/genkit/go/ai"
	"github.com/firebase/genkit/go/genkit"
	"github.com/firebase/genkit/go/plugins/googlegenai"
)

func main() {
	// ... Init genkit with googlegenai plugin ...

	resp, err := genkit.Generate(ctx, g,
		ai.WithModel(googlegenai.ModelRef("gemini-2.5-flash", nil)),
		ai.WithPrompt("Explain how neural networks learn in simple terms."),
	)
	if err != nil {
		log.Fatal(err)
	}

	fmt.Println(resp.Text())
}
```

### Model References and Configuration

You can reference models in several ways. Using **`googlegenai.ModelRef`** is generally considered best practice because it provides **strong typing**, ensuring that you are using the correct configuration type for the specific plugin:

- **`googlegenai.ModelRef(name, config)`**: Creates a static reference that includes a specific configuration. This is the preferred way as it enforces type safety for the `config` parameter.
- **`ai.WithModelName(name)`**: Resolves a model by its strong identifier (e.g., `"googleai/gemini-2.5-flash"`). A quick way to reference a model when no configuration is needed.
- **`googlegenai.GoogleAIModel(g, name)`**: Returns a handle to a model registered with your Genkit instance. Use this when you want to provide configuration dynamically at the request level.

You can provide configuration either when referencing the model or per-request to the `genkit.Generate` call:

```go
import (
	"google.golang.org/genai"
	"github.com/firebase/genkit/go/plugins/googlegenai"
)

config := &genai.GenerateContentConfig{
	Temperature: genai.Ptr[float32](0.5),
}

// Option 1: Use a model reference with "baked-in" config
modelRef := googlegenai.ModelRef("gemini-2.5-flash", config)
resp, err := genkit.Generate(ctx, g, ai.WithModel(modelRef), ai.WithPrompt("..."))

// Option 2: Pass configuration per-request (requires the model to be resolved properly)
// Using googlegenai.GoogleAIModel allows setting the plugin-specific config directly
model := googlegenai.GoogleAIModel(g, "gemini-2.5-flash")
resp, err = genkit.Generate(ctx, g,
	ai.WithModel(model),
	ai.WithConfig(config), // Pass config explicitly
	ai.WithPrompt("..."),
)
```

### Structured Output

Gemini models support structured output generation, which guarantees that the model output will conform to a specified schema. Genkit Go provides type-safe generics to make this easy.

**Using `GenerateData` (Recommended):**

```go
type Character struct {
	Name string `json:"name"`
	Bio  string `json:"bio"`
	Age  int    `json:"age"`
}

// Automatically infers schema from the struct and unmarshals the result
char, resp, err := genkit.GenerateData[Character](ctx, g,
	ai.WithModel(googlegenai.ModelRef("gemini-2.5-flash", nil)),
	ai.WithPrompt("Generate a profile for a fictional character"),
)
if err != nil {
	log.Fatal(err)
}

fmt.Printf("Name: %s, Age: %d\n", char.Name, char.Age)
```

**Using `Generate` (Standard):**

You can also use the standard `Generate` function and unmarshal manually:

```go
resp, err := genkit.Generate(ctx, g,
	ai.WithModel(googlegenai.ModelRef("gemini-2.5-flash", nil)),
	ai.WithPrompt("Generate a profile for a fictional character"),
	ai.WithOutputType(Character{}),
)
if err != nil {
	log.Fatal(err)
}

var char Character
if err := resp.Output(&char); err != nil {
	log.Fatal(err)
}
```

#### Schema Limitations

The Gemini API relies on a specific subset of the OpenAPI 3.0 standard. When defining schemas (Go structs), keep the following limitations in mind:

**Supported Features**
- **Objects & Arrays**: Standard object properties and array items.
- **Enums**: Supported via `enum` tag or string slices in schema.
- **Nullable**: Supported (mapped to `nullable: true`).

**Critical Limitations**
- **Validation**: Keywords like `pattern`, `minLength`, `maxLength`, `minItems`, and `maxItems` are **not supported** by the API's constrained decoding.
- **Unions**: Complex unions are often problematic.
- **Recursion**: Recursive schemas are generally not supported.

**Best Practices**
- Keep schemas simple and flat where possible.
- Use property descriptions to guide the model instead of complex validation rules.
- If you need strict validation (e.g., regex), perform it in your application code *after* receiving the structured response.

### Thinking and Reasoning

Gemini 2.5 and newer models use an internal thinking process that improves reasoning for complex tasks.

**Thinking Level (Gemini 3.0):**

```go
import "google.golang.org/genai"

resp, err := genkit.Generate(ctx, g,
	ai.WithModel(googlegenai.ModelRef("gemini-3-pro-preview", &genai.GenerateContentConfig{
		ThinkingConfig: &genai.ThinkingConfig{
			ThinkingLevel: genai.Ptr("HIGH"), // Or "LOW", "MEDIUM", "MINIMAL"
		},
	})),
	ai.WithPrompt("what is heavier, one kilo of steel or one kilo of feathers"),
)
```

Gemini 3 models support the following configuration options for thinking:

- **ThinkingLevel** _string_
  The reasoning depth for the model (`"HIGH"`, `"MEDIUM"`, `"LOW"`, `"MINIMAL"`).

**Thinking Budget (Gemini 2.5):**

```go
import "google.golang.org/genai"

resp, err := genkit.Generate(ctx, g,
	ai.WithModel(googlegenai.ModelRef("gemini-2.5-pro", &genai.GenerateContentConfig{
		ThinkingConfig: &genai.ThinkingConfig{
			ThinkingBudget:  genai.Ptr[int32](8192), // Number of thinking tokens
			IncludeThoughts: true,                   // Include thought summaries
		},
	})),
	ai.WithPrompt("what is heavier, one kilo of steel or one kilo of feathers"),
)

// If IncludeThoughts is true, thoughts are included in the response reasoning
```

### Context Caching

Gemini 2.5 and newer models automatically cache common content prefixes. In Genkit Go, you can mark content for caching using `WithCacheTTL` or `WithCacheName`.

```go
// Create a message with cached content
cachedMsg := ai.NewUserTextMessage(largeContent).WithCacheTTL(300)

// First request - content will be cached
resp1, err := genkit.Generate(ctx, g,
	ai.WithModel(googlegenai.ModelRef("gemini-2.5-flash", nil)),
	ai.WithMessages(cachedMsg),
	ai.WithPrompt("Task 1..."),
)

// Second request with same prefix - eligible for cache hit
resp2, err := genkit.Generate(ctx, g,
	ai.WithModel(googlegenai.ModelRef("gemini-2.5-flash", nil)),
	// Reuse the history from previous response or construct messages with same prefix
	ai.WithMessages(resp1.History()...),
	ai.WithPrompt("Task 2..."),
)
```

### Safety Settings

You can configure safety settings to control content filtering:

```go
import "google.golang.org/genai"

resp, err := genkit.Generate(ctx, g,
	ai.WithModel(googlegenai.ModelRef("gemini-2.5-flash", &genai.GenerateContentConfig{
		SafetySettings: []*genai.SafetySetting{
			{
				Category:  genai.HarmCategoryHateSpeech,
				Threshold: genai.HarmBlockThresholdBlockMediumAndAbove,
			},
			{
				Category:  genai.HarmCategoryDangerousContent,
				Threshold: genai.HarmBlockThresholdBlockMediumAndAbove,
			},
		},
	})),
	ai.WithPrompt("Your prompt here"),
)
```

### Google Search Grounding

Enable Google Search to provide answers with current information and verifiable sources.

```go
import "google.golang.org/genai"

resp, err := genkit.Generate(ctx, g,
	ai.WithModel(googlegenai.ModelRef("gemini-2.5-flash", &genai.GenerateContentConfig{
		Tools: []*genai.Tool{
			{
				GoogleSearch: &genai.GoogleSearch{},
			},
		},
	})),
	ai.WithPrompt("What are the top tech news stories this week?"),
)
```

### Google Maps Grounding

Enable Google Maps to provide location-aware responses.

```go
import "google.golang.org/genai"

resp, err := genkit.Generate(ctx, g,
	ai.WithModel(googlegenai.ModelRef("gemini-2.5-flash", &genai.GenerateContentConfig{
		Tools: []*genai.Tool{
			{
				GoogleMaps: &genai.GoogleMaps{
					EnableWidget: genai.Ptr(true),
				},
			},
		},
		ToolConfig: &genai.ToolConfig{
			RetrievalConfig: &genai.RetrievalConfig{
				LatLng: &genai.LatLng{
					Latitude:  genai.Ptr(37.7749),
					Longitude: genai.Ptr(-122.4194),
				},
			},
		},
	})),
	ai.WithPrompt("Find coffee shops near Times Square"),
)

// Access grounding metadata (e.g., for map widget)
if custom, ok := resp.Custom["candidates"].([]*genai.Candidate); ok {
	for _, cand := range custom {
		if cand.GroundingMetadata != nil && cand.GroundingMetadata.GoogleMapsWidgetContextToken != "" {
			fmt.Printf("Map Widget Token: %s\n", cand.GroundingMetadata.GoogleMapsWidgetContextToken)
		}
	}
}
```

### Code Execution

Enable the model to write and execute Python code for calculations and logic.

```go
import "google.golang.org/genai"

resp, err := genkit.Generate(ctx, g,
	ai.WithModel(googlegenai.ModelRef("gemini-2.5-pro", &genai.GenerateContentConfig{
		Tools: []*genai.Tool{
			{
				CodeExecution: &genai.ToolCodeExecution{},
			},
		},
	})),
	ai.WithPrompt("Calculate the 20th Fibonacci number"),
)
```

### Generating Text and Images

Some Gemini models (like `gemini-2.5-flash-image`) can output images natively alongside text.

```go
import "google.golang.org/genai"

resp, err := genkit.Generate(ctx, g,
	ai.WithModel(googlegenai.ModelRef("gemini-2.5-flash-image", &genai.GenerateContentConfig{
		ResponseModalities: []string{"IMAGE", "TEXT"},
	})),
	ai.WithPrompt("Create a picture of a futuristic city and describe it"),
)

for _, part := range resp.Message.Content {
	if part.IsMedia() {
		fmt.Printf("Generated image: %s\n", part.ContentType)
		// Access data via part.Text (data URI) or helper functions
	}
}
```

### Multimodal Input Capabilities

Genkit supports multimodal input (text, image, video, audio) via `ai.Part`.

#### Video Understanding

Gemini models can process videos passed as URIs or inline data.

```go
resp, err := genkit.Generate(ctx, g,
	ai.WithModel(googlegenai.ModelRef("gemini-2.5-flash", nil)),
	ai.WithMessages(
		ai.NewUserMessage(
			ai.NewTextPart("What happens in this video?"),
			ai.NewMediaPart("video/mp4", "https://example.com/video.mp4"),
		),
	),
)
```

#### Image Understanding

Gemini models can reason about images passed as inline data or URLs.

```go
resp, err := genkit.Generate(ctx, g,
	ai.WithModel(googlegenai.ModelRef("gemini-2.5-flash", nil)),
	ai.WithMessages(
		ai.NewUserMessage(
			ai.NewTextPart("Describe what is in this image"),
			ai.NewMediaPart("image/jpeg", "https://example.com/image.jpg"),
		),
	),
)
```

#### Audio Understanding

Gemini models can process audio files to transcribe speech text or answer questions.

```go
resp, err := genkit.Generate(ctx, g,
	ai.WithModel(googlegenai.ModelRef("gemini-2.5-flash", nil)),
	ai.WithMessages(
		ai.NewUserMessage(
			ai.NewTextPart("Transcribe this audio clip"),
			ai.NewMediaPart("audio/mp3", "https://example.com/audio.mp3"),
		),
	),
)
```

#### PDF Support

Gemini models can process PDF documents to extract information, summarize content, or answer questions based on the visual layout and text.

```go
resp, err := genkit.Generate(ctx, g,
	ai.WithModel(googlegenai.ModelRef("gemini-2.5-flash", nil)),
	ai.WithMessages(
		ai.NewUserMessage(
			ai.NewTextPart("Summarize this document"),
			ai.NewMediaPart("application/pdf", "https://example.com/doc.pdf"),
		),
	),
)
```

## Embedding Models

### Available Models

- `text-embedding-004`
- `gemini-embedding-001`
- `multimodalembedding`

### Usage

```go
res, err := genkit.Embed(ctx, g,
	ai.WithEmbedderName("googleai/gemini-embedding-001"),
	ai.WithTextDocs("Machine learning models process data to make predictions."),
)
if err != nil {
	log.Fatal(err)
}

fmt.Printf("Embedding: %v\n", res.Embeddings[0].Embedding)
```

## Image Models

### Available Models

**Imagen 4 Series** - Latest generation with improved quality:
- `imagen-4.0-generate-001` - Standard quality
- `imagen-4.0-ultra-generate-001` - Ultra-high quality
- `imagen-4.0-fast-generate-001` - Fast generation

**Imagen 3 Series**:
- `imagen-3.0-generate-001`
- `imagen-3.0-generate-002`
- `imagen-3.0-fast-generate-001`

### Usage

```go
import "google.golang.org/genai"

resp, err := genkit.Generate(ctx, g,
	ai.WithModel(googlegenai.ModelRef("imagen-4.0-generate-001", &genai.GenerateImagesConfig{
		NumberOfImages:   genai.Ptr[int32](4),
		AspectRatio:      genai.Ptr("16:9"),
		PersonGeneration: genai.Ptr("allow_adult"),
	})),
	ai.WithPrompt("A serene Japanese garden with cherry blossoms"),
)

// Access generated images in resp.Message.Content
```

## Video Models

The Google AI plugin provides access to video generation capabilities through the Veo models.

### Available Models

**Veo 3.1 Series** - Latest generation with native audio and high fidelity:
- `veo-3.1-generate-preview`
- `veo-3.1-fast-generate-preview`

**Veo 3.0 Series**:
- `veo-3.0-generate-001`
- `veo-3.0-fast-generate-001`

**Veo 2.0 Series**:
- `veo-2.0-generate-001`

### Usage

Veo operations are long-running and support multiple generation modes.

#### Backend-Specific Considerations

The output format and behavior of Veo differ depending on whether you are using the **Google AI** or **Vertex AI** backend.

##### Model Names

Ensure you use the correct provider prefix:
- **Google AI**: `googleai/veo-3.1-generate-preview`
- **Vertex AI**: `vertexai/veo-3.1-generate-preview`

##### Output Format (Video URLs vs. Raw Bytes)

Depending on the backend and configuration, the generated video will be returned as either a remote URI or as raw bytes encoded in a base64 data URI.

- **Google AI**: Typically returns a public URI for the video. To download it via HTTP, you must append your API key to the URL: `https://.../video.mp4?key=YOUR_API_KEY`.
- **Vertex AI**: Can return a Cloud Storage URI (`gs://...`) if configured, but by default often returns **raw video bytes**. The Genkit plugin automatically encodes these raw bytes as a **base64 data URI** in the message's text field.

Your application should be prepared to handle both formats. For example, to save the output directly to a file:

```go
for _, part := range op.Output.Message.Content {
	if part.IsMedia() {
		if strings.HasPrefix(part.Text, "data:video/mp4;base64,") {
			// Handle base64 encoded bytes (Common for Vertex AI default)
			data := strings.TrimPrefix(part.Text, "data:video/mp4;base64,")
			b, _ := base64.StdEncoding.DecodeString(data)
			os.WriteFile("video.mp4", b, 0644)
		} else {
			// Handle remote URI (Common for Google AI or Vertex AI with GCS)
			// You would typically use an HTTP client or Google Cloud Storage client here
			fmt.Printf("Video available at URI: %s\n", part.Text)
		}
	}
}
```

##### Safety Filtering (RAI)

Veo has strict safety policies. If a prompt triggers a safety filter, the operation will complete but return no video. In this case:

1. `FinishReason` will be `ai.FinishReasonBlocked`.
2. The output message will contain a text part listing the specific reasons the content was filtered.
3. The original API response (including RAI counts) is available in the `Raw` field.

#### Text-to-Video

Generate a video from a text description.

```go
op, err := genkit.GenerateOperation(ctx, g,
	ai.WithModelName("googleai/veo-3.1-generate-preview"),
	ai.WithMessages(ai.NewUserTextMessage("A majestic dragon soaring over a mystical forest at dawn.")),
	ai.WithConfig(&genai.GenerateVideosConfig{
		AspectRatio:     genai.Ptr("16:9"),
		DurationSeconds: genai.Ptr(int32(8)),
		Resolution:      genai.Ptr("720p"),
	}),
)
if err != nil {
	log.Fatal(err)
}

// Poll for completion
op, err = genkit.CheckModelOperation(ctx, g, op)
```

#### Image-to-Video

Animate a static image using a text prompt.

```go
// Load image data (e.g., base64 encoded)
imagePart := ai.NewMediaPart("image/jpeg", "data:image/jpeg;base64,...")

op, err := genkit.GenerateOperation(ctx, g,
	ai.WithModelName("googleai/veo-3.1-generate-preview"),
	ai.WithMessages(ai.NewUserMessage(
		ai.NewTextPart("The cat wakes up and starts accelerating the go-kart."),
		imagePart,
	)),
	ai.WithConfig(&genai.GenerateVideosConfig{
		AspectRatio: genai.Ptr("16:9"),
	}),
)
```

#### Video-to-Video (Video Editing)

Edit or transform an existing video.

:::note
Video-to-video generation requires a **Veo video URL** (a URL generated by a previous Veo model operation). Arbitrary external video URLs or files are not currently supported for this mode.
:::

```go
// Provide the URI of a Veo-generated video to edit
videoPart := ai.NewMediaPart("video/mp4", "https://generativelanguage.googleapis.com/...")

op, err := genkit.GenerateOperation(ctx, g,
	ai.WithModelName("googleai/veo-3.1-generate-preview"),
	ai.WithMessages(ai.NewUserMessage(
		ai.NewTextPart("Change the video style to be a cartoon from 1950."),
		videoPart,
	)),
	ai.WithConfig(&genai.GenerateVideosConfig{
		AspectRatio: genai.Ptr("16:9"),
	}),
)
```

#### Configuration Options

The Veo models support the following configuration options via `genai.GenerateVideosConfig`:

- **NegativePrompt** _string_

  Text that describes anything you want to discourage the model from generating.

- **AspectRatio** _string_

  Changes the aspect ratio of the generated video.
  - `"16:9"`
  - `"9:16"`

- **PersonGeneration** _string_

  Allow the model to generate videos of people.
  - **Text-to-video generation**:
    - `"allow_all"`: Generate videos that include adults and children. Currently the only available value for Veo 3.
    - `"dont_allow"` (Veo 2 only): Don't allow people or faces.
    - `"allow_adult"` (Veo 2 only): Generate videos with adults, but not children.
  - **Image-to-video generation** (Veo 2 only):
    - `"dont_allow"`: Don't allow people or faces.
    - `"allow_adult"`: Generate videos with adults, but not children.

- **NumberOfVideos** _int32_

  Output videos requested.
  - `1`: Supported in Veo 3 and Veo 2.
  - `2`: Supported in Veo 2 only.

- **DurationSeconds** _int32_ (Veo 2 only)

  Length of each output video in seconds (5 to 8). Not configurable for Veo 3.1/3.0 (defaults to 8 seconds).

- **Resolution** _string_ (Veo 3.1 only)

  Resolution of the generated video.
  - `"720p"` (default)
  - `"1080p"` (Available for 16:9 aspect ratio)

- **Seed** _int32_ (Veo 3.1/3.0 only)

  Sets the random seed for generation consistency. Doesn't guarantee determinism but improves consistency.

- **EnhancePrompt** _bool_ (Veo 2 only)

  Enable or disable the prompt rewriter. Enabled by default. For Veo 3.1/3.0, the prompt enhancer is always on.


## Speech Models

Use `gemini-2.5-flash` or `gemini-2.5-pro` with audio output modality, or use the dedicated TTS models (`gemini-2.5-flash-preview-tts` and `gemini-2.5-pro-preview-tts`).

### Usage

```go
import "google.golang.org/genai"

resp, err := genkit.Generate(ctx, g,
	ai.WithModel(googlegenai.ModelRef("gemini-2.5-flash", &genai.GenerateContentConfig{
		ResponseModalities: []string{"AUDIO"},
		SpeechConfig: &genai.SpeechConfig{
			VoiceConfig: &genai.VoiceConfig{
				PrebuiltVoiceConfig: &genai.PrebuiltVoiceConfig{
					VoiceName: "Algenib",
				},
			},
		},
	})),
	ai.WithPrompt("Say that Genkit is an amazing AI framework"),
)

// The audio data will be in resp.Message.Content as a media part.
// The model output can also be a base64 encoded string in resp.Text()
// You can decode this and save it as a PCM file or convert to WAV.
```
</LanguageContent>

<LanguageContent lang="python">

The `genkit-plugin-google-genai` package provides the `GoogleAI` plugin for accessing Google's generative AI models via the Google Gemini API (requires an API key).

## Installation

```bash
pip3 install genkit-plugin-google-genai
```

## Configuration

To use the Google Gemini API, you need an API key.

```python
from genkit.ai import Genkit
from genkit.plugins.google_genai import GoogleAI

ai = Genkit(
  plugins=[GoogleAI()],
  model='googleai/gemini-2.5-flash',
)
```

You will need to set GEMINI_API_KEY environment variable or you can provide the API Key directly:

```python
ai = Genkit(
  plugins=[GoogleAI(api_key='...')]
)
```

## Usage

### Text Generation

```python
response = await ai.generate('What should I do when I visit Melbourne?')
print(response.text)
```

### Text Embedding

```python
embeddings = await ai.embed(
    embedder='googleai/text-embedding-004',
    content='How many widgets do you have in stock?',
)
```

### Image Generation

```python
response = await ai.generate(
    model='googleai/imagen-3.0-generate-002',
    prompt='a banana riding a bicycle',
)
```

## Next Steps

- Learn about [generating content](/docs/models) to understand how to use these models effectively
- Explore [creating flows](/docs/flows) to build structured AI workflows
- To use the Gemini API at enterprise scale see the [Vertex AI plugin](/docs/integrations/vertex-ai)

</LanguageContent>
