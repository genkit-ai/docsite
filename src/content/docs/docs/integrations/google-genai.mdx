---
title: Google AI plugin
description: Learn how to use Google's Gemini models with Genkit across JavaScript, Go, and Python through the Google AI Studio API, including text generation, embeddings, image generation, video generation, and text-to-speech.
---

import LanguageSelector from '../../../../components/LanguageSelector.astro';
import CopyMarkdownButton from '../../../../components/CopyMarkdownButton.astro';
import LanguageContent from '../../../../components/LanguageContent.astro';

<div style="display: flex; justify-content: space-between; align-items: center; gap: 1rem; margin: 1rem 0 1rem 0;">
  <LanguageSelector />
  <CopyMarkdownButton />
</div>

<LanguageContent lang="js">

## Provider Overview

The Google GenAI plugin provides a unified interface to connect with Google's generative AI models through the **Gemini Developer API** using API key authentication. It is a replacement for the previous `googleAI` plugin.

The plugin supports a wide range of capabilities:
- **Language Models**: Gemini models for text generation, reasoning, and multimodal tasks
- **Embedding Models**: Text and multimodal embeddings
- **Image Models**: Imagen for generation and Gemini for image analysis
- **Video Models**: Veo for video generation and Gemini for video understanding
- **Speech Models**: Polyglot text-to-speech generation

## Setup

### Installation

```bash
npm i --save @genkit-ai/google-genai
```

### Configuration

```typescript
import { genkit } from 'genkit';
import { googleAI } from '@genkit-ai/google-genai';

const ai = genkit({
  plugins: [
    googleAI(),
    // Or with an explicit API key:
    // googleAI({ apiKey: 'your-api-key' }),
  ],
});
```

### Authentication

Requires a Google AI API Key, which you can get from [Google AI Studio](https://aistudio.google.com/apikey). You can provide this key in several ways:

1. **Environment variables**: Set `GEMINI_API_KEY` or `GOOGLE_API_KEY`
2. **Plugin configuration**: Pass `apiKey` when initializing the plugin (shown above)
3. **Per-request**: Override the API key for specific requests in the config:

```typescript
const response = await ai.generate({
  model: googleAI.model('gemini-2.5-flash'),
  prompt: 'Your prompt here',
  config: {
    apiKey: 'different-api-key', // Use a different API key for this request
  },
});
```

This per-request API key option is useful for routing specific requests to different API keys, such as for multi-tenant applications or cost tracking.

### Troubleshooting Setup

#### API Key Issues
Ensure your API key is properly configured:
- Set the `GEMINI_API_KEY` or `GOOGLE_API_KEY` environment variable
- Or pass the key directly in plugin configuration: `googleAI({ apiKey: 'your-key' })`
- Get your API key from [Google AI Studio](https://aistudio.google.com/apikey)

## Language Models

You can create models that call the Google Generative AI API. The models support tool calls and some have multi-modal capabilities.

### Available Models

**Gemini 3 Series** - Latest experimental models with state-of-the-art reasoning:
- `gemini-3-pro-preview` - Preview of the most capable model for complex tasks

**Gemini 2.5 Series** - Latest stable models with advanced reasoning and multimodal capabilities:
- `gemini-2.5-pro` - Most capable stable model for complex tasks
- `gemini-2.5-flash` - Fast and efficient for most use cases
- `gemini-2.5-flash-lite` - Lightweight version for simple tasks
- `gemini-2.5-flash-image` - Supports image generation outputs

**Gemma 3 Series** - Open models for various use cases:
- `gemma-3-27b-it` - Large instruction-tuned model
- `gemma-3-12b-it` - Medium instruction-tuned model
- `gemma-3-4b-it` - Small instruction-tuned model
- `gemma-3-1b-it` - Tiny instruction-tuned model
- `gemma-3n-e4b-it` - Efficient 4-bit model

:::note
See the [Google Generative AI models documentation](https://ai.google.dev/gemini-api/docs/models) for a complete list of available models and their capabilities.
:::

### Model Capabilities

The following table shows the capabilities of popular Gemini models:

| Model | Image Input | Structured Output | Tool Usage | Tool Streaming | Thinking | Code Execution | Google Search |
|-------|-------------|-------------------|------------|----------------|----------|----------------|---------------|
| `gemini-3-pro-preview` | ✓ | ✓ | ✓ | ✓ | ✓ | ✓ | ✓ |
| `gemini-2.5-pro` | ✓ | ✓ | ✓ | ✓ | ✓ | ✓ | ✓ |
| `gemini-2.5-flash` | ✓ | ✓ | ✓ | ✓ | ✓ | ✓ | ✓ |
| `gemini-2.5-flash-lite` | ✓ | ✓ | ✓ | ✓ | ✓ | ✓ | ✓ |

### Basic Usage

```typescript
import { genkit } from 'genkit';
import { googleAI } from '@genkit-ai/google-genai';

const ai = genkit({
  plugins: [googleAI()],
});

const response = await ai.generate({
  model: googleAI.model('gemini-2.5-flash'),
  prompt: 'Explain how neural networks learn in simple terms.',
});

console.log(response.text);
```

### Configuration Options

#### Safety Settings

You can configure safety settings to control content filtering for different harm categories:

```typescript
const response = await ai.generate({
  model: googleAI.model('gemini-2.5-flash'),
  prompt: 'Your prompt here',
  config: {
    safetySettings: [
      {
        category: 'HARM_CATEGORY_HATE_SPEECH',
        threshold: 'BLOCK_MEDIUM_AND_ABOVE',
      },
      {
        category: 'HARM_CATEGORY_DANGEROUS_CONTENT',
        threshold: 'BLOCK_MEDIUM_AND_ABOVE',
      },
    ],
  },
});
```

Available harm categories:
- `HARM_CATEGORY_HATE_SPEECH`
- `HARM_CATEGORY_DANGEROUS_CONTENT`
- `HARM_CATEGORY_HARASSMENT`
- `HARM_CATEGORY_SEXUALLY_EXPLICIT`

Available thresholds:
- `HARM_BLOCK_THRESHOLD_UNSPECIFIED`
- `BLOCK_LOW_AND_ABOVE`
- `BLOCK_MEDIUM_AND_ABOVE`
- `BLOCK_ONLY_HIGH`
- `BLOCK_NONE`

**Accessing Safety Ratings:**

Safety ratings are typically only included when content is flagged. You can access them from the response custom metadata:

```typescript
const geminiResponse = response.custom as any;
const candidateSafetyRatings = geminiResponse?.candidates?.[0]?.safetyRatings;
const promptSafetyRatings = geminiResponse?.promptFeedback?.safetyRatings;
```

#### Thinking and Reasoning

Gemini 2.5 and newer models use an internal thinking process that improves reasoning for complex tasks.

**Thinking Level (Gemini 3.0):**

```typescript
const response = await ai.generate({
  model: googleAI.model('gemini-3-pro-preview'),
  prompt: 'what is heavier, one kilo of steel or one kilo of feathers',
    config: {
      thinkingConfig: {
        thinkingLevel: 'HIGH',  // Or 'LOW' or 'MEDIUM'
    },
  },
});
```

**Thinking Budget (Gemini 2.5):**

```typescript
const response = await ai.generate({
  model: googleAI.model('gemini-2.5-pro'),
  prompt: 'what is heavier, one kilo of steel or one kilo of feathers',
  config: {
    thinkingConfig: {
      thinkingBudget: 8192, // Number of thinking tokens
      includeThoughts: true, // Include thought summaries
    },
  },
});

if (response.reasoning) {
  console.log('Reasoning:', response.reasoning);
}
```

#### Context Caching

Gemini 2.5 and newer models automatically cache common content prefixes (min 1024 tokens for Flash, 2048 for Pro), providing a 75% token discount on cached tokens.

```typescript
// Structure prompts with consistent content at the beginning
const baseContext = `You are a helpful cook... (large context) ...`.repeat(50);

// First request - content will be cached
await ai.generate({
  model: googleAI.model('gemini-2.5-pro'),
  prompt: `${baseContext}\n\nTask 1...`,
});

// Second request with same prefix - eligible for cache hit
await ai.generate({
  model: googleAI.model('gemini-2.5-pro'),
  prompt: `${baseContext}\n\nTask 2...`,
});
```

### Tools and Grounding

#### Google Search Grounding

Enable Google Search to provide answers with current information and verifiable sources.

```typescript
const response = await ai.generate({
  model: googleAI.model('gemini-2.5-flash'),
  prompt: 'What are the top tech news stories this week?',
  config: {
    googleSearchRetrieval: true,
  },
});

// Access grounding metadata
const groundingMetadata = (response.custom as any)?.candidates?.[0]?.groundingMetadata;
if (groundingMetadata) {
  console.log('Sources:', groundingMetadata.groundingChunks);
}
```

#### Google Maps Grounding

Enable Google Maps to provide location-aware responses.

```typescript
const response = await ai.generate({
  model: googleAI.model('gemini-2.5-flash'),
  prompt: 'Find coffee shops near Times Square',
  config: {
    tools: [{ googleMaps: {} }],
  },
});
```

You can also request a widget token to render an interactive map:

```typescript
const response = await ai.generate({
  model: googleAI.model('gemini-2.5-flash'),
  prompt: 'Show me a map of San Francisco',
  config: {
    tools: [{ googleMaps: { enableWidget: true } }],
  },
});
```

#### URL Context

Provide specific URLs for the model to analyze:

```typescript
const response = await ai.generate({
  model: googleAI.model('gemini-2.5-flash'),
  prompt: 'Summarize this page',
  config: {
    tools: [{ urlContext: {} }],
  },
});
```

When using `urlContext`, the model will fetch content from URLs found in your prompt.

#### Code Execution

Enable the model to write and execute Python code for calculations and logic.

```typescript
const response = await ai.generate({
  model: googleAI.model('gemini-2.5-pro'),
  prompt: 'Calculate the 20th Fibonacci number',
  config: {
    codeExecution: true,
  },
});
```

### Generating Text and Images

Some Gemini models (like `gemini-2.5-flash-image`) can output images natively alongside text:

```typescript
const response = await ai.generate({
  model: googleAI.model('gemini-2.5-flash-image'),
  prompt: 'Create a picture of a futuristic city and describe it',
  config: {
    responseModalities: ['IMAGE', 'TEXT'],
  },
});
```

### Multimodal Capabilities

#### File Inputs and Gemini Files API

Gemini models support various file types. For small files, you can use inline data. For larger files (up to 2GB), use the Gemini Files API.

**Using Files API:**

```typescript
import { GoogleGenAI } from '@google/genai';
// ... init genaiClient ...

// Upload file
const uploadedFile = await genaiClient.files.upload({
  file: 'path/to/video.mp4',
  config: { mimeType: 'video/mp4' },
});

// Use in generation
const response = await ai.generate({
  model: googleAI.model('gemini-2.5-flash'),
  prompt: [
    { text: 'Describe this video' },
    {
      media: {
        contentType: uploadedFile.mimeType,
        url: uploadedFile.uri,
      },
    },
  ],
});
```

#### Video Understanding

Gemini models can process videos to describe content, answer questions, and refer to timestamps (in `MM:SS` format).

```typescript
const response = await ai.generate({
  model: googleAI.model('gemini-2.5-flash'),
  prompt: [
    { text: 'What happens at 00:05?' },
    { media: { contentType: 'video/mp4', url: 'https://youtube.com/watch?v=...' } },
  ],
});
```

**Video Processing Details:**
- **Sampling**: 1 frame per second (default)
- **Context**: 2M context models can handle up to 2 hours of video.
- **Inputs**: Up to 10 videos per request (Gemini 2.5+).

### Troubleshooting Language Models

**Schema Limitations**: The API uses a subset of OpenAPI 3.0. Avoid `z.union` or `z.record`; use explicit properties.
**Rate Limits**: Use exponential backoff or increase your quota.

## Embedding Models

### Available Models

- `gemini-embedding-001` - Latest Gemini embedding model (3072 dimensions, customizable)
- `text-embedding-004` - Text embedding model (768 dimensions, customizable)

### Usage

```typescript
const embeddings = await ai.embed({
  embedder: googleAI.embedder('gemini-embedding-001'),
  content: 'Machine learning models process data to make predictions.',
});

console.log(embeddings);
```

## Image Models

### Available Models

**Imagen 4 Series** - Latest generation with improved quality:
- `imagen-4.0-generate-001` - Standard quality
- `imagen-4.0-ultra-generate-001` - Ultra-high quality
- `imagen-4.0-fast-generate-001` - Fast generation

**Imagen 3 Series**:
- `imagen-3.0-generate-002`

### Usage

```typescript
const response = await ai.generate({
  model: googleAI.model('imagen-4.0-generate-001'),
  prompt: 'A serene Japanese garden with cherry blossoms and a koi pond.',
  config: {
    numberOfImages: 4,
    aspectRatio: '16:9',
    personGeneration: 'allow_adult',
    addWatermark: true,
  },
});

const generatedImage = response.media;
```


## Video Models

### Available Models (Veo)

- `veo-3.0-generate-001` - Latest Veo model
- `veo-3.0-fast-generate-001` - Fast video generation
- `veo-2.0-generate-001` - Previous generation

### Usage

#### Text-to-Video

```typescript
let { operation } = await ai.generate({
  model: googleAI.model('veo-3.0-generate-001'),
  prompt: 'A cinematic shot of a an old car driving down a deserted road at sunset.',
  config: {
    aspectRatio: '16:9',
    resolution: '1080p',
  },
});

// Polling for completion...
while (!operation.done) {
  operation = await ai.checkOperation(operation);
  await new Promise(r => setTimeout(r, 5000));
}

const video = operation.output?.message?.content.find(p => !!p.media);
```

#### Image-to-Video

Animate a static image:

```typescript
const response = await ai.generate({
  model: googleAI.model('veo-3.0-generate-001'),
  prompt: [
    { text: 'make the subject move' },
    { media: { contentType: 'image/jpeg', url: '...' } }
  ],
});
```

## Speech Models

### Available Models

- `gemini-2.5-flash-tts` - Flash model with TTS
- `gemini-2.5-pro-tts` - Pro model with TTS

### Usage

```typescript
const { media } = await ai.generate({
  model: googleAI.model('gemini-2.5-flash-tts'),
  prompt: 'Say that Genkit is amazing',
  config: {
    responseModalities: ['AUDIO'],
    speechConfig: {
      voiceConfig: {
        prebuiltVoiceConfig: { voiceName: 'Algenib' },
      },
    },
  },
});
```

</LanguageContent>

<LanguageContent lang="go">

## Provider Overview

The Google Generative AI plugin provides interfaces to Google's Gemini models through the Gemini API.

## Setup

To use this plugin, import the `googlegenai` package and pass `googlegenai.GoogleAI` to `WithPlugins()`:

```go
import "github.com/firebase/genkit/go/plugins/googlegenai"
```

```go
g := genkit.Init(context.Background(), genkit.WithPlugins(&googlegenai.GoogleAI{}))
```

**Authentication:**
Set the `GEMINI_API_KEY` environment variable to your API key, or specify it in initialization (not recommended for production code):

```go
genkit.WithPlugins(&googlegenai.GoogleAI{APIKey: "YOUR_API_KEY"})
```

## Language Models

To use a supported model, specify its identifier:

```go
model := googlegenai.GoogleAIModel(g, "gemini-2.5-flash")
```

Or with configuration:

```go
modelRef := googlegenai.GoogleAIModelRef("gemini-2.5-flash", &genai.GenerateContentConfig{
	Temperature: genai.Ptr[float32](0.5),
	MaxOutputTokens: genai.Ptr[int32](500),
})
```

Generate content:

```go
resp, err := genkit.Generate(ctx, g, ai.WithModel(modelRef), ai.WithPrompt("Tell me a joke."))
```

## Embedding Models

To use an embedding model:

```go
embeddingModel := googlegenai.GoogleAIEmbedder(g, "text-embedding-004")
```

Embed text:

```go
resp, err := genkit.Embed(ctx, g, ai.WithEmbedder(embeddingModel), ai.WithTextDocs(userInput))
```

## Image Models, Video Models, Speech Models

Image, Video, and Speech generation features are also supported through their respective model names (e.g., `imagen-4.0-generate-001`, `veo-3.0-generate-001`, `gemini-2.5-flash-tts`) following standard Genkit patterns.

</LanguageContent>

<LanguageContent lang="python">

## Provider Overview

The `genkit-plugin-google-genai` package provides access to Google's Gemini models via the Google Gemini API.

## Setup

```bash
pip3 install genkit-plugin-google-genai
```

Configuration:

```python
from genkit.ai import Genkit
from genkit.plugins.google_genai import GoogleAI

ai = Genkit(
  plugins=[GoogleAI()], # Reads GEMINI_API_KEY from env
  model='googleai/gemini-2.5-flash',
)
```

## Language Models

```python
response = await ai.generate('What should I do when I visit Melbourne?')
print(response.text)
```

## Embedding Models

```python
embeddings = await ai.embed(
    embedder='googleai/text-embedding-004',
    content='How many widgets do you have in stock?',
)
```

## Image Models

```python
response = await ai.generate(
    model='googleai/imagen-4.0-generate-001',
    prompt='a banana riding a bicycle',
)
```

## Video Models & Speech Models

Video and Speech models are supported via their model names (e.g., `googleai/veo-3.0-generate-001`, `googleai/gemini-2.5-flash-tts`).

</LanguageContent>
