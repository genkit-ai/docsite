---
title: Vertex AI plugin
description: Learn how to use Google Cloud Vertex AI with Genkit across JavaScript, Go, and Python, including enterprise features like grounding, Vector Search, Model Garden, and evaluation metrics.
---

import LanguageSelector from '../../../../components/LanguageSelector.astro';
import CopyMarkdownButton from '../../../../components/CopyMarkdownButton.astro';
import LanguageContent from '../../../../components/LanguageContent.astro';

<div style="display: flex; justify-content: space-between; align-items: center; gap: 1rem; margin: 1rem 0 1rem 0;">
  <LanguageSelector />
  <CopyMarkdownButton />
</div>

The Vertex AI plugin provides access to Google Cloud's enterprise-grade AI platform, offering advanced features beyond basic model access. Use this for enterprise applications that need grounding, Vector Search, Model Garden, or evaluation capabilities.

:::tip[Getting Started]
For simple API key access to Google's AI models, start with the [Google AI plugin](/docs/integrations/google-genai). This page covers enterprise features available through Vertex AI.
:::

## Accessing Google GenAI Models via Vertex AI

All languages support accessing Google's generative AI models (Gemini, Imagen, etc.) through Vertex AI with enterprise authentication and features.

<LanguageContent lang="js">

The unified Google GenAI plugin provides access to models via Vertex AI using the `vertexAI` initializer:

## Basic Model Access

### Installation

```bash
npm i --save @genkit-ai/google-genai
```

### Configuration

```typescript
import { genkit } from 'genkit';
import { vertexAI } from '@genkit-ai/google-genai';

const ai = genkit({
  plugins: [
    vertexAI({ location: 'us-central1' }), // Regional endpoint
    // vertexAI({ location: 'global' }),      // Global endpoint
  ],
});
```

**Authentication Methods:**

- **Application Default Credentials (ADC):** The standard method for most Vertex AI use cases, especially in production. It uses the credentials from the environment (e.g., service account on GCP, user credentials from `gcloud auth application-default login` locally). This method requires a Google Cloud Project with billing enabled and the Vertex AI API enabled.
- **Vertex AI Express Mode:** A streamlined way to try out many Vertex AI features using just an API key, without needing to set up billing or full project configurations. This is ideal for quick experimentation and has generous free tier quotas. [Learn More about Express Mode](https://cloud.google.com/vertex-ai/generative-ai/docs/start/express-mode/overview).

```typescript
// Using Vertex AI Express Mode (Easy to start, some limitations)
// Get an API key from the Vertex AI Studio Express Mode setup.
vertexAI({ apiKey: process.env.VERTEX_EXPRESS_API_KEY }),
```

_Note: When using Express Mode, you do not provide `projectId` and `location` in the plugin config._

### Basic Usage

```typescript
import { genkit } from 'genkit';
import { vertexAI } from '@genkit-ai/google-genai';

const ai = genkit({
  plugins: [vertexAI({ location: 'us-central1' })],
});

const response = await ai.generate({
  model: vertexAI.model('gemini-2.5-pro'),
  prompt: 'Explain Vertex AI in simple terms.',
});

console.log(response.text());
```

### Text Embedding

```typescript
const embeddings = await ai.embed({
  embedder: vertexAI.embedder('text-embedding-005'),
  content: 'Embed this text.',
});
```

### Image Generation (Imagen)

```typescript
const response = await ai.generate({
  model: vertexAI.model('imagen-3.0-generate-002'),
  prompt: 'A beautiful watercolor painting of a castle in the mountains.',
});

const generatedImage = response.media();
```

### Thinking Config

#### Thinking Level (Gemini 3.0)

```typescript
const response = await ai.generate({
  model: googleAI.model('gemini-3-pro-preview'),
  prompt: 'what is heavier, one kilo of steel or one kilo of feathers',
    config: {
      thinkingConfig: {
        thinkingLevel: 'HIGH',  // Or 'LOW' or 'MEDIUM'
    },
  },
});
```

#### Thinking Budget (Gemini 2.5)

```typescript
const { message } = await ai.generate({
  model: googleAI.model('gemini-2.5-pro'),
  prompt: 'what is heavier, one kilo of steel or one kilo of feathers',
  config: {
    thinkingConfig: {
      thinkingBudget: 1024,
      includeThoughts: true,
    },
  },
});
```

## Enterprise Features (JavaScript Only)

The following advanced features are available only in JavaScript using the dedicated `@genkit-ai/vertexai` plugin:

### Installation for Advanced Features

```bash
npm install @genkit-ai/vertexai
```

If you want to locally run flows that use this plugin, you also need the [Google Cloud CLI tool](https://cloud.google.com/sdk/docs/install) installed.

### Configuration for Advanced Features

```ts
import { genkit } from 'genkit';
import { vertexAI } from '@genkit-ai/vertexai';

const ai = genkit({
  plugins: [vertexAI({ location: 'us-central1' })],
});
```

The plugin requires you to specify your Google Cloud project ID, the [region](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/locations) to which you want to make Vertex API requests, and your Google Cloud project credentials.

- You can specify your Google Cloud project ID either by setting `projectId` in the `vertexAI()` configuration or by setting the `GCLOUD_PROJECT` environment variable. If you're running your flow from a Google Cloud environment (Cloud Functions, Cloud Run, and so on), `GCLOUD_PROJECT` is automatically set to the project ID of the environment.
- You can specify the API location either by setting `location` in the `vertexAI()` configuration or by setting the `GCLOUD_LOCATION` environment variable.
- To provide API credentials, you need to set up Google Cloud Application Default Credentials.

  1. To specify your credentials:

     - If you're running your flow from a Google Cloud environment (Cloud Functions, Cloud Run, and so on), this is set automatically.
     - On your local dev environment, do this by running:

       ```bash
       gcloud auth application-default login --project YOUR_PROJECT_ID
       ```

     - For other environments, see the [Application Default Credentials](https://cloud.google.com/docs/authentication/provide-credentials-adc) docs.

  1. In addition, make sure the account is granted the Vertex AI User IAM role (`roles/aiplatform.user`). See the Vertex AI [access control](https://cloud.google.com/vertex-ai/generative-ai/docs/access-control) docs.

### Grounding

This plugin supports grounding Gemini text responses using [Google Search](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/ground-gemini#web-ground-gemini) or [your own data](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/ground-gemini#private-ground-gemini).

Important: Vertex AI charges a fee for grounding requests in addition to the cost of making LLM requests. See the [Vertex AI pricing](https://cloud.google.com/vertex-ai/generative-ai/pricing) page and be sure you understand grounding request pricing before you use this feature.

Example:

```ts
const ai = genkit({
  plugins: [vertexAI({ location: 'us-central1' })],
});

await ai.generate({
  model: vertexAI.model('gemini-2.5-flash'),
  prompt: '...',
  config: {
    googleSearchRetrieval: {
      disableAttribution: true,
    }
    vertexRetrieval: {
      datastore: {
        projectId: 'your-cloud-project',
        location: 'us-central1',
        collection: 'your-collection',
      },
      disableAttribution: true,
    }
  }
})
```

### Context Caching

The Vertex AI Genkit plugin supports **Context Caching**, which allows models to reuse previously cached content to optimize token usage when dealing with large pieces of content. This feature is especially useful for conversational flows or scenarios where the model references a large piece of content consistently across multiple requests.

#### How to Use Context Caching

To enable context caching, ensure your model supports it. For example, `gemini-2.5-flash` and `gemini-2.0-pro` are models that support context caching, and you will have to specify version number `001`.

You can define a caching mechanism in your application like this:

```ts
const ai = genkit({
  plugins: [vertexAI({ location: 'us-central1' })],
});

const llmResponse = await ai.generate({
  messages: [
    {
      role: 'user',
      content: [{ text: 'Here is the relevant text from War and Peace.' }],
    },
    {
      role: 'model',
      content: [
        {
          text: "Based on War and Peace, here is some analysis of Pierre Bezukhov's character.",
        },
      ],
      metadata: {
        cache: {
          ttlSeconds: 300, // Cache this message for 5 minutes
        },
      },
    },
  ],
  model: vertexAI.model('gemini-2.5-flash'),
  prompt: "Describe Pierre's transformation throughout the novel.",
});
```

In this setup:

- **`messages`**: Allows you to pass conversation history.
- **`metadata.cache.ttlSeconds`**: Specifies the time-to-live (TTL) for caching a specific response.

#### Example: Leveraging Large Texts with Context

For applications referencing long documents, such as _War and Peace_ or _Lord of the Rings_, you can structure your queries to reuse cached contexts:

```ts
const textContent = await fs.readFile('path/to/war_and_peace.txt', 'utf-8');

const llmResponse = await ai.generate({
  messages: [
    {
      role: 'user',
      content: [{ text: textContent }], // Include the large text as context
    },
    {
      role: 'model',
      content: [
        {
          text: 'This analysis is based on the provided text from War and Peace.',
        },
      ],
      metadata: {
        cache: {
          ttlSeconds: 300, // Cache the response to avoid reloading the full text
        },
      },
    },
  ],
  model: vertexAI.model('gemini-2.5-flash'),
  prompt: 'Analyze the relationship between Pierre and Natasha.',
});
```

**Supported models**: `gemini-2.5-flash-001`, `gemini-2.0-pro-001`

### Model Garden Integration

Access third-party models through Vertex AI Model Garden:

#### Claude 3 Models

```ts
import { vertexAIModelGarden } from '@genkit-ai/vertexai/modelgarden';

const ai = genkit({
  plugins: [
    vertexAIModelGarden({
      location: 'us-central1',
      models: ['claude-3-haiku', 'claude-3-sonnet', 'claude-3-opus'],
    }),
  ],
});

const response = await ai.generate({
  model: 'claude-3-sonnet',
  prompt: 'What should I do when I visit Melbourne?',
});
```

#### Llama 3.1 405b

```ts
const ai = genkit({
  plugins: [
    vertexAIModelGarden({
      location: 'us-central1',
      models: ['llama3-405b-instruct-maas'],
    }),
  ],
});

const response = await ai.generate({
  model: 'llama3-405b-instruct-maas',
  prompt: 'Write a function that adds two numbers together',
});
```

#### Mistral Models

```ts
const ai = genkit({
  plugins: [
    vertexAIModelGarden({
      location: 'us-central1',
      models: ['mistral-large', 'mistral-nemo', 'codestral'],
    }),
  ],
});

const response = await ai.generate({
  model: 'mistral-large',
  prompt: 'Write a function that adds two numbers together',
  config: {
    version: 'mistral-large-2411',
    temperature: 0.7,
    maxOutputTokens: 1024,
    topP: 0.9,
    stopSequences: ['###'],
  },
});
```

The models support:

- `mistral-large`: Latest Mistral large model with function calling capabilities
- `mistral-nemo`: Optimized for efficiency and speed
- `codestral`: Specialized for code generation tasks

### Evaluation Metrics

Use Vertex AI Rapid Evaluation API for model evaluation:

```ts
import { vertexAIEvaluation, VertexAIEvaluationMetricType } from '@genkit-ai/vertexai/evaluation';

const ai = genkit({
  plugins: [
    vertexAIEvaluation({
      location: 'us-central1',
      metrics: [
        VertexAIEvaluationMetricType.SAFETY,
        {
          type: VertexAIEvaluationMetricType.ROUGE,
          metricSpec: {
            rougeType: 'rougeLsum',
          },
        },
      ],
    }),
  ],
});
```

Available metrics:

- **BLEU**: Translation quality
- **ROUGE**: Summarization quality
- **Fluency**: Text fluency
- **Safety**: Content safety
- **Groundedness**: Factual accuracy
- **Summarization Quality/Helpfulness/Verbosity**: Summary evaluation

Run evaluations:

```bash
genkit eval:run
genkit eval:flow -e vertexai/safety
```

### Vector Search

Use Vertex AI Vector Search for enterprise-grade vector operations:

#### Setup

1. Create a Vector Search index in the [Google Cloud Console](https://console.cloud.google.com/vertex-ai/matching-engine/indexes)
2. Configure dimensions based on your embedding model:
   - `gemini-embedding-001`: 768 dimensions
   - `text-multilingual-embedding-002`: 768 dimensions
   - `multimodalEmbedding001`: 128, 256, 512, or 1408 dimensions
3. Deploy the index to a standard endpoint

#### Configuration

```ts
import { vertexAIVectorSearch } from '@genkit-ai/vertexai/vectorsearch';
import { getFirestoreDocumentIndexer, getFirestoreDocumentRetriever } from '@genkit-ai/vertexai/vectorsearch';

const ai = genkit({
  plugins: [
    vertexAIVectorSearch({
      projectId: 'your-project-id',
      location: 'us-central1',
      vectorSearchOptions: [
        {
          indexId: 'your-index-id',
          indexEndpointId: 'your-endpoint-id',
          deployedIndexId: 'your-deployed-index-id',
          publicDomainName: 'your-domain-name',
          documentRetriever: firestoreDocumentRetriever,
          documentIndexer: firestoreDocumentIndexer,
          embedder: vertexAI.embedder('gemini-embedding-001'),
        },
      ],
    }),
  ],
});
```

#### Usage

```ts
import { vertexAiIndexerRef, vertexAiRetrieverRef } from '@genkit-ai/vertexai/vectorsearch';

// Index documents
await ai.index({
  indexer: vertexAiIndexerRef({
    indexId: 'your-index-id',
  }),
  documents,
});

// Retrieve similar documents
const results = await ai.retrieve({
  retriever: vertexAiRetrieverRef({
    indexId: 'your-index-id',
  }),
  query: queryDocument,
});
```

:::caution[Pricing]
Vector Search has both ingestion and hosting costs. See [Vertex AI pricing](https://cloud.google.com/vertex-ai/pricing#vectorsearch) for details.
:::

## Next Steps

- Learn about [generating content](/docs/models) to understand how to use these models effectively
- Explore [evaluation](/docs/evaluation) to leverage Vertex AI's evaluation metrics
- See [RAG](/docs/rag) to implement retrieval-augmented generation with Vector Search
- Check out [creating flows](/docs/flows) to build structured AI workflows
- For simple API key access, see the [Google AI plugin](/docs/integrations/google-genai)

</LanguageContent>

<LanguageContent lang="go">

The Google Generative AI plugin provides access to Google's Gemini models through Vertex AI using Google Cloud authentication.

## Configuration

To use this plugin, import the `googlegenai` package and pass `googlegenai.VertexAI` to `WithPlugins()` in the Genkit initializer:

```go
import "github.com/firebase/genkit/go/plugins/googlegenai"
```

```go
g := genkit.Init(context.Background(), genkit.WithPlugins(&googlegenai.VertexAI{}))
```

The plugin requires you to specify your Google Cloud project ID, the [region](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/locations) to which you want to make Vertex API requests, and your Google Cloud project credentials.

- By default, `googlegenai.VertexAI` gets your Google Cloud project ID from the `GOOGLE_CLOUD_PROJECT` environment variable.

  You can also pass this value directly:

  ```go
  genkit.WithPlugins(&googlegenai.VertexAI{ProjectID: "my-project-id"})
  ```

- By default, `googlegenai.VertexAI` gets the Vertex AI API location from the `GOOGLE_CLOUD_LOCATION` environment variable.

  You can also pass this value directly:

  ```go
  genkit.WithPlugins(&googlegenai.VertexAI{Location: "us-central1"})
  ```

- To provide API credentials, you need to set up Google Cloud Application Default Credentials.

  1. To specify your credentials:

     - If you're running your flow from a Google Cloud environment (Cloud Functions, Cloud Run, and so on), this is set automatically.

     - On your local dev environment, do this by running:

       ```shell
       gcloud auth application-default login
       ```

     - For other environments, see the [Application Default Credentials](https://cloud.google.com/docs/authentication/provide-credentials-adc) docs.

  2. In addition, make sure the account is granted the Vertex AI User IAM role (`roles/aiplatform.user`). See the Vertex AI [access control](https://cloud.google.com/vertex-ai/generative-ai/docs/access-control) docs.

## Usage

### Generative models

To get a reference to a supported model, specify its identifier to `googlegenai.VertexAIModel`:

```go
model := googlegenai.VertexAIModel(g, "gemini-2.5-flash")
```

Alternatively, you may create a `ModelRef` which pairs the model name with its config:

```go
modelRef := googlegenai.VertexAIModelRef("gemini-2.5-flash", &genai.GenerateContentConfig{
	Temperature: genai.Ptr[float32](0.5),
	MaxOutputTokens: genai.Ptr[int32](500),
	// Other configuration...
})
```

The following models are supported: `gemini-1.5-pro`, `gemini-1.5-flash`, `gemini-2.0-pro`, `gemini-2.5-flash`, and other experimental models.

Model references have a `Generate()` method that calls the Vertex AI API:

```go
resp, err := genkit.Generate(ctx, g, ai.WithModel(modelRef), ai.WithPrompt("Tell me a joke."))
if err != nil {
	return err
}

log.Println(resp.Text())
```

See [Generating content with AI models](/docs/models) for more information.

### Embedding models

To get a reference to a supported embedding model, specify its identifier to `googlegenai.VertexAIEmbedder`:

```go
embeddingModel := googlegenai.VertexAIEmbedder(g, "text-embedding-004")
```

The following models are supported:

- `textembedding-gecko@003`, `textembedding-gecko@002`, `textembedding-gecko@001`, `text-embedding-004`, `textembedding-gecko-multilingual@001`, `text-multilingual-embedding-002`, and `multimodalembedding`

Embedder references have an `Embed()` method that calls the Vertex AI API:

```go
resp, err := genkit.Embed(ctx, g, ai.WithEmbedder(embeddingModel), ai.WithTextDocs(userInput))
if err != nil {
	return err
}
```

You can retrieve docs by passing in an input to a Retriever's `Retrieve()` method:

```go
resp, err := genkit.Retrieve(ctx, g, ai.WithRetriever(myRetriever), ai.WithTextDocs(userInput))
if err != nil {
	return err
}
```

See [Retrieval-augmented generation (RAG)](/docs/rag) for more information.

## Advanced Features

Advanced Vertex AI features like Vector Search, Model Garden, and evaluation metrics require custom implementation using the Google Cloud SDK directly. See the [Vertex AI documentation](https://cloud.google.com/vertex-ai/docs) for implementation details.

## Next Steps

- Learn about [generating content](/docs/models) to understand how to use these models effectively
- Explore [evaluation](/docs/evaluation) to leverage Vertex AI's evaluation metrics
- See [RAG](/docs/rag) to implement retrieval-augmented generation with Vector Search
- Check out [creating flows](/docs/flows) to build structured AI workflows
- For simple API key access, see the [Google AI plugin](/docs/integrations/google-genai)

</LanguageContent>

<LanguageContent lang="python">

The unified Google GenAI plugin provides access to models via Vertex AI using the `VertexAI` initializer:

## Basic Model Access

### Installation

```bash
uv add genkit-plugin-google-genai
```

### Configuration

```python
from genkit import Genkit
from genkit.plugins.google_genai import VertexAI

ai = Genkit(
    plugins=[
        VertexAI(location='us-central1'),  # Regional endpoint
        # VertexAI(location='global'),      # Global endpoint
    ],
)
```

**Authentication Methods:**

- **Application Default Credentials (ADC):** The standard method for most Vertex AI use cases, especially in production. It uses the credentials from the environment (e.g., service account on GCP, user credentials from `gcloud auth application-default login` locally). This method requires a Google Cloud Project with billing enabled and the Vertex AI API enabled.
- **Vertex AI Express Mode:** A streamlined way to try out many Vertex AI features using just an API key, without needing to set up billing or full project configurations. This is ideal for quick experimentation and has generous free tier quotas. [Learn More about Express Mode](https://cloud.google.com/vertex-ai/generative-ai/docs/start/express-mode/overview).

```python
# Using Vertex AI Express Mode (Easy to start, some limitations)
# Get an API key from the Vertex AI Studio Express Mode setup.
import os
VertexAI(api_key=os.environ.get('VERTEX_EXPRESS_API_KEY'))
```

_Note: When using Express Mode, you do not provide `project` and `location` in the plugin config._

### Basic Usage

```python
from genkit import Genkit
from genkit.plugins.google_genai import VertexAI

ai = Genkit(
    plugins=[VertexAI(location='us-central1')],
)

response = await ai.generate(
    model='vertexai/gemini-2.5-pro',
    prompt='Explain Vertex AI in simple terms.',
)

print(response.text)
```

### Text Embedding

```python
embeddings = await ai.embed(
    embedder='vertexai/text-embedding-005',
    content='Embed this text.',
)
```

### Image Generation (Imagen)

```python
response = await ai.generate(
    model='vertexai/imagen-3.0-generate-002',
    prompt='A beautiful watercolor painting of a castle in the mountains.',
)

if response.message and response.message.content:
    media_part = response.message.content[0]
    generated_image = media_part.root.media.url
```

### Thinking Config

#### Thinking Level (Gemini 3.0)

```python
response = await ai.generate(
    model='vertexai/gemini-3-pro-preview',
    prompt='what is heavier, one kilo of steel or one kilo of feathers',
    config={
        'thinking_config': {
            'thinking_level': 'HIGH',  # Or 'LOW' or 'MEDIUM'
        },
    },
)
```

#### Thinking Budget (Gemini 2.5)

```python
message = (await ai.generate(
    model='vertexai/gemini-2.5-pro',
    prompt='what is heavier, one kilo of steel or one kilo of feathers',
    config={
        'thinking_config': {
            'thinking_budget': 1024,
            'include_thoughts': True,
        },
    },
)).message
```

## Enterprise Features (Python)

The following advanced features are available in Python. Note that some features require additional plugin packages:

### Installation for Advanced Features

**Core Vertex AI features** (included in `genkit-plugin-google-genai`):

```bash
uv add genkit-plugin-google-genai
```

**Model Garden and Vector Search** (requires separate plugin):

```bash
uv add genkit-plugin-vertex-ai
```

**Evaluation Metrics** (requires separate plugin):

```bash
uv add genkit-plugin-evaluators
```

If you want to locally run flows that use these plugins, you also need the [Google Cloud CLI tool](https://cloud.google.com/sdk/docs/install) installed.

### Configuration for Advanced Features

```python
from genkit import Genkit
from genkit.plugins.google_genai import VertexAI

ai = Genkit(
    plugins=[VertexAI(location='us-central1')],
)
```

The plugin requires you to specify your Google Cloud project ID, the [region](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/locations) to which you want to make Vertex API requests, and your Google Cloud project credentials.

- You can specify your Google Cloud project ID either by setting `project` in the `VertexAI()` configuration or by setting the `GCLOUD_PROJECT` environment variable. If you're running your flow from a Google Cloud environment (Cloud Functions, Cloud Run, and so on), `GCLOUD_PROJECT` is automatically set to the project ID of the environment.
- You can specify the API location either by setting `location` in the `VertexAI()` configuration or by setting the `GCLOUD_LOCATION` environment variable.
- To provide API credentials, you need to set up Google Cloud Application Default Credentials.

  1. To specify your credentials:

     - If you're running your flow from a Google Cloud environment (Cloud Functions, Cloud Run, and so on), this is set automatically.
     - On your local dev environment, do this by running:

       ```bash
       gcloud auth application-default login --project YOUR_PROJECT_ID
       ```

     - For other environments, see the [Application Default Credentials](https://cloud.google.com/docs/authentication/provide-credentials-adc) docs.

  1. In addition, make sure the account is granted the Vertex AI User IAM role (`roles/aiplatform.user`). See the Vertex AI [access control](https://cloud.google.com/vertex-ai/generative-ai/docs/access-control) docs.

### Grounding

This plugin supports grounding Gemini text responses using [Google Search](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/ground-gemini#web-ground-gemini).

Important: Vertex AI charges a fee for grounding requests in addition to the cost of making LLM requests. See the [Vertex AI pricing](https://cloud.google.com/vertex-ai/generative-ai/pricing) page and be sure you understand grounding request pricing before you use this feature.

Example:

```python
ai = Genkit(
    plugins=[VertexAI(location='us-central1')],
)

await ai.generate(
    model='vertexai/gemini-2.5-flash',
    prompt='What are the latest developments in quantum computing?',
    config={
        'google_search_retrieval': {
            'disable_attribution': True,
        },
    }
)
```

### Context Caching

The Vertex AI Genkit plugin supports **Context Caching**, which allows models to reuse previously cached content to optimize token usage when dealing with large pieces of content. This feature is especially useful for conversational flows or scenarios where the model references a large piece of content consistently across multiple requests.

#### How to Use Context Caching

To enable context caching, ensure your model supports it. For example, `gemini-2.5-flash` and `gemini-2.0-pro` are models that support context caching, and you will have to specify version number `001`.

You can define a caching mechanism in your application like this:

```python
from genkit import Message, Part, TextPart, Role

ai = Genkit(
    plugins=[VertexAI(location='us-central1')],
)

llm_response = await ai.generate(
    messages=[
        Message(
            role=Role.USER,
            content=[Part(root=TextPart(text='Here is the relevant text from War and Peace.'))],
        ),
        Message(
            role=Role.MODEL,
            content=[
                Part(root=TextPart(text="Based on War and Peace, here is some analysis of Pierre Bezukhov's character.")),
            ],
            metadata={
                'cache': {
                    'ttl_seconds': 300,  # Cache this message for 5 minutes
                },
            },
        ),
    ],
    model='vertexai/gemini-2.5-flash',
    prompt="Describe Pierre's transformation throughout the novel.",
)
```

In this setup:

- **`messages`**: Allows you to pass conversation history.
- **`metadata.cache.ttl_seconds`**: Specifies the time-to-live (TTL) for caching a specific response.

#### Example: Leveraging Large Texts with Context

For applications referencing long documents, such as _War and Peace_ or _Lord of the Rings_, you can structure your queries to reuse cached contexts:

```python
from pathlib import Path
from genkit import Message, Part, TextPart, Role

text_content = Path('path/to/war_and_peace.txt').read_text()

llm_response = await ai.generate(
    messages=[
        Message(
            role=Role.USER,
            content=[Part(root=TextPart(text=text_content))],  # Include the large text as context
        ),
        Message(
            role=Role.MODEL,
            content=[
                Part(root=TextPart(text='This analysis is based on the provided text from War and Peace.')),
            ],
            metadata={
                'cache': {
                    'ttl_seconds': 300,  # Cache the response to avoid reloading the full text
                },
            },
        ),
    ],
    model='vertexai/gemini-2.5-flash',
    prompt='Analyze the relationship between Pierre and Natasha.',
)
```

**Supported models**: `gemini-2.5-flash-001`, `gemini-2.0-pro-001`

### Model Garden Integration

Access third-party models through Vertex AI Model Garden using the separate `vertex_ai` plugin:

**Installation:**

```bash
uv add genkit-plugin-vertex-ai
```

#### Claude 3 Models

```python
from genkit import Genkit
from genkit.plugins.vertex_ai import ModelGardenPlugin

ai = Genkit(
    plugins=[
        ModelGardenPlugin(
            location='us-central1',
            models=['claude-3-haiku', 'claude-3-sonnet', 'claude-3-opus'],
        ),
    ],
)

response = await ai.generate(
    model='claude-3-sonnet',
    prompt='What should I do when I visit Melbourne?',
)
```

#### Llama 3.1 405b

```python
ai = Genkit(
    plugins=[
        ModelGardenPlugin(
            location='us-central1',
            models=['llama3-405b-instruct-maas'],
        ),
    ],
)

response = await ai.generate(
    model='llama3-405b-instruct-maas',
    prompt='Write a function that adds two numbers together',
)
```

#### Mistral Models

```python
ai = Genkit(
    plugins=[
        ModelGardenPlugin(
            location='us-central1',
            models=['mistral-large', 'mistral-small'],
        ),
    ],
)

response = await ai.generate(
    model='mistral-large',
    prompt='Explain quantum computing',
)
```

Vertex AI provides access to various third-party models through Model Garden. Consult the [Vertex AI Model Garden documentation](https://cloud.google.com/vertex-ai/generative-ai/docs/model-garden/explore-models) for the full list of supported models and their capabilities.

### Evaluation Metrics

Genkit provides evaluation metrics through the evaluators plugin:

**Installation:**

```bash
uv add genkit-plugin-evaluators
```

**Usage:**

```python
from genkit import Genkit
from genkit.plugins.google_genai import VertexAI
from genkit.plugins.evaluators import define_genkit_evaluators, GenkitMetricType, MetricConfig
from genkit.blocks.model import ModelReference

ai = Genkit(
    plugins=[VertexAI(location='us-central1')],
)

# Register evaluators using the evaluators plugin
define_genkit_evaluators(
    ai,
    [
        MetricConfig(
            metric_type=GenkitMetricType.FAITHFULNESS,
            judge=ModelReference(name='vertexai/gemini-2.5-flash'),
        ),
        MetricConfig(
            metric_type=GenkitMetricType.ANSWER_RELEVANCY,
            judge=ModelReference(name='vertexai/gemini-2.5-flash'),
        ),
        MetricConfig(
            metric_type=GenkitMetricType.MALICIOUSNESS,
            judge=ModelReference(name='vertexai/gemini-2.5-flash'),
        ),
    ],
)
```

Available built-in metrics from the evaluators plugin include:

- **Answer Relevancy**: Measures how relevant the answer is to the question
- **Faithfulness**: Evaluates if the answer is grounded in the provided context
- **Maliciousness**: Detects harmful or malicious content
- **Regex Match**: Pattern-based validation
- **Deep Equal**: Exact match comparison

See the [evaluation documentation](/docs/evaluation) for more details on implementing comprehensive evaluation workflows.

### Vector Search

Vertex AI Vector Search enables efficient similarity search for RAG applications using the separate `vertex_ai` plugin:

**Installation:**

```bash
uv add genkit-plugin-vertex-ai
```

#### Setup

1. Create a Vector Search index in your Google Cloud project
2. Deploy the index to an endpoint
3. Configure the retriever in your Genkit application

#### Configuration

**Using Firestore:**

```python
from genkit import Genkit
from genkit.plugins.google_genai import VertexAI
from genkit.plugins.vertex_ai import define_vertex_vector_search_firestore

ai = Genkit(
    plugins=[VertexAI(location='us-central1')],
)

from google.cloud import firestore
firestore_client = firestore.AsyncClient()

# Define a Firestore-backed retriever
retriever = define_vertex_vector_search_firestore(
    ai,
    name='my_retriever',
    collection_name='documents',
    embedder='vertexai/text-embedding-004',
    firestore_client=firestore_client,
)
```

**Using BigQuery:**

```python
from genkit.plugins.vertex_ai import define_vertex_vector_search_big_query

from google.cloud import bigquery
bq_client = bigquery.Client()

# Define a BigQuery-backed retriever
retriever = define_vertex_vector_search_big_query(
    ai,
    name='bigquery_retriever',
    dataset_id='my_dataset',
    table_id='my_table',
    embedder='vertexai/text-embedding-004',
    bq_client=bq_client,
)
```

#### Usage

```python
# Use the retriever in a RAG flow
docs = await ai.retrieve(retriever='my_retriever', query='your search query')
response = await ai.generate(
    prompt='How do I use the vector search feature?',
    docs=docs.documents,
)
```

See the [RAG documentation](/docs/rag) for comprehensive examples of implementing retrieval-augmented generation with Vertex AI Vector Search.

## Next Steps

- Learn about [generating content](/docs/models) to understand how to use these models effectively
- Explore [evaluation](/docs/evaluation) to leverage Vertex AI's evaluation metrics
- See [RAG](/docs/rag) to implement retrieval-augmented generation with Vector Search
- Check out [creating flows](/docs/flows) to build structured AI workflows
- For simple API key access, see the [Google AI plugin](/docs/integrations/google-genai)

</LanguageContent>
