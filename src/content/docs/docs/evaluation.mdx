---
title: Evaluation
description: Learn about Genkit's evaluation capabilities across JavaScript and Go, including inference-based and raw evaluation, dataset creation, and how to use the Developer UI and CLI for testing and analysis.
---

import LanguageSelector from '../../../components/LanguageSelector.astro';
import CopyMarkdownButton from '../../../components/CopyMarkdownButton.astro';
import LanguageContent from '../../../components/LanguageContent.astro';
import ThemeImage from '../../../components/ThemeImage.astro';

<div style="display: flex; justify-content: space-between; align-items: center; gap: 1rem; margin: 1rem 0 1rem 0;">
	<LanguageSelector supportedLanguages="js go python" />
	<CopyMarkdownButton />
</div>

<LanguageContent lang="js">

Evaluation is a form of testing that helps you validate your LLM's responses and
ensure they meet your quality bar.

Genkit supports third-party evaluation tools through plugins, paired
with powerful observability features that provide insight into the runtime state
of your LLM-powered applications. Genkit tooling helps you automatically extract
data including inputs, outputs, and information from intermediate steps to
evaluate the end-to-end quality of LLM responses as well as understand the
performance of your system's building blocks.

### Types of evaluation

Genkit supports two types of evaluation:

- **Inference-based evaluation**: This type of evaluation runs against a
  collection of pre-determined inputs, assessing the corresponding outputs for
  quality.

    This is the most common evaluation type, suitable for most use cases. This approach tests a system's actual output for each evaluation run.

    You can perform the quality assessment manually, by visually inspecting the results. Alternatively, you can automate the assessment by using an evaluation metric.

- **Raw evaluation**: This type of evaluation directly assesses the quality of
  inputs without any inference. This approach typically is used with automated
  evaluation using metrics. All required fields for evaluation (e.g., `input`,
  `context`, `output` and `reference`) must be present in the input dataset. This
  is useful when you have data coming from an external source (e.g., collected
  from your production traces) and you want to have an objective measurement of
  the quality of the collected data.

    For more information, see the [Advanced use](#advanced-use) section of this page.

This section explains how to perform inference-based evaluation using Genkit.

## Quick start

### Setup

1.  Use an existing Genkit app or create a new one by following our [Get started](/docs/get-started) guide.
2.  Add the following code to define a simple RAG application to evaluate. For this guide, we use a dummy retriever that always returns the same documents.

    ```js
    import { genkit, z, Document } from 'genkit';
    import { googleAI } from '@genkit-ai/google-genai';

    // Initialize Genkit
    export const ai = genkit({ plugins: [googleAI()] });

    // Dummy retriever that always returns the same docs
    export const dummyRetriever = ai.defineRetriever(
    	{
    		name: 'dummyRetriever',
    	},
    	async (i) => {
    		const facts = ["Dog is man's best friend", 'Dogs have evolved and were domesticated from wolves'];
    		// Just return facts as documents.
    		return { documents: facts.map((t) => Document.fromText(t)) };
    	},
    );

    // A simple question-answering flow
    export const qaFlow = ai.defineFlow(
    	{
    		name: 'qaFlow',
    		inputSchema: z.object({ query: z.string() }),
    		outputSchema: z.object({ answer: z.string() }),
    	},
    	async ({ query }) => {
    		const factDocs = await ai.retrieve({
    			retriever: dummyRetriever,
    			query,
    		});

    		const { text } = await ai.generate({
    			model: googleAI.model('gemini-2.5-flash'),
    			prompt: `Answer this question with the given context ${query}`,
    			docs: factDocs,
    		});
    		return { answer: text };
    	},
    );
    ```

3.  (Optional) Add evaluation metrics to your application to use while evaluating. This guide uses the `MALICIOUSNESS` metric from the `genkitEval` plugin.

    ```js
    import { genkitEval, GenkitMetric } from '@genkit-ai/evaluator';
    import { googleAI } from '@genkit-ai/google-genai';

    export const ai = genkit({
    	plugins: [
    		googleAI(),
    		// Add this plugin to your Genkit initialization block
    		genkitEval({
    			judge: googleAI.model('gemini-2.5-flash'),
    			metrics: [GenkitMetric.MALICIOUSNESS],
    		}),
    	],
    });
    ```

    **Note:** The configuration above requires installation of the [`@genkit-ai/evaluator`](https://www.npmjs.com/package/@genkit-ai/evaluator) package.

    ```bash
    npm install @genkit-ai/evaluator
    ```

4.  Start your Genkit application.

    ```bash
    genkit start -- <command to start your app>
    ```

### Create a dataset

Create a dataset to define the examples we want to use for evaluating our flow.

1. Go to the Dev UI at `http://localhost:4000` and click the **Datasets** button
   to open the Datasets page.

2. Click on the **Create Dataset** button to open the create dataset dialog.

    a. Provide a `datasetId` for your new dataset. This guide uses
    `myFactsQaDataset`.

    b. Select `Flow` dataset type.

    c. Leave the validation target field empty and click **Save**

3. Your new dataset page appears, showing an empty dataset. Add examples to it by following these steps:

    a. Click the **Add example** button to open the example editor panel.

    b. Only the `input` field is required. Enter `{"query": "Who is man's best friend?"}` in the `input` field, and click **Save** to add the example has to your dataset.

    c. Repeat steps (a) and (b) a couple more times to add more examples. This guide adds the following example inputs to the dataset:

    ```
    {"query": "Can I give milk to my cats?"}
    {"query": "From which animals did dogs evolve?"}
    ```

By the end of this step, your dataset should have 3 examples in it, with the
values mentioned above.

### Run evaluation and view results

To start evaluating the flow, click the **Run new evaluation** button on your
dataset page. You can also start a new evaluation from the _Evaluations_ tab.

1. Select the `Flow` radio button to evaluate a flow.

2. Select `qaFlow` as the target flow to evaluate.

3. Select `myFactsQaDataset` as the target dataset to use for evaluation.

4. (Optional) If you have installed an evaluator metric using Genkit plugins,
   you can see these metrics in this page. Select the metrics that you want to use
   with this evaluation run. This is entirely optional: Omitting this step will
   still return the results in the evaluation run, but without any associated
   metrics.

5. Finally, click **Run evaluation** to start evaluation. Depending on the flow
   you're testing, this may take a while. Once the evaluation is complete, a
   success message appears with a link to view the results. Click on the link to go
   to the _Evaluation details_ page.

You can see the details of your evaluation on this page, including original
input, extracted context and metrics (if any).

## Core concepts

### Terminology

- **Evaluation**: An evaluation is a process that assesses system performance.
  In Genkit, such a system is usually a Genkit primitive, such as a flow, a
  prompt, or a model. An evaluation can be automated or manual (human
  evaluation).

- **Bulk inference** Inference is the act of running an input on a flow or model to get the corresponding output. Bulk inference involves performing inference on multiple inputs simultaneously.

- **Metric** An evaluation metric is a criterion on which an inference is scored. Examples include accuracy, faithfulness, maliciousness, whether the output is in English, etc.

- **Dataset** A dataset is a collection of examples to use for inference-based  
  evaluation. A dataset typically consists of `input` and optional `reference`
  fields. The `reference` field does not affect the inference step of evaluation
  but it is passed verbatim to any evaluation metrics. In Genkit, you can create
  a dataset through the Dev UI. There are three types of datasets in Genkit:
  _Flow_ datasets, _Model_ datasets, and _Prompt_ datasets.

### Schema validation

Depending on the type, datasets have schema validation support in the Dev UI:

- Flow datasets support validation of the `input` and `reference` fields of the dataset against a flow in the Genkit application. Schema validation is optional and is only enforced if a schema is specified on the target flow.

- Prompt datasets support validation of the `input` field against the prompt's input schema.

- Model datasets have implicit schema, supporting both `string` and `GenerateRequest` input types. String validation provides a convenient way to evaluate simple text prompts, while `GenerateRequest` provides complete control for advanced use cases (e.g. providing model parameters, message history, tools, etc). You can find the full schema for `GenerateRequest` in our [API reference docs](https://js.api.genkit.dev/interfaces/genkit._.GenerateRequest.html).

Note: Schema validation is a helper tool for editing examples, but it is
possible to save an example with invalid schema. These examples may fail when
the running an evaluation.

:::note[Evaluating Prompts]
When evaluating a prompt, Genkit executes the prompt against the inputs in your dataset. If your prompt definition includes multiple variants (e.g., different model configurations or instructions), the Developer UI allows you to select the specific variant you want to evaluate. This enables A/B testing of different prompt strategies.

If variants have different input schemas, schema validation will be performed against the schema of the currently selected variant.
:::

## Supported evaluators

### Genkit evaluators

Genkit includes a small number of native evaluators, inspired by [RAGAS](https://docs.ragas.io/en/stable/), to help you get started:

- Faithfulness -- Measures the factual consistency of the generated answer against the given context
- Answer Relevancy -- Assesses how pertinent the generated answer is to the given prompt
- Maliciousness -- Measures whether the generated output intends to deceive, harm, or exploit

### Evaluator plugins

Genkit supports additional evaluators through plugins, like the Vertex Rapid Evaluators, which you can access via the [VertexAI Plugin](/docs/integrations/vertex-ai#evaluation-metrics).

### Custom Evaluators

You can extend Genkit to support custom evaluation by defining your own evaluator functions. An evaluator can use an LLM as a judge, perform programmatic (heuristic) checks, or call external APIs to assess the quality of a response.

You define a custom evaluator using the `ai.defineEvaluator` method. The callback function for the evaluator can contain any logic you need.

Here's an example of a custom evaluator that uses an LLM to check for "deliciousness":

```typescript
import { googleAI } from '@genkit-ai/google-genai';
import { BaseEvalDataPoint } from 'genkit/evaluator';

export const customFoodEvaluator = ai.defineEvaluator(
	{
		name: `custom/foodEvaluator`,
		displayName: 'Food Evaluator',
		definition: 'Determines if an output is a delicious food item.',
	},
	async (datapoint: BaseEvalDataPoint) => {
		if (!datapoint.output || typeof datapoint.output !== 'string') {
			throw new Error('String output is required for food evaluation');
		}

		// You can use an LLM as a judge for more complex evaluations.
		const { text } = await ai.generate({
			model: googleAI.model('gemini-2.5-flash'),
			prompt: `Is the following food delicious? Respond with "yes", "no", or "maybe". Food: ${datapoint.output}`,
		});

		// You can also perform any custom logic in the evaluator.
		// if (datapoint.output.includes("marmite")) {
		//     handleMarmite();
		// }
		// or...
		// const score = await myApi.evaluate({
		//      type: 'deliciousness',
		//      value: datapoint.output
		// });

		return {
			testCaseId: datapoint.testCaseId,
			evaluation: { score: text },
		};
	},
);
```

You can then use this custom evaluator just like any other Genkit evaluator. You
can use them with your datasets in the Dev UI or with the CLI in the `eval:run`
or `eval:flow` commands:

```bash
genkit eval:flow myFlow --input myDataset.json --evaluators=custom/foodEvaluator
```

## Advanced use

### Evaluation comparison

The Developer UI offers visual tools for side-by-side comparison of multiple
evaluation runs. This feature allows you to analyze variations across different
executions within a unified interface, making it easier to assess changes in
output quality. Additionally, you can highlight outputs based on the performance
of specific metrics, indicating improvements or regressions.

When comparing evaluations, one run is designated as the _Baseline_. All other
evaluations are compared against this baseline to determine whether their
performance has improved or regressed.

<ThemeImage
	lightSrc="/src/assets/evals_compare_light.png"
	darkSrc="/src/assets/evals_compare_dark.png"
	alt="Evaluation comparison with metric highlighting"
/>

#### Prerequisites

To use the evaluation comparison feature, the following conditions must be met:

- Evaluations must originate from a dataset source. Evaluations from file
  sources are not comparable.
- All evaluations being compared must be from the same dataset.
- For metric highlighting, all evaluations must use at least one common
  metric that produces a `number` or `boolean` score.

#### Comparing evaluations

1.  Ensure you have at least two evaluation runs performed on the same dataset.
    For instructions, refer to the
    [Run evaluation section](#run-evaluation-and-view-results).

2.  In the Developer UI, navigate to the **Datasets** page.

3.  Select the relevant dataset and open its **Evaluations** tab. You should see
    all evaluation runs associated with that dataset.

4.  Choose one evaluation to serve as the baseline for comparison.

5.  On the evaluation results page, click the **+ Comparison** button. If this
    button is disabled, it means no other comparable evaluations are available
    for this dataset.

6.  A new column will appear with a dropdown menu. Select another evaluation
    from this menu to load its results alongside the baseline.

You can now view the outputs side-by-side to visually inspect differences in
quality. This feature supports comparing up to three evaluations simultaneously.

##### Metric highlighting (Optional)

If your evaluations include metrics, you can enable metric highlighting to
color-code the results. This feature helps you quickly identify changes in
performance: improvements are colored green, while regressions are red.

Note that highlighting is only supported for numeric and boolean metrics, and
the selected metric must be present in all evaluations being compared.

To enable metric highlighting:

1.  After initiating a comparison, a **Choose a metric to compare** menu will
    become available.

2.  Select a metric from the dropdown. By default, lower scores (for numeric
    metrics) and `false` values (for boolean metrics) are considered
    improvements and highlighted in green. You can reverse this logic by
    ticking the checkbox in the menu.

The comparison columns will now be color-coded according to the selected metric
and configuration, providing an at-a-glance overview of performance changes.

### Evaluation using the CLI

Genkit CLI provides a rich API for performing evaluation. This is especially
useful in environments where the Dev UI is not available (e.g. in a CI/CD
workflow).

Genkit CLI provides 3 main evaluation commands: `eval:flow`, `eval:extractData`,
and `eval:run`.

#### `eval:flow` command

The `eval:flow` command runs inference-based evaluation on an input dataset.
This dataset may be provided either as a JSON file or by referencing an existing
dataset in your Genkit runtime.

```bash
# Referencing an existing dataset
genkit eval:flow qaFlow --input myFactsQaDataset

# or, using a dataset from a file
genkit eval:flow qaFlow --input testInputs.json
```

Note: Make sure that you start your genkit app before running these CLI
commands.

```bash
genkit start -- <command to start your app>
```

Here, `testInputs.json` should be an array of objects containing an `input`
field and an optional `reference` field, like below:

```json
[
	{
		"input": { "query": "What is the French word for Cheese?" }
	},
	{
		"input": { "query": "What green vegetable looks like cauliflower?" },
		"reference": "Broccoli"
	}
]
```

If your flow requires auth, you may specify it using the `--context` argument:

```bash
genkit eval:flow qaFlow --input testInputs.json --context '{"auth": {"email_verified": true}}'
```

By default, the `eval:flow` and `eval:run` commands use all available metrics
for evaluation. To run on a subset of the configured evaluators, use the
`--evaluators` flag and provide a comma-separated list of evaluators by name:

```bash
genkit eval:flow qaFlow --input testInputs.json --evaluators=genkitEval/maliciousness,genkitEval/answer_relevancy
```

You can view the results of your evaluation run in the Dev UI at
`localhost:4000/evaluate`.

#### `eval:extractData` and `eval:run` commands

To support _raw evaluation_, Genkit provides tools to extract data from traces
and run evaluation metrics on extracted data. This is useful, for example, if
you are using a different framework for evaluation or if you are collecting
inferences from a different environment to test locally for output quality.

You can batch run your Genkit flow and add a unique label to the run which then
can be used to extract an _evaluation dataset_. A raw evaluation dataset is a
collection of inputs for evaluation metrics, _without_ running any prior
inference.

Run your flow over your test inputs:

```bash
genkit flow:batchRun qaFlow testInputs.json --label firstRunSimple
```

Extract the evaluation data:

```bash
genkit eval:extractData qaFlow --label firstRunSimple --output factsEvalDataset.json
```

The exported data has a format different from the dataset format presented
earlier. This is because this data is intended to be used with evaluation
metrics directly, without any inference step. Here is the syntax of the
extracted data.

```json
Array<{
  "testCaseId": string,
  "input": any,
  "output": any,
  "context": any[],
  "traceIds": string[],
}>;
```

The data extractor automatically locates retrievers and adds the produced docs
to the context array. You can run evaluation metrics on this extracted dataset
using the `eval:run` command.

```bash
genkit eval:run factsEvalDataset.json
```

By default, `eval:run` runs against all configured evaluators, and as with
`eval:flow`, results for `eval:run` appear in the evaluation page of Developer
UI, located at `localhost:4000/evaluate`.

### Batching evaluations

:::note
This feature is only available in the Node.js SDK.
:::

You can speed up evaluations by processing the inputs in batches using the CLI and Dev UI. When batching is enabled, the input data is grouped into batches of size `batchSize`. The data points in a batch are all run in parallel to provide significant performance improvements, especially when dealing with large datasets and/or complex evaluators. By default (when the flag is omitted), batching is disabled.

The `batchSize` option has been integrated into the `eval:flow` and `eval:run` CLI commands. When a `batchSize` greater than 1 is provided, the evaluator will process the dataset in chunks of the specified size. This feature only affects the evaluator logic and not inference (when using `eval:flow`). Here are some examples of enabling batching with the CLI:

```bash
genkit eval:flow myFlow --input yourDataset.json --evaluators=custom/myEval --batchSize 10
```

Or, with `eval:run`

```bash
genkit eval:run yourDataset.json --evaluators=custom/myEval --batchSize 10
```

Batching is also available in the Dev UI for Genkit (JS) applications. You can set batch size when running a new evaluation, to enable parallelization.

### Custom extractors

Genkit provides reasonable default logic for extracting the necessary fields
(`input`, `output` and `context`) while doing an evaluation. However, you may
find that you need more control over the extraction logic for these fields.
Genkit supports customs extractors to achieve this. You can provide custom
extractors to be used in `eval:extractData` and `eval:flow` commands.

First, as a preparatory step, introduce an auxilary step in our `qaFlow`
example:

```js
export const qaFlow = ai.defineFlow(
	{
		name: 'qaFlow',
		inputSchema: z.object({ query: z.string() }),
		outputSchema: z.object({ answer: z.string() }),
	},
	async ({ query }) => {
		const factDocs = await ai.retrieve({
			retriever: dummyRetriever,
			query,
		});
		const factDocsModified = await ai.run('factModified', async () => {
			// Let us use only facts that are considered silly. This is a
			// hypothetical step for demo purposes, you may perform any
			// arbitrary task inside a step and reference it in custom
			// extractors.
			//
			// Assume you have a method that checks if a fact is silly
			return factDocs.filter((d) => isSillyFact(d.text));
		});

		const { text } = await ai.generate({
			model: googleAI.model('gemini-2.5-flash'),
			prompt: `Answer this question with the given context ${query}`,
			docs: factDocsModified,
		});
		return { answer: text };
	},
);
```

Next, configure a custom extractor to use the output of the `factModified` step
when evaluating this flow.

If you don't have one a tools-config file to configure custom extractors, add
one named `genkit-tools.conf.js` to your project root.

```bash
cd /path/to/your/genkit/app

touch genkit-tools.conf.js
```

In the tools config file, add the following code:

```js
module.exports = {
	evaluators: [
		{
			actionRef: '/flow/qaFlow',
			extractors: {
				context: { outputOf: 'factModified' },
			},
		},
	],
};
```

This config overrides the default extractors of Genkit's tooling, specifically
changing what is considered as `context` when evaluating this flow.

Running evaluation again reveals that context is now populated as the output of
the step `factModified`.

```bash
genkit eval:flow qaFlow --input testInputs.json
```

Evaluation extractors are specified as follows:

- `evaluators` field accepts an array of EvaluatorConfig objects, which are
  scoped by `flowName`
- `extractors` is an object that specifies the extractor overrides. The
  current supported keys in `extractors` are `[input, output, context]`. The
  acceptable value types are:
    - `string` - this should be a step name, specified as a string. The output
      of this step is extracted for this key.
    - `{ inputOf: string }` or `{ outputOf: string }` - These objects
      represent specific channels (input or output) of a step. For example, `{
inputOf: 'foo-step' }` would extract the input of step `foo-step` for
      this key.
    - `(trace) => string;` - For further flexibility, you can provide a
      function that accepts a Genkit trace and returns an `any`-type value,
      and specify the extraction logic inside this function. Refer to
      `genkit/genkit-tools/common/src/types/trace.ts` for the exact TraceData
      schema.

**Note:** The extracted data for all these extractors is the type corresponding
to the extractor. For example, if you use context: `{ outputOf: 'foo-step' }`,
and `foo-step` returns an array of objects, the extracted context is also an
array of objects.

### Synthesizing test data using an LLM

Here is an example flow that uses a PDF file to generate potential user
questions.

```ts
import { genkit, z } from 'genkit';
import { googleAI } from '@genkit-ai/google-genai';
import { chunk } from 'llm-chunk'; // npm install llm-chunk
import path from 'path';
import { readFile } from 'fs/promises';
import pdf from 'pdf-parse'; // npm install pdf-parse

const ai = genkit({ plugins: [googleAI()] });

const chunkingConfig = {
	minLength: 1000, // number of minimum characters into chunk
	maxLength: 2000, // number of maximum characters into chunk
	splitter: 'sentence', // paragraph | sentence
	overlap: 100, // number of overlap chracters
	delimiters: '', // regex for base split method
} as any;

async function extractText(filePath: string) {
	const pdfFile = path.resolve(filePath);
	const dataBuffer = await readFile(pdfFile);
	const data = await pdf(dataBuffer);
	return data.text;
}

export const synthesizeQuestions = ai.defineFlow(
	{
		name: 'synthesizeQuestions',
		inputSchema: z.object({ filePath: z.string().describe('PDF file path') }),
		outputSchema: z.object({
			questions: z.array(
				z.object({
					query: z.string(),
				}),
			),
		}),
	},
	async ({ filePath }) => {
		filePath = path.resolve(filePath);
		// `extractText` loads the PDF and extracts its contents as text.
		const pdfTxt = await ai.run('extract-text', () => extractText(filePath));

		const chunks = await ai.run('chunk-it', async () => chunk(pdfTxt, chunkingConfig));

		const questions = [];
		for (var i = 0; i < chunks.length; i++) {
			const { text } = await ai.generate({
				model: googleAI.model('gemini-2.5-flash'),
				prompt: {
					text: `Generate one question about the following text: ${chunks[i]}`,
				},
			});
			questions.push({ query: text });
		}
		return { questions };
	},
);
```

You can then use this command to export the data into a file and use for
evaluation.

```bash
genkit flow:run synthesizeQuestions '{"filePath": "my_input.pdf"}' --output synthesizedQuestions.json
```

## Next steps

- Learn about [creating flows](/docs/flows) to build AI workflows that can be evaluated
- Explore [retrieval-augmented generation (RAG)](/docs/rag) for building knowledge-based systems that benefit from evaluation
- See [tool calling](/docs/tool-calling) for creating AI agents that can be tested with evaluation metrics
- Check out the [developer tools documentation](/docs/devtools) for more information about the Genkit Developer UI

## Learn More

- [Flows](/docs/flows)
- [Retrieval-Augmented Generation (RAG)](/docs/rag)
- [Tool Calling](/docs/tool-calling)
- [Developer Tools](/docs/devtools)
- [Models](/docs/models)

</LanguageContent>

<LanguageContent lang="go">

Evaluation is a form of testing that helps you validate your LLM's responses and
ensure they meet your quality bar.

Genkit supports third-party evaluation tools through plugins, paired
with powerful observability features that provide insight into the runtime state
of your LLM-powered applications. Genkit tooling helps you automatically extract
data including inputs, outputs, and information from intermediate steps to
evaluate the end-to-end quality of LLM responses as well as understand the
performance of your system's building blocks.

### Types of evaluation

Genkit supports two types of evaluation:

- **Inference-based evaluation**: This type of evaluation runs against a
  collection of pre-determined inputs, assessing the corresponding outputs for
  quality.

    This is the most common evaluation type, suitable for most use cases. This
    approach tests a system's actual output for each evaluation run.

    You can perform the quality assessment manually, by visually inspecting the
    results. Alternatively, you can automate the assessment by using an
    evaluation metric.

- **Raw evaluation**: This type of evaluation directly assesses the quality of
  inputs without any inference. This approach typically is used with automated
  evaluation using metrics. All required fields for evaluation (e.g., `input`,
  `context`, `output` and `reference`) must be present in the input dataset. This
  is useful when you have data coming from an external source (e.g., collected
  from your production traces) and you want to have an objective measurement of
  the quality of the collected data.

    For more information, see the [Advanced use](#advanced-use) section of this
    page.

This section explains how to perform inference-based evaluation using Genkit.

## Quick start

Perform these steps to get started quickly with Genkit.

### Setup

1.  Use an existing Genkit app or create a new one by following our
    [Get started](/docs/get-started) guide.

2.  Add the following code to define a simple RAG application to evaluate. For
    this guide, we use a dummy retriever that always returns the same documents.

    ```go
    package main

    import (
        "context"
        "fmt"
        "log"

        "github.com/firebase/genkit/go/ai"
        "github.com/firebase/genkit/go/genkit"
        "github.com/firebase/genkit/go/plugins/googlegenai"
    )

    func main() {
        ctx := context.Background()

        // Initialize Genkit
        g := genkit.Init(ctx,
            genkit.WithPlugins(&googlegenai.GoogleAI{}),
            genkit.WithDefaultModel("googleai/gemini-2.5-flash"),
        )

        // Dummy retriever that always returns the same facts
        dummyRetrieverFunc := func(ctx context.Context, req *ai.RetrieverRequest) (*ai.RetrieverResponse, error) {
            facts := []string{
                "Dog is man's best friend",
                "Dogs have evolved and were domesticated from wolves",
            }
            // Just return facts as documents.
            var docs []*ai.Document
            for _, fact := range facts {
                docs = append(docs, ai.DocumentFromText(fact, nil))
            }
            return &ai.RetrieverResponse{Documents: docs}, nil
        }
        factsRetriever := genkit.DefineRetriever(g, "local", "dogFacts", dummyRetrieverFunc)

        m := googlegenai.GoogleAIModel(g, "gemini-2.5-flash")
        if m == nil {
            log.Fatal("failed to find model")
        }

        // A simple question-answering flow
        genkit.DefineFlow(g, "qaFlow", func(ctx context.Context, query string) (string, error) {
            factDocs, err := ai.Retrieve(ctx, factsRetriever, ai.WithTextDocs(query))
            if err != nil {
                return "", fmt.Errorf("retrieval failed: %w", err)
            }
            llmResponse, err := genkit.Generate(ctx, g,
                ai.WithModelName("googleai/gemini-2.5-flash"),
                ai.WithPrompt("Answer this question with the given context: %s", query),
                ai.WithDocs(factDocs.Documents...)
            )
            if err != nil {
                return "", fmt.Errorf("generation failed: %w", err)
            }
            return llmResponse.Text(), nil
        })
    }
    ```

3.  You can optionally add evaluation metrics to your application to use while
    evaluating. This guide uses the `EvaluatorRegex` metric from the
    `evaluators` package.

    ```go
    import (
        "github.com/firebase/genkit/go/plugins/evaluators"
    )

    func main() {
        // ...

        metrics := []evaluators.MetricConfig{
            {
                MetricType: evaluators.EvaluatorRegex,
            },
        }

        // Initialize Genkit
        g := genkit.Init(ctx,
            genkit.WithPlugins(
                &googlegenai.GoogleAI{},
                &evaluators.GenkitEval{Metrics: metrics}, // Add this plugin
            ),
            genkit.WithDefaultModel("googleai/gemini-2.5-flash"),
        )
    }
    ```

    **Note:** Ensure that the `evaluators` package is
    installed in your go project:

    ```bash
    go get github.com/firebase/genkit/go/plugins/evaluators
    ```

4.  Start your Genkit application.

    ```bash
    genkit start -- go run main.go
    ```

### Create a dataset

Create a dataset to define the examples we want to use for evaluating our flow.

1.  Go to the Dev UI at `http://localhost:4000` and click the **Datasets**
    button to open the Datasets page.

2.  Click the **Create Dataset** button to open the create dataset dialog.

    a. Provide a `datasetId` for your new dataset. This guide uses
    `myFactsQaDataset`.

    b. Select `Flow` dataset type.

    c. Leave the validation target field empty and click **Save**

3.  Your new dataset page appears, showing an empty dataset. Add examples to it
    by following these steps:

    a. Click the **Add example** button to open the example editor panel.

    b. Only the `Input` field is required. Enter `"Who is man's best friend?"`
    in the `Input` field, and click **Save** to add the example has to your
    dataset.

    If you have configured the `EvaluatorRegex` metric and would like
    to try it out, you need to specify a Reference string that contains the
    pattern to match the output against. For the preceding input, set the
    `Reference output` text to `"(?i)dog"`, which is a case-insensitive regular-
    expression pattern to match the word "dog" in the flow output.

    c. Repeat steps (a) and (b) a couple of more times to add more examples.
    This guide adds the following example inputs to the dataset:

    ```text
    "Can I give milk to my cats?"
    "From which animals did dogs evolve?"
    ```

    If you are using the regular-expression evaluator, use the corresponding
    reference strings:

    ```text
    "(?i)don't know"
    "(?i)wolf|wolves"
    ```

    Note that this is a contrived example and the regular-expression
    evaluator may not be the right choice to evaluate the responses
    from `qaFlow`. However, this guide can be applied to any
    Genkit Go evaluator of your choice.

    By the end of this step, your dataset should have 3 examples in it, with the
    values mentioned above.

### Run evaluation and view results

To start evaluating the flow, click the **Run new evaluation** button on your
dataset page. You can also start a new evaluation from the _Evaluations_ tab.

1.  Select the `Flow` radio button to evaluate a flow.

2.  Select `qaFlow` as the target flow to evaluate.

3.  Select `myFactsQaDataset` as the target dataset to use for evaluation.

4.  If you have installed an evaluator metric using Genkit plugins,
    you can see these metrics in this page. Select the metrics that you want to
    use with this evaluation run. This is entirely optional: Omitting this step
    will still return the results in the evaluation run, but without any
    associated metrics.

    If you have not provided any reference values and are using the
    `EvaluatorRegex` metric, your evaluation will fail since this metric needs
    reference to be set.

5.  Click **Run evaluation** to start evaluation. Depending on the flow
    you're testing, this may take a while. Once the evaluation is complete, a
    success message appears with a link to view the results. Click the link
    to go to the _Evaluation details_ page.

You can see the details of your evaluation on this page, including original
input, extracted context and metrics (if any).

## Core concepts

### Terminology

Knowing the following terms can help ensure that you correctly understand
the information provided on this page:

- **Evaluation**: An evaluation is a process that assesses system performance.
  In Genkit, such a system is usually a Genkit primitive, such as a flow, a
  prompt, or a model. An evaluation can be automated or manual (human
  evaluation).

- **Bulk inference** Inference is the act of running an input on a flow or
  model to get the corresponding output. Bulk inference involves performing
  inference on multiple inputs simultaneously.

- **Metric** An evaluation metric is a criterion on which an inference is
  scored. Examples include accuracy, faithfulness, maliciousness, whether the
  output is in English, etc.

- **Dataset** A dataset is a collection of examples to use for inference-based
  evaluation. A dataset typically consists of `Input` and optional `Reference`
  fields. The `Reference` field does not affect the inference step of evaluation
  but it is passed verbatim to any evaluation metrics. In Genkit, you can create
  a dataset through the Dev UI. There are three types of datasets in Genkit:
  _Flow_ datasets, _Model_ datasets, and _Prompt_ datasets.

## Supported evaluators

Genkit supports several evaluators, some built-in, and others
provided externally.

### Genkit evaluators

Genkit includes a small number of built-in evaluators, ported from
the [JS evaluators plugin](https://js.api.genkit.dev/enums/_genkit-ai_evaluator.GenkitMetric.html),
to help you get started:

- EvaluatorDeepEqual -- Checks if the generated output is deep-equal to the
  reference output provided.
- EvaluatorRegex -- Checks if the generated output matches the regular
  expression provided in the reference field.
- EvaluatorJsonata -- Checks if the generated output matches the
  [JSONATA](https://jsonata.org/) expression provided in the
  reference field.

### Custom Evaluators

You can extend Genkit to support custom evaluation by defining your own evaluator functions. An evaluator can use an LLM as a judge, perform programmatic (heuristic) checks, or call external APIs to assess the quality of a response.

You define a custom evaluator using the `ai.DefineEvaluator` function. The callback function for the evaluator can contain any logic you need.

Here's an example of a custom evaluator that uses an LLM to check for "deliciousness":

```go
import (
	"context"
	"errors"
	"fmt"
	"strings"

	"github.com/firebase/genkit/go/ai"
	"github.com/firebase/genkit/go/genkit"
	"github.com/firebase/genkit/go/core/api"
)

// NewFoodEvaluator creates a custom evaluator for food.
func NewFoodEvaluator(g *genkit.Genkit) ai.Evaluator {
	return genkit.DefineEvaluator(g, api.NewName("custom", "foodEvaluator"),
		&ai.EvaluatorOptions{
			DisplayName: "Food Evaluator",
			Definition:  "Determines if an output is a delicious food item.",
		},
		func(ctx context.Context, req *ai.EvaluatorCallbackRequest) (*ai.EvaluatorCallbackResponse, error) {
			if req.Input.Output == nil {
				return nil, errors.New("output is required for food evaluation")
			}
			outputStr, ok := req.Input.Output.(string)
			if !ok {
				return nil, errors.New("output must be a string")
			}

			// You can use an LLM as a judge for more complex evaluations.
			resp, err := genkit.Generate(ctx, g,
				ai.WithModelName("googleai/gemini-2.5-flash"),
				ai.WithPrompt(fmt.Sprintf(`Is the following food delicious? Respond with "yes", "no", or "maybe". Food: %s`, outputStr)),
			)
			if err != nil {
				return nil, fmt.Errorf("failed to generate evaluation: %w", err)
			}

			// You can also perform any custom logic in the evaluator.
			// if strings.Contains(outputStr, "marmite") {
			//     handleMarmite()
			// }
			// or...
			// score, err := myApi.Evaluate(ctx, &myApi.Request{
			//      Type: "deliciousness",
			//      Value: outputStr,
			// })

			return &ai.EvaluatorCallbackResponse{
				TestCaseId: req.Input.TestCaseId,
				Evaluation: []ai.Score{{Score: resp.Text()}},
			}, nil
		},
	)
}
```

You can then use this custom evaluator just like any other Genkit evaluator. You
can use them with your datasets in the Dev UI or with the CLI in the `eval:run`
or `eval:flow` commands:

```bash
genkit eval:flow myFlow --input myDataset.json --evaluators=custom/foodEvaluator
```

## Advanced use

Along with its basic functionality, Genkit also provides advanced support for
certain evaluation use cases.

### Evaluation using the CLI

Genkit CLI provides a rich API for performing evaluation. This is especially
useful in environments where the Dev UI is not available (e.g. in a CI/CD
workflow).

Genkit CLI provides 3 main evaluation commands: `eval:flow`, `eval:extractData`,
and `eval:run`.

#### Evaluation `eval:flow` command

The `eval:flow` command runs inference-based evaluation on an input dataset.
This dataset may be provided either as a JSON file or by referencing an existing
dataset in your Genkit runtime.

```bash
# Referencing an existing dataset
genkit eval:flow qaFlow --input myFactsQaDataset

# or, using a dataset from a file
genkit eval:flow qaFlow --input testInputs.json
```

**Note:** Make sure that you start your genkit app before running these CLI
commands.

```bash
genkit start -- go run main.go
```

Here, `testInputs.json` should be an array of objects containing an `input`
field and an optional `reference` field, like below:

```json
[
	{
		"input": "What is the French word for Cheese?"
	},
	{
		"input": "What green vegetable looks like cauliflower?",
		"reference": "Broccoli"
	}
]
```

If your flow requires auth, you may specify it using the `--context` argument:

```bash
genkit eval:flow qaFlow --input testInputs.json --context '{"auth": {"email_verified": true}}'
```

By default, the `eval:flow` and `eval:run` commands use all available metrics
for evaluation. To run on a subset of the configured evaluators, use the
`--evaluators` flag and provide a comma-separated list of evaluators by name:

```bash
genkit eval:flow qaFlow --input testInputs.json --evaluators=genkitEval/regex,genkitEval/jsonata
```

You can view the results of your evaluation run in the Dev UI at
`localhost:4000/evaluate`.

#### `eval:extractData` and `eval:run` commands

To support _raw evaluation_, Genkit provides tools to extract data from traces
and run evaluation metrics on extracted data. This is useful, for example, if
you are using a different framework for evaluation or if you are collecting
inferences from a different environment to test locally for output quality.

You can batch run your Genkit flow and extract an _evaluation dataset_ from the
resultant traces. A raw evaluation dataset is a collection of inputs for
evaluation metrics, _without_ running any prior inference.

Run your flow over your test inputs:

```bash
genkit flow:batchRun qaFlow testInputs.json
```

Extract the evaluation data:

```bash
genkit eval:extractData qaFlow --maxRows 2 --output factsEvalDataset.json
```

The exported data has a format different from the dataset format presented
earlier. This is because this data is intended to be used with evaluation
metrics directly, without any inference step. Here is the syntax of the
extracted data.

```json
Array<{
  "testCaseId": string,
  "input": any,
  "output": any,
  "context": any[],
  "traceIds": string[],
}>;
```

The data extractor automatically locates retrievers and adds the produced docs
to the context array. You can run evaluation metrics on this extracted dataset
using the `eval:run` command.

```bash
genkit eval:run factsEvalDataset.json
```

By default, `eval:run` runs against all configured evaluators, and as with
`eval:flow`, results for `eval:run` appear in the evaluation page of Developer
UI, located at `localhost:4000/evaluate`.

## Next steps

- Learn about [creating flows](/docs/flows) to build AI workflows that can be evaluated
- Explore [retrieval-augmented generation (RAG)](/docs/rag) for building knowledge-based systems that benefit from evaluation
- See [tool calling](/docs/tool-calling) for creating AI agents that can be tested with evaluation metrics
- Check out the [developer tools documentation](/docs/devtools) for more information about the Genkit Developer UI

## Learn More

- [Flows](/docs/flows)
- [Retrieval-Augmented Generation (RAG)](/docs/rag)
- [Tool Calling](/docs/tool-calling)
- [Developer Tools](/docs/devtools)
- [Models](/docs/models)

</LanguageContent>

<LanguageContent lang="python">

Evaluation is a form of testing that helps you validate your LLM's responses and
ensure they meet your quality bar.

Genkit supports third-party evaluation tools through plugins, paired
with powerful observability features that provide insight into the runtime state
of your LLM-powered applications. Genkit tooling helps you automatically extract
data including inputs, outputs, and information from intermediate steps to
evaluate the end-to-end quality of LLM responses as well as understand the
performance of your system's building blocks.

### Types of evaluation

Genkit supports two types of evaluation:

- **Inference-based evaluation**: This type of evaluation runs against a
  collection of pre-determined inputs, assessing the corresponding outputs for
  quality.

    This is the most common evaluation type, suitable for most use cases. This approach tests a system's actual output for each evaluation run.

    You can perform the quality assessment manually, by visually inspecting the results. Alternatively, you can automate the assessment by using an evaluation metric.

- **Raw evaluation**: This type of evaluation directly assesses the quality of
  inputs without any inference. This approach typically is used with automated
  evaluation using metrics. All required fields for evaluation (e.g., `input`,
  `context`, `output` and `reference`) must be present in the input dataset. This
  is useful when you have data coming from an external source (e.g., collected
  from your production traces) and you want to have an objective measurement of
  the quality of the collected data.

    For more information, see the [Advanced use](#advanced-use) section of this page.

This section explains how to perform inference-based evaluation using Genkit.

## Quick start

### Setup

1.  Use an existing Genkit app or create a new one by following our [Get started](/docs/get-started) guide.
2.  Add the following code to define a simple RAG application to evaluate. For this guide, we use a dummy retriever that always returns the same documents.

    ```python
    from genkit import Genkit
    from genkit.plugins.google_genai import GoogleAI
    from pydantic import BaseModel

    # Initialize Genkit
    ai = Genkit(plugins=[GoogleAI()])

    # Dummy retriever that always returns the same docs
    async def dummy_retriever_fn(query, options):
        facts = [
            "Dog is man's best friend",
            "Dogs have evolved and were domesticated from wolves",
        ]
        return facts

    ai.define_simple_retriever(
        'dummy_retriever',
        dummy_retriever_fn,
    )

    # Define input/output schemas
    class QAInput(BaseModel):
        query: str

    class QAOutput(BaseModel):
        answer: str

    # A simple question-answering flow
    @ai.flow()
    async def qa_flow(input: QAInput) -> QAOutput:
        fact_docs = await ai.retrieve(
            retriever='dummy_retriever',
            query=input.query,
        )

        response = await ai.generate(
            model='googleai/gemini-2.5-flash',
            prompt=f'Answer this question with the given context: {input.query}',
            docs=fact_docs.documents,
        )
        return QAOutput(answer=response.text)
    ```

3.  (Optional) Add evaluation metrics to your application to use while evaluating. This guide uses the `MALICIOUSNESS` metric from the `genkit-plugin-evaluators` package.

    ```python
    from genkit import Genkit
    from genkit.blocks.model import ModelReference
    from genkit.plugins.google_genai import GoogleAI
    from genkit.plugins.evaluators import (
        define_genkit_evaluators,
        GenkitMetricType,
        MetricConfig,
    )

    ai = Genkit(plugins=[GoogleAI()])

    # Add evaluators to your Genkit instance
    define_genkit_evaluators(
        ai,
        [
            MetricConfig(
                metric_type=GenkitMetricType.MALICIOUSNESS,
                judge=ModelReference(name='googleai/gemini-2.5-flash'),
            ),
        ],
    )
    ```

    **Note:** The configuration above requires installation of the `genkit-plugin-evaluators` package.

    ```bash
    uv add genkit-plugin-evaluators
    ```

4.  Start your Genkit application.

    ```bash
    genkit start -- uv run main.py
    ```

### Create a dataset

Create a dataset to define the examples we want to use for evaluating our flow.

1. Go to the Dev UI at `http://localhost:4000` and click the **Datasets** button
   to open the Datasets page.

2. Click on the **Create Dataset** button to open the create dataset dialog.

    a. Provide a `datasetId` for your new dataset. This guide uses
    `myFactsQaDataset`.

    b. Select `Flow` dataset type.

    c. Leave the validation target field empty and click **Save**

3. Your new dataset page appears, showing an empty dataset. Add examples to it by following these steps:

    a. Click the **Add example** button to open the example editor panel.

    b. Only the `input` field is required. Enter `{"query": "Who is man's best friend?"}` in the `input` field, and click **Save** to add the example has to your dataset.

    c. Repeat steps (a) and (b) a couple more times to add more examples. This guide adds the following example inputs to the dataset:

    ```
    {"query": "Can I give milk to my cats?"}
    {"query": "From which animals did dogs evolve?"}
    ```

By the end of this step, your dataset should have 3 examples in it, with the
values mentioned above.

### Run evaluation and view results

To start evaluating the flow, click the **Run new evaluation** button on your
dataset page. You can also start a new evaluation from the _Evaluations_ tab.

1. Select the `Flow` radio button to evaluate a flow.

2. Select `qa_flow` as the target flow to evaluate.

3. Select `myFactsQaDataset` as the target dataset to use for evaluation.

4. (Optional) If you have installed an evaluator metric using Genkit plugins,
   you can see these metrics in this page. Select the metrics that you want to use
   with this evaluation run. This is entirely optional: Omitting this step will
   still return the results in the evaluation run, but without any associated
   metrics.

5. Finally, click **Run evaluation** to start evaluation. Depending on the flow
   you're testing, this may take a while. Once the evaluation is complete, a
   success message appears with a link to view the results. Click on the link to go
   to the _Evaluation details_ page.

You can see the details of your evaluation on this page, including original
input, extracted context and metrics (if any).

## Core concepts

### Terminology

- **Evaluation**: An evaluation is a process that assesses system performance.
  In Genkit, such a system is usually a Genkit primitive, such as a flow, a
  prompt, or a model. An evaluation can be automated or manual (human
  evaluation).

- **Bulk inference** Inference is the act of running an input on a flow or model to get the corresponding output. Bulk inference involves performing inference on multiple inputs simultaneously.

- **Metric** An evaluation metric is a criterion on which an inference is scored. Examples include accuracy, faithfulness, maliciousness, whether the output is in English, etc.

- **Dataset** A dataset is a collection of examples to use for inference-based  
  evaluation. A dataset typically consists of `input` and optional `reference`
  fields. The `reference` field does not affect the inference step of evaluation
  but it is passed verbatim to any evaluation metrics. In Genkit, you can create
  a dataset through the Dev UI. There are three types of datasets in Genkit:
  _Flow_ datasets, _Model_ datasets, and _Prompt_ datasets.

### Schema validation

Depending on the type, datasets have schema validation support in the Dev UI:

- Flow datasets support validation of the `input` and `reference` fields of the dataset against a flow in the Genkit application. Schema validation is optional and is only enforced if a schema is specified on the target flow.

- Prompt datasets support validation of the `input` field against the prompt's input schema.

- Model datasets have implicit schema, supporting both `string` and `GenerateRequest` input types. String validation provides a convenient way to evaluate simple text prompts, while `GenerateRequest` provides complete control for advanced use cases (e.g. providing model parameters, message history, tools, etc).

Note: Schema validation is a helper tool for editing examples, but it is
possible to save an example with invalid schema. These examples may fail when
the running an evaluation.

:::note[Evaluating Prompts]
When evaluating a prompt, Genkit executes the prompt against the inputs in your dataset. If your prompt definition includes multiple variants (e.g., different model configurations or instructions), the Developer UI allows you to select the specific variant you want to evaluate. This enables A/B testing of different prompt strategies.

If variants have different input schemas, schema validation will be performed against the schema of the currently selected variant.
:::

## Supported evaluators

### Genkit evaluators

Genkit includes a number of built-in evaluators, inspired by [RAGAS](https://docs.ragas.io/en/stable/), to help you get started:

- **Faithfulness** -- Measures the factual consistency of the generated answer against the given context
- **Answer Relevancy** -- Assesses how pertinent the generated answer is to the given prompt
- **Maliciousness** -- Measures whether the generated output intends to deceive, harm, or exploit
- **Regex** -- Checks if the generated output matches a regular expression pattern provided in the reference field
- **Deep Equal** -- Checks if the generated output is deep-equal to the reference output
- **JSONata** -- Checks if the generated output matches a JSONata expression provided in the reference field

### Evaluator plugins

Genkit supports additional evaluators through plugins, like the Vertex Rapid Evaluators, which you can access via the [VertexAI Plugin](/docs/integrations/vertex-ai#evaluation-metrics).

### Custom Evaluators

You can extend Genkit to support custom evaluation by defining your own evaluator functions. An evaluator can use an LLM as a judge, perform programmatic (heuristic) checks, or call external APIs to assess the quality of a response.

You define a custom evaluator using the `ai.define_evaluator()` method. The callback function for the evaluator can contain any logic you need.

Here's an example of a custom evaluator that uses an LLM to check for "deliciousness":

```python
from genkit import Genkit
from genkit.plugins.google_genai import GoogleAI
from genkit.types import (
    BaseEvalDataPoint,
    EvalFnResponse,
    Score,
    EvalStatusEnum,
)

ai = Genkit(plugins=[GoogleAI()])

async def food_evaluator(
    datapoint: BaseEvalDataPoint,
    options: dict[str, object] | None = None,
) -> EvalFnResponse:
    """Determines if an output is a delicious food item."""
    if not datapoint.output or not isinstance(datapoint.output, str):
        raise ValueError("String output is required for food evaluation")

    # You can use an LLM as a judge for more complex evaluations.
    response = await ai.generate(
        model='googleai/gemini-2.5-flash',
        prompt=f'Is the following food delicious? Respond with "yes", "no", or "maybe". Food: {datapoint.output}',
    )

    # You can also perform any custom logic in the evaluator.
    # if "marmite" in datapoint.output:
    #     handle_marmite()
    # or...
    # score = await my_api.evaluate(
    #     type='deliciousness',
    #     value=datapoint.output
    # )

    return EvalFnResponse(
        test_case_id=datapoint.test_case_id or '',
        evaluation=Score(
            score=response.text,
            status=EvalStatusEnum.PASS_,
            details={'reasoning': f'LLM judged: {response.text}'},
        ),
    )

ai.define_evaluator(
    name='custom/foodEvaluator',
    display_name='Food Evaluator',
    definition='Determines if an output is a delicious food item.',
    fn=food_evaluator,
)
```

You can then use this custom evaluator just like any other Genkit evaluator. You
can use them with your datasets in the Dev UI or with the CLI in the `eval:run`
or `eval:flow` commands:

```bash
genkit eval:flow myFlow --input myDataset.json --evaluators=custom/foodEvaluator
```

### Running evaluations programmatically

You can also run evaluations directly in your code using `ai.evaluate()`:

```python
import asyncio
from genkit import Genkit
from genkit.plugins.google_genai import GoogleAI
from genkit.types import BaseEvalDataPoint, EvalResponse

ai = Genkit(plugins=[GoogleAI()])

# ... define your evaluator ...

async def run_evaluation() -> EvalResponse:
    dataset = [
        BaseEvalDataPoint(
            test_case_id='test-1',
            input='What is the capital of France?',
            output='The capital of France is Paris.',
            context=['France is a country in Europe. Paris is its capital.'],
        ),
        BaseEvalDataPoint(
            test_case_id='test-2',
            input='What color is the sky?',
            output='The sky is blue during the day.',
            context=['The sky appears blue due to light scattering.'],
        ),
    ]

    result = await ai.evaluate(
        evaluator='custom/foodEvaluator',
        dataset=dataset,
        eval_run_id='my-eval-run',
    )

    for item in result.root:
        print(f'Test case: {item.test_case_id}')
        print(f'Score: {item.evaluation}')

    return result

asyncio.run(run_evaluation())
```

## Advanced use

### Evaluation comparison

The Developer UI offers visual tools for side-by-side comparison of multiple
evaluation runs. This feature allows you to analyze variations across different
executions within a unified interface, making it easier to assess changes in
output quality. Additionally, you can highlight outputs based on the performance
of specific metrics, indicating improvements or regressions.

When comparing evaluations, one run is designated as the _Baseline_. All other
evaluations are compared against this baseline to determine whether their
performance has improved or regressed.

<ThemeImage
	lightSrc="/src/assets/evals_compare_light.png"
	darkSrc="/src/assets/evals_compare_dark.png"
	alt="Evaluation comparison with metric highlighting"
/>

#### Prerequisites

To use the evaluation comparison feature, the following conditions must be met:

- Evaluations must originate from a dataset source. Evaluations from file
  sources are not comparable.
- All evaluations being compared must be from the same dataset.
- For metric highlighting, all evaluations must use at least one common
  metric that produces a `number` or `boolean` score.

#### Comparing evaluations

1.  Ensure you have at least two evaluation runs performed on the same dataset.
    For instructions, refer to the
    [Run evaluation section](#run-evaluation-and-view-results).

2.  In the Developer UI, navigate to the **Datasets** page.

3.  Select the relevant dataset and open its **Evaluations** tab. You should see
    all evaluation runs associated with that dataset.

4.  Choose one evaluation to serve as the baseline for comparison.

5.  On the evaluation results page, click the **+ Comparison** button. If this
    button is disabled, it means no other comparable evaluations are available
    for this dataset.

6.  A new column will appear with a dropdown menu. Select another evaluation
    from this menu to load its results alongside the baseline.

You can now view the outputs side-by-side to visually inspect differences in
quality. This feature supports comparing up to three evaluations simultaneously.

##### Metric highlighting (Optional)

If your evaluations include metrics, you can enable metric highlighting to
color-code the results. This feature helps you quickly identify changes in
performance: improvements are colored green, while regressions are red.

Note that highlighting is only supported for numeric and boolean metrics, and
the selected metric must be present in all evaluations being compared.

To enable metric highlighting:

1.  After initiating a comparison, a **Choose a metric to compare** menu will
    become available.

2.  Select a metric from the dropdown. By default, lower scores (for numeric
    metrics) and `false` values (for boolean metrics) are considered
    improvements and highlighted in green. You can reverse this logic by
    ticking the checkbox in the menu.

The comparison columns will now be color-coded according to the selected metric
and configuration, providing an at-a-glance overview of performance changes.

### Evaluation using the CLI

Genkit CLI provides a rich API for performing evaluation. This is especially
useful in environments where the Dev UI is not available (e.g. in a CI/CD
workflow).

Genkit CLI provides 3 main evaluation commands: `eval:flow`, `eval:extractData`,
and `eval:run`.

#### `eval:flow` command

The `eval:flow` command runs inference-based evaluation on an input dataset.
This dataset may be provided either as a JSON file or by referencing an existing
dataset in your Genkit runtime.

```bash
# Referencing an existing dataset
genkit eval:flow qa_flow --input myFactsQaDataset

# or, using a dataset from a file
genkit eval:flow qa_flow --input testInputs.json
```

Note: Make sure that you start your genkit app before running these CLI
commands.

```bash
genkit start -- uv run main.py
```

Here, `testInputs.json` should be an array of objects containing an `input`
field and an optional `reference` field, like below:

```json
[
	{
		"input": { "query": "What is the French word for Cheese?" }
	},
	{
		"input": { "query": "What green vegetable looks like cauliflower?" },
		"reference": "Broccoli"
	}
]
```

If your flow requires auth, you may specify it using the `--context` argument:

```bash
genkit eval:flow qa_flow --input testInputs.json --context '{"auth": {"email_verified": true}}'
```

By default, the `eval:flow` and `eval:run` commands use all available metrics
for evaluation. To run on a subset of the configured evaluators, use the
`--evaluators` flag and provide a comma-separated list of evaluators by name:

```bash
genkit eval:flow qa_flow --input testInputs.json --evaluators=genkitEval/maliciousness,genkitEval/answer_relevancy
```

You can view the results of your evaluation run in the Dev UI at
`localhost:4000/evaluate`.

#### `eval:extractData` and `eval:run` commands

To support _raw evaluation_, Genkit provides tools to extract data from traces
and run evaluation metrics on extracted data. This is useful, for example, if
you are using a different framework for evaluation or if you are collecting
inferences from a different environment to test locally for output quality.

You can batch run your Genkit flow and add a unique label to the run which then
can be used to extract an _evaluation dataset_. A raw evaluation dataset is a
collection of inputs for evaluation metrics, _without_ running any prior
inference.

Run your flow over your test inputs:

```bash
genkit flow:batchRun qa_flow testInputs.json --label firstRunSimple
```

Extract the evaluation data:

```bash
genkit eval:extractData qa_flow --label firstRunSimple --output factsEvalDataset.json
```

The exported data has a format different from the dataset format presented
earlier. This is because this data is intended to be used with evaluation
metrics directly, without any inference step. Here is the syntax of the
extracted data.

```json
Array<{
  "testCaseId": string,
  "input": any,
  "output": any,
  "context": any[],
  "traceIds": string[],
}>;
```

The data extractor automatically locates retrievers and adds the produced docs
to the context array. You can run evaluation metrics on this extracted dataset
using the `eval:run` command.

```bash
genkit eval:run factsEvalDataset.json
```

By default, `eval:run` runs against all configured evaluators, and as with
`eval:flow`, results for `eval:run` appear in the evaluation page of Developer
UI, located at `localhost:4000/evaluate`.

### Custom extractors

Genkit provides reasonable default logic for extracting the necessary fields
(`input`, `output` and `context`) while doing an evaluation. However, you may
find that you need more control over the extraction logic for these fields.
Genkit supports customs extractors to achieve this. You can provide custom
extractors to be used in `eval:extractData` and `eval:flow` commands.

First, as a preparatory step, introduce an auxiliary step in our `qa_flow`
example:

```python
@ai.flow()
async def qa_flow(input: QAInput) -> QAOutput:
    fact_docs = await ai.retrieve(
        retriever='dummy_retriever',
        query=input.query,
    )
    
    # Add a custom step to process the retrieved facts
    fact_docs_modified = await ai.run('factModified', lambda: [
        # Let us use only facts that are considered silly. This is a
        # hypothetical step for demo purposes, you may perform any
        # arbitrary task inside a step and reference it in custom
        # extractors.
        #
        # Assume you have a method that checks if a fact is silly
        doc for doc in fact_docs.documents if is_silly_fact(doc)
    ])

    response = await ai.generate(
        model='googleai/gemini-2.5-flash',
        prompt=f'Answer this question with the given context: {input.query}',
        docs=fact_docs_modified,
    )
    return QAOutput(answer=response.text)
```




### Synthesizing test data using an LLM

Here is an example flow that uses a PDF file to generate potential user
questions.

```python
from pathlib import Path
from genkit import Genkit
from genkit.plugins.google_genai import GoogleAI
from pydantic import BaseModel, Field
import pypdf  # uv add pypdf

ai = Genkit(plugins=[GoogleAI()])

class SynthesizeInput(BaseModel):
    file_path: str = Field(description="PDF file path")

class Question(BaseModel):
    query: str

class SynthesizeOutput(BaseModel):
    questions: list[Question]

def extract_text(file_path: str) -> str:
    """Extract text from a PDF file."""
    pdf_path = Path(file_path).resolve()
    with open(pdf_path, 'rb') as file:
        reader = pypdf.PdfReader(file)
        text = ''
        for page in reader.pages:
            text += page.extract_text()
    return text

def chunk_text(text: str, chunk_size: int = 2000, overlap: int = 100) -> list[str]:
    """Split text into overlapping chunks."""
    chunks = []
    start = 0
    while start < len(text):
        end = start + chunk_size
        chunks.append(text[start:end])
        start = end - overlap
    return chunks

@ai.flow()
async def synthesize_questions(input: SynthesizeInput) -> SynthesizeOutput:
    file_path = str(Path(input.file_path).resolve())
    
    # Extract text from the PDF
    pdf_txt = await ai.run('extract-text', lambda: extract_text(file_path))
    
    # Chunk the text
    chunks = await ai.run('chunk-it', lambda: chunk_text(pdf_txt))
    
    questions = []
    for chunk in chunks:
        response = await ai.generate(
            model='googleai/gemini-2.5-flash',
            prompt=f'Generate one question about the following text: {chunk}',
        )
        questions.append(Question(query=response.text))
    
    return SynthesizeOutput(questions=questions)
```

You can then use this command to export the data into a file and use for
evaluation.

```bash
genkit flow:run synthesize_questions '{"file_path": "my_input.pdf"}' --output synthesizedQuestions.json
```

## Next steps

- Learn about [creating flows](/docs/flows) to build AI workflows that can be evaluated
- Explore [retrieval-augmented generation (RAG)](/docs/rag) for building knowledge-based systems that benefit from evaluation
- See [tool calling](/docs/tool-calling) for creating AI agents that can be tested with evaluation metrics
- Check out the [developer tools documentation](/docs/devtools) for more information about the Genkit Developer UI

## Learn More

- [Flows](/docs/flows)
- [Retrieval-Augmented Generation (RAG)](/docs/rag)
- [Tool Calling](/docs/tool-calling)
- [Developer Tools](/docs/devtools)
- [Models](/docs/models)

</LanguageContent>
