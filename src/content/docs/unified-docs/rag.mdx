---
title: Retrieval-augmented generation (RAG)
description: Learn how Genkit simplifies retrieval-augmented generation (RAG) across JavaScript, Go, and Python by providing abstractions and plugins for indexers, embedders, and retrievers to incorporate external data into LLM responses.
---

import LangTabs from '@/components/LangTabs.astro';
import LangTabItem from '@/components/LangTabItem.astro';

Genkit provides abstractions that help you build retrieval-augmented
generation (RAG) flows, as well as plugins that provide integrations with
related tools.

## What is RAG?

Retrieval-augmented generation is a technique used to incorporate external
sources of information into an LLM's responses. It's important to be able to do
so because, while LLMs are typically trained on a broad body of material,
practical use of LLMs often requires specific domain knowledge (for example, you
might want to use an LLM to answer customers' questions about your company's
products).

One solution is to fine-tune the model using more specific data. However, this
can be expensive both in terms of compute cost and in terms of the effort needed
to prepare adequate training data.

In contrast, RAG works by incorporating external data sources into a prompt at
the time it's passed to the model. For example, you could imagine the prompt,
"What is Bart's relationship to Lisa?" might be expanded ("augmented") by
prepending some relevant information, resulting in the prompt, "Homer and
Marge's children are named Bart, Lisa, and Maggie. What is Bart's relationship
to Lisa?"

This approach has several advantages:

- It can be more cost-effective because you don't have to retrain the model.
- You can continuously update your data source and the LLM can immediately
  make use of the updated information.
- You now have the potential to cite references in your LLM's responses.

On the other hand, using RAG naturally means longer prompts, and some LLM API
services charge for each input token you send. Ultimately, you must evaluate the
cost tradeoffs for your applications.

RAG is a very broad area and there are many different techniques used to achieve
the best quality RAG. The core Genkit framework offers main abstractions to
help you do RAG:

<LangTabs>
  <LangTabItem lang="js">
    - **Indexers**: add documents to an "index"
    - **Embedders**: transforms documents into a vector representation
    - **Retrievers**: retrieve documents from an "index", given a query
  </LangTabItem>
  <LangTabItem lang="go">
    - **Embedders**: transforms documents into a vector representation
    - **Retrievers**: retrieve documents from an "index", given a query
  </LangTabItem>
  <LangTabItem lang="python">
    - **Embedders**: transforms documents into a vector representation
    - **Retrievers**: retrieve documents from an "index", given a query
  </LangTabItem>
</LangTabs>

These definitions are broad on purpose because Genkit is un-opinionated about
what an "index" is or how exactly documents are retrieved from it. Genkit only
provides a `Document` format and everything else is defined by the retriever or
indexer implementation provider.

### Indexers

<LangTabs>
  <LangTabItem lang="js">
    The index is responsible for keeping track of your documents in such a way that
    you can quickly retrieve relevant documents given a specific query. This is most
    often accomplished using a vector database, which indexes your documents using
    multidimensional vectors called embeddings. A text embedding (opaquely)
    represents the concepts expressed by a passage of text; these are generated
    using special-purpose ML models. By indexing text using its embedding, a vector
    database is able to cluster conceptually related text and retrieve documents
    related to a novel string of text (the query).

    Before you can retrieve documents for the purpose of generation, you need to
    ingest them into your document index. A typical ingestion flow does the
    following:

    1. Split up large documents into smaller documents so that only relevant
       portions are used to augment your prompts â€“ "chunking". This is necessary
       because many LLMs have a limited context window, making it impractical to
       include entire documents with a prompt.

       Genkit doesn't provide built-in chunking libraries; however, there are open
       source libraries available that are compatible with Genkit.

    2. Generate embeddings for each chunk. Depending on the database you're using,
       you might explicitly do this with an embedding generation model, or you might
       use the embedding generator provided by the database.
    3. Add the text chunk and its index to the database.

    You might run your ingestion flow infrequently or only once if you are working
    with a stable source of data. On the other hand, if you are working with data
    that frequently changes, you might continuously run the ingestion flow (for
    example, in a Cloud Firestore trigger, whenever a document is updated).
  </LangTabItem>
  <LangTabItem lang="go">
    In Go, indexing is typically handled by your chosen vector database or storage solution. 
    Genkit provides the abstractions for working with indexed documents, but the actual 
    indexing process is implementation-specific to your storage backend.

    Users are expected to add their own functionality to index documents using their 
    preferred vector database or storage solution.
  </LangTabItem>
  <LangTabItem lang="python">
    In Python, indexing is outside the scope of Genkit and you should use the 
    SDKs/APIs provided by the vector store you are using. Genkit provides the 
    abstractions for working with indexed documents through retrievers.
  </LangTabItem>
</LangTabs>

### Embedders

An embedder is a function that takes content (text, images, audio, etc.) and
creates a numeric vector that encodes the semantic meaning of the original
content. As mentioned above, embedders are leveraged as part of the process of
indexing, however, they can also be used independently to create embeddings
without an index.

### Retrievers

A retriever is a concept that encapsulates logic related to any kind of document
retrieval. The most popular retrieval cases typically include retrieval from
vector stores, however, in Genkit a retriever can be any function that returns
data.

To create a retriever, you can use one of the provided implementations or create
your own.

## Supported indexers, retrievers, and embedders

Genkit provides indexer and retriever support through its plugin system. The
following plugins are officially supported:

<LangTabs>
  <LangTabItem lang="js">
    **Vector Databases:**
    - [Astra DB](/docs/plugins/astra-db) - DataStax Astra DB vector database
    - [Chroma DB](/docs/plugins/chroma) vector database
    - [Cloud Firestore vector store](/docs/plugins/firebase)
    - [Cloud SQL for PostgreSQL](/docs/plugins/cloud-sql-pg) with pgvector extension
    - [LanceDB](/docs/plugins/lancedb) open-source vector database
    - [Neo4j](/docs/plugins/neo4j) graph database with vector search
    - [Pinecone](/docs/plugins/pinecone) cloud vector database
    - [Vertex AI Vector Search](/docs/plugins/vertex-ai)

    **Templates:**
    - PostgreSQL with [`pgvector`](/docs/templates/pgvector)

    **Embedding Models:**
    - Google AI and Vertex AI plugins provide text embedding models
  </LangTabItem>
  <LangTabItem lang="go">
    **Vector Databases:**
    - [Pinecone](/go/docs/plugins/pinecone) cloud vector database
    - PostgreSQL with [`pgvector`](/go/docs/plugins/pgvector)

    **Embedding Models:**
    - [Google Generative AI](/go/docs/plugins/google-genai) - Text embedding models
  </LangTabItem>
  <LangTabItem lang="python">
    **Vector Databases:**
    - Firestore Vector Store (via Firebase plugin)
    - Dev Local Vector Store (for development/testing)

    **Embedding Models:**
    - Google GenAI plugin provides text embedding models
  </LangTabItem>
</LangTabs>

## Defining a RAG Flow

The following examples show how you could ingest a collection of restaurant menu
PDF documents into a vector database and retrieve them for use in a flow that
determines what food items are available.

### Install dependencies

<LangTabs>
  <LangTabItem lang="js">
    Install dependencies for processing PDFs:

    ```bash
    npm install llm-chunk pdf-parse @genkit-ai/dev-local-vectorstore

    npm install --save-dev @types/pdf-parse
    ```
  </LangTabItem>
  <LangTabItem lang="go">
    Install dependencies for text processing and PDF parsing:

    ```bash
    go get github.com/tmc/langchaingo/textsplitter
    go get github.com/ledongthuc/pdf
    ```
  </LangTabItem>
  <LangTabItem lang="python">
    Install dependencies for your chosen vector store and PDF processing:

    ```bash
    pip install genkit[google-genai,firebase]
    # Add other dependencies as needed for PDF processing
    ```
  </LangTabItem>
</LangTabs>

### Configure vector store

<LangTabs>
  <LangTabItem lang="js">
    Add a local vector store to your configuration:

    ```ts
    import { devLocalIndexerRef, devLocalVectorstore } from '@genkit-ai/dev-local-vectorstore';
    import { googleAI } from '@genkit-ai/googleai';
    import { z, genkit } from 'genkit';

    const ai = genkit({
      plugins: [
        // googleAI provides the gemini-embedding-001 embedder
        googleAI(),

        // the local vector store requires an embedder to translate from text to vector
        devLocalVectorstore([
          {
            indexName: 'menuQA',
            embedder: googleAI.embedder('gemini-embedding-001'),
          },
        ]),
      ],
    });
    ```
  </LangTabItem>
  <LangTabItem lang="go">
    Configure your Genkit instance with embedding support:

    ```go
    ctx := context.Background()

    g, err := genkit.Init(ctx, genkit.WithPlugins(&googlegenai.GoogleAI{}))
    if err != nil {
        log.Fatal(err)
    }

    if err = localvec.Init(); err != nil {
        log.Fatal(err)
    }
    ```
  </LangTabItem>
  <LangTabItem lang="python">
    Configure your Genkit instance with vector store support:

    ```python
    from genkit.ai import Genkit, Document
    from genkit.plugins.google_genai import GoogleGenai
    from genkit.plugins.firebase.firestore import FirestoreVectorStore, DistanceMeasure

    ai = Genkit(
        plugins=[
            GoogleGenai(),
            FirestoreVectorStore(
                name='my_firestore_retriever',
                collection='mycollection',
                vector_field='embedding',
                content_field='text',
                embedder='googleai/text-embedding-004',
                distance_measure=DistanceMeasure.EUCLIDEAN,
                firestore_client=firestore_client,
            ),
        ],
    )
    ```
  </LangTabItem>
</LangTabs>

### Define an Indexer

<LangTabs>
  <LangTabItem lang="js">
    The following example shows how to create an indexer to ingest a collection of
    PDF documents and store them in a local vector database.

    #### Create the indexer

    ```ts
    export const menuPdfIndexer = devLocalIndexerRef('menuQA');
    ```

    #### Create chunking config

    This example uses the `llm-chunk` library which provides a simple text splitter
    to break up documents into segments that can be vectorized.

    The following definition configures the chunking function to guarantee a
    document segment of between 1000 and 2000 characters, broken at the end of a
    sentence, with an overlap between chunks of 100 characters.

    ```ts
    const chunkingConfig = {
      minLength: 1000,
      maxLength: 2000,
      splitter: 'sentence',
      overlap: 100,
      delimiters: '',
    } as any;
    ```

    More chunking options for this library can be found in the [llm-chunk
    documentation](https://www.npmjs.com/package/llm-chunk).

    #### Define your indexer flow

    ```ts
    import { Document } from 'genkit/retriever';
    import { chunk } from 'llm-chunk';
    import { readFile } from 'fs/promises';
    import path from 'path';
    import pdf from 'pdf-parse';

    async function extractTextFromPdf(filePath: string) {
      const pdfFile = path.resolve(filePath);
      const dataBuffer = await readFile(pdfFile);
      const data = await pdf(dataBuffer);
      return data.text;
    }

    export const indexMenu = ai.defineFlow(
      {
        name: 'indexMenu',
        inputSchema: z.object({ filePath: z.string().describe('PDF file path') }),
        outputSchema: z.object({
          success: z.boolean(),
          documentsIndexed: z.number(),
          error: z.string().optional(),
        }),
      },
      async ({ filePath }) => {
        try {
          filePath = path.resolve(filePath);

          // Read the pdf
          const pdfTxt = await ai.run('extract-text', () => extractTextFromPdf(filePath));

          // Divide the pdf text into segments
          const chunks = await ai.run('chunk-it', async () => chunk(pdfTxt, chunkingConfig));

          // Convert chunks of text into documents to store in the index.
          const documents = chunks.map((text) => {
            return Document.fromText(text, { filePath });
          });

          // Add documents to the index
          await ai.index({
            indexer: menuPdfIndexer,
            documents,
          });

          return {
            success: true,
            documentsIndexed: documents.length,
          };
        } catch (err) {
          // For unexpected errors that throw exceptions
          return {
            success: false,
            documentsIndexed: 0,
            error: err instanceof Error ? err.message : String(err)
          };
        }
      },
    );
    ```

    #### Run the indexer flow

    ```bash
    genkit flow:run indexMenu '{"filePath": "menu.pdf"}'
    ```
  </LangTabItem>
  <LangTabItem lang="go">
    #### Create chunking config

    This example uses the `textsplitter` library which provides a simple text
    splitter to break up documents into segments that can be vectorized.

    The following definition configures the chunking function to return document
    segments of 200 characters, with an overlap between chunks of 20 characters.

    ```go
    splitter := textsplitter.NewRecursiveCharacter(
        textsplitter.WithChunkSize(200),
        textsplitter.WithChunkOverlap(20),
    )
    ```

    More chunking options for this library can be found in the
    [`langchaingo` documentation](https://pkg.go.dev/github.com/tmc/langchaingo/textsplitter#Option).

    #### Define your indexer flow

    ```go
    genkit.DefineFlow(
        g, "indexMenu",
        func(ctx context.Context, path string) (any, error) {
            // Extract plain text from the PDF. Wrap the logic in Run so it
            // appears as a step in your traces.
            pdfText, err := genkit.Run(ctx, "extract", func() (string, error) {
                return readPDF(path)
            })
            if err != nil {
                return nil, err
            }

            // Split the text into chunks. Wrap the logic in Run so it appears as a
            // step in your traces.
            docs, err := genkit.Run(ctx, "chunk", func() ([]*ai.Document, error) {
                chunks, err := splitter.SplitText(pdfText)
                if err != nil {
                    return nil, err
                }

                var docs []*ai.Document
                for _, chunk := range chunks {
                    docs = append(docs, ai.DocumentFromText(chunk, nil))
                }
                return docs, nil
            })
            if err != nil {
                return nil, err
            }

            // Add chunks to the index using custom index function
            // Implementation depends on your chosen vector database
            return map[string]interface{}{
                "success": true,
                "documentsIndexed": len(docs),
            }, nil
        },
    )
    ```

    ```go
    // Helper function to extract plain text from a PDF. Excerpted from
    // https://github.com/ledongthuc/pdf
    func readPDF(path string) (string, error) {
        f, r, err := pdf.Open(path)
        if f != nil {
            defer f.Close()
        }
        if err != nil {
            return "", err
        }

        reader, err := r.GetPlainText()
        if err != nil {
            return "", err
        }

        bytes, err := io.ReadAll(reader)
        if err != nil {
            return "", err
        }

        return string(bytes), nil
    }
    ```

    #### Run the indexer flow

    ```bash
    genkit flow:run indexMenu '"menu.pdf"'
    ```
  </LangTabItem>
  <LangTabItem lang="python">
    In Python, indexing is typically handled by your vector store's SDK. Here's an example of how you might structure an indexing flow:

    ```python
    @ai.flow()
    async def index_menu(file_path: str):
        # Extract text from PDF (implementation depends on your PDF library)
        pdf_text = extract_text_from_pdf(file_path)
        
        # Chunk the text (implementation depends on your chunking library)
        chunks = chunk_text(pdf_text)
        
        # Index using your vector store's SDK
        # This is specific to your chosen vector database
        # For Firestore, you would use the Firestore SDK directly
        
        return {
            "success": True,
            "documents_indexed": len(chunks)
        }
    ```

    Note: Indexing is outside the scope of Genkit Python and should be done using your vector store's native SDK.
  </LangTabItem>
</LangTabs>

After running the indexing flow, the vector database will be seeded with
documents and ready to be used in Genkit flows with retrieval steps.

### Define a flow with retrieval

The following example shows how you might use a retriever in a RAG flow:

<LangTabs>
  <LangTabItem lang="js">
    ```ts
    import { devLocalRetrieverRef } from '@genkit-ai/dev-local-vectorstore';
    import { googleAI } from '@genkit-ai/googleai';

    // Define the retriever reference
    export const menuRetriever = devLocalRetrieverRef('menuQA');

    export const menuQAFlow = ai.defineFlow(
      { 
        name: 'menuQA', 
        inputSchema: z.object({ query: z.string() }), 
        outputSchema: z.object({ answer: z.string() }) 
      },
      async ({ query }) => {
        // retrieve relevant documents
        const docs = await ai.retrieve({
          retriever: menuRetriever,
          query,
          options: { k: 3 },
        });

        // generate a response
        const { text } = await ai.generate({
          model: googleAI.model('gemini-2.5-flash'),
          prompt: `
    You are acting as a helpful AI assistant that can answer 
    questions about the food available on the menu at Genkit Grub Pub.

    Use only the context provided to answer the question.
    If you don't know, do not make up an answer.
    Do not add or change items on the menu.

    Question: ${query}`,
          docs,
        });

        return { answer: text };
      },
    );
    ```

    #### Run the retriever flow

    ```bash
    genkit flow:run menuQA '{"query": "Recommend a dessert from the menu while avoiding dairy and nuts"}'
    ```
  </LangTabItem>
  <LangTabItem lang="go">
    ```go
    model := googlegenai.Model(g, "gemini-2.5-flash")

    _, menuPdfRetriever, err := localvec.DefineRetriever(
        g, "menuQA", localvec.Config{Embedder: googlegenai.Embedder(g, "text-embedding-004")},
    )
    if err != nil {
        log.Fatal(err)
    }

    genkit.DefineFlow(
      g, "menuQA",
      func(ctx context.Context, question string) (string, error) {
        // Retrieve text relevant to the user's question.
        resp, err := ai.Retrieve(ctx, menuPdfRetriever, ai.WithTextDocs(question))
        if err != nil {
            return "", err
        }

        // Call Generate, including the menu information in your prompt.
        return genkit.GenerateText(ctx, g,
            ai.WithModelName("googleai/gemini-2.5-flash"),
            ai.WithDocs(resp.Documents),
            ai.WithSystem(`
    You are acting as a helpful AI assistant that can answer questions about the
    food available on the menu at Genkit Grub Pub.
    Use only the context provided to answer the question. If you don't know, do not
    make up an answer. Do not add or change items on the menu.`),
            ai.WithPrompt(question))
      })
    ```

    #### Run the retriever flow

    ```bash
    genkit flow:run menuQA '"Recommend a dessert from the menu while avoiding dairy and nuts"'
    ```
  </LangTabItem>
  <LangTabItem lang="python">
    ```python
    @ai.flow()
    async def qa_flow(query: str):
        docs = await ai.retrieve(
            query=Document.from_text(query),
            retriever='firestore/my_firestore_retriever'
        )
        
        response = await ai.generate(
            prompt=f"""
    You are acting as a helpful AI assistant that can answer 
    questions about the food available on the menu at Genkit Grub Pub.

    Use only the context provided to answer the question.
    If you don't know, do not make up an answer.
    Do not add or change items on the menu.

    Question: {query}""",
            docs=docs
        )
        return response.text
    ```

    #### Run the retriever flow

    ```python
    result = await qa_flow('Recommend a dessert from the menu while avoiding dairy and nuts')
    print(result)
    ```
  </LangTabItem>
</LangTabs>

The output for this command should contain a response from the model, grounded
in the indexed menu file.

## Write your own retrievers

It's also possible to create your own retriever. This is useful if your
documents are managed in a document store that is not supported in Genkit (eg:
MySQL, Google Drive, etc.). The Genkit SDK provides flexible methods that let
you provide custom code for fetching documents. You can also define custom
retrievers that build on top of existing retrievers in Genkit and apply advanced
RAG techniques (such as reranking or prompt extensions) on top.

### Simple Retrievers

<LangTabs>
  <LangTabItem lang="js">
    Simple retrievers let you easily convert existing code into retrievers:

    ```ts
    import { z } from 'genkit';
    import { searchEmails } from './db';

    ai.defineSimpleRetriever(
      {
        name: 'myDatabase',
        configSchema: z
          .object({
            limit: z.number().optional(),
          })
          .optional(),
        // we'll extract "message" from the returned email item
        content: 'message',
        // and several keys to use as metadata
        metadata: ['from', 'to', 'subject'],
      },
      async (query, config) => {
        const result = await searchEmails(query.text, { limit: config.limit });
        return result.data.emails;
      },
    );
    ```
  </LangTabItem>
  <LangTabItem lang="go">
    ```go
    // Simple retriever example in Go
    // Implementation depends on your specific use case and data source
    ```
  </LangTabItem>
  <LangTabItem lang="python">
    ```python
    from genkit.types import (
        RetrieverRequest,
        RetrieverResponse,
        Document,
        ActionRunContext
    )

    async def my_retriever(request: RetrieverRequest, ctx: ActionRunContext):
        """Example of a simple retriever.

        Args:
            request: The request to the retriever.
            ctx: The context of the retriever.
        """
        # Your custom retrieval logic here
        return RetrieverResponse(documents=[
            Document.from_text('Hello'), 
            Document.from_text('World')
        ])

    ai.define_retriever(name='my_retriever', fn=my_retriever)
    ```
  </LangTabItem>
</LangTabs>

### Custom Retrievers

<LangTabs>
  <LangTabItem lang="js">
    ```ts
    import { CommonRetrieverOptionsSchema } from 'genkit/retriever';
    import { z } from 'genkit';

    export const menuRetriever = devLocalRetrieverRef('menuQA');

    const advancedMenuRetrieverOptionsSchema = CommonRetrieverOptionsSchema.extend({
      preRerankK: z.number().max(1000),
    });

    const advancedMenuRetriever = ai.defineRetriever(
      {
        name: `custom/advancedMenuRetriever`,
        configSchema: advancedMenuRetrieverOptionsSchema,
      },
      async (input, options) => {
        const extendedPrompt = await extendPrompt(input);
        const docs = await ai.retrieve({
          retriever: menuRetriever,
          query: extendedPrompt,
          options: { k: options.preRerankK || 10 },
        });
        const rerankedDocs = await rerank(docs);
        return rerankedDocs.slice(0, options.k || 3);
      },
    );
    ```

    (`extendPrompt` and `rerank` is something you would have to implement yourself,
    not provided by the framework)

    And then you can just swap out your retriever:

    ```ts
    const docs = await ai.retrieve({
      retriever: advancedRetriever,
      query: input,
      options: { preRerankK: 7, k: 3 },
    });
    ```
  </LangTabItem>
  <LangTabItem lang="go">
    For example, suppose you have a custom re-ranking function you want to use. The
    following example defines a custom retriever that applies your function to the
    menu retriever defined earlier:

    ```go
    type CustomMenuRetrieverOptions struct {
        K          int
        PreRerankK int
    }

    advancedMenuRetriever := genkit.DefineRetriever(
        g, "custom", "advancedMenuRetriever",
        func(ctx context.Context, req *ai.RetrieverRequest) (*ai.RetrieverResponse, error) {
            // Handle options passed using our custom type.
            opts, _ := req.Options.(CustomMenuRetrieverOptions)
            // Set fields to default values when either the field was undefined
            // or when req.Options is not a CustomMenuRetrieverOptions.
            if opts.K == 0 {
                opts.K = 3
            }
            if opts.PreRerankK == 0 {
                opts.PreRerankK = 10
            }

            // Call the retriever as in the simple case.
            resp, err := ai.Retrieve(ctx, menuPDFRetriever,
                ai.WithDocs(req.Query),
                ai.WithConfig(localvec.RetrieverOptions{K: opts.PreRerankK}),
            )
            if err != nil {
                return nil, err
            }

            // Re-rank the returned documents using your custom function.
            rerankedDocs := rerank(resp.Documents)
            resp.Documents = rerankedDocs[:opts.K]

            return resp, nil
        },
    )
    ```
  </LangTabItem>
  <LangTabItem lang="python">
    ```python
    async def advanced_retriever(request: RetrieverRequest, ctx: ActionRunContext):
        """Example of an advanced retriever with custom logic."""
        
        # First, get initial results from base retriever
        initial_docs = await ai.retrieve(
            query=request.query,
            retriever='my_base_retriever'
        )
        
        # Apply custom reranking or filtering logic
        reranked_docs = custom_rerank_function(initial_docs, request.query)
        
        # Return top K results
        k = getattr(request.options, 'k', 3)
        return RetrieverResponse(documents=reranked_docs[:k])

    ai.define_retriever(name='advanced_retriever', fn=advanced_retriever)
    ```

    Then you can use your custom retriever:

    ```python
    docs = await ai.retrieve(
        query=Document.from_text(query),
        retriever='advanced_retriever'
    )
    ```
  </LangTabItem>
</LangTabs>

## Rerankers and Two-Stage Retrieval

<LangTabs>
  <LangTabItem lang="js">
    A reranking model â€” also known as a cross-encoder â€” is a type of model that,
    given a query and document, will output a similarity score. We use this score to
    reorder the documents by relevance to our query. Reranker APIs take a list of
    documents (for example the output of a retriever) and reorders the documents
    based on their relevance to the query. This step can be useful for fine-tuning
    the results and ensuring that the most pertinent information is used in the
    prompt provided to a generative model.

    #### Reranker Example

    A reranker in Genkit is defined in a similar syntax to retrievers and indexers.
    Here is an example using a reranker in Genkit. This flow reranks a set of
    documents based on their relevance to the provided query using a predefined
    Vertex AI reranker.

    ```ts
    const FAKE_DOCUMENT_CONTENT = [
      'pythagorean theorem',
      'e=mc^2',
      'pi',
      'dinosaurs',
      'quantum mechanics',
      'pizza',
      'harry potter',
    ];

    export const rerankFlow = ai.defineFlow(
      {
        name: 'rerankFlow',
        inputSchema: z.object({ query: z.string() }),
        outputSchema: z.array(
          z.object({
            text: z.string(),
            score: z.number(),
          }),
        ),
      },
      async ({ query }) => {
        const documents = FAKE_DOCUMENT_CONTENT.map((text) => ({ content: text }));

        const rerankedDocuments = await ai.rerank({
          reranker: 'vertexai/semantic-ranker-512',
          query: { content: query },
          documents,
        });

        return rerankedDocuments.map((doc) => ({
          text: doc.content,
          score: doc.metadata.score,
        }));
      },
    );
    ```

    This reranker uses the Vertex AI genkit plugin with `semantic-ranker-512` to
    score and rank documents. The higher the score, the more relevant the document
    is to the query.

    #### Custom Rerankers

    You can also define custom rerankers to suit your specific use case. This is
    helpful when you need to rerank documents using your own custom logic or a
    custom model. Here's a simple example of defining a custom reranker:

    ```ts
    export const customReranker = ai.defineReranker(
      {
        name: 'custom/reranker',
        configSchema: z.object({
          k: z.number().optional(),
        }),
      },
      async (query, documents, options) => {
        // Your custom reranking logic here
        const rerankedDocs = documents.map((doc) => {
          const score = Math.random(); // Assign random scores for demonstration
          return {
            ...doc,
            metadata: { ...doc.metadata, score },
          };
        });

        return rerankedDocs.sort((a, b) => b.metadata.score - a.metadata.score).slice(0, options.k || 3);
      },
    );
    ```

    Once defined, this custom reranker can be used just like any other reranker in
    your RAG flows, giving you flexibility to implement advanced reranking
    strategies.
  </LangTabItem>
  <LangTabItem lang="go">
    Reranking functionality in Go can be implemented as part of custom retrievers.
    You can apply reranking logic within your custom retriever implementations.
  </LangTabItem>
  <LangTabItem lang="python">
    Reranking functionality in Python can be implemented as part of custom retrievers.
    You can apply reranking logic within your custom retriever implementations.
  </LangTabItem>
</LangTabs>

## Next steps

- Learn about [tool calling](/unified-docs/tool-calling) to give your RAG system access to external APIs and functions
- Explore [multi-agent systems](/docs/multi-agent) for coordinating multiple AI agents with RAG capabilities
- Check out the vector database plugins for production-ready RAG implementations
- See the [evaluation guide](/docs/evaluation) for testing and improving your RAG system's performance
