---
title: LanceDB Vector Database
description: Learn how to use LanceDB with Genkit across JavaScript, Go, and Python for embedded vector storage, semantic search, and RAG applications.
---

import LanguageSelector from '../../../../components/LanguageSelector.astro';
import LanguageContent from '../../../../components/LanguageContent.astro';

<LanguageSelector />

LanceDB is an open-source vector database designed for AI applications. It provides embedded vector storage with high performance, making it ideal for applications that need fast vector similarity search without the complexity of managing a separate database server.

## Installation and Setup

<LanguageContent lang="js">
Install the LanceDB plugin:

    ```bash
    npm install genkitx-lancedb
    ```

    Configure the plugin when initializing Genkit:

    ```ts
    import { genkit } from 'genkit';
    import { lancedb } from 'genkitx-lancedb';
    import { googleAI } from '@genkit-ai/googleai';

    const ai = genkit({
      plugins: [
        googleAI(),
        lancedb([
          {
            dbUri: '.db', // Database directory
            tableName: 'documents',
            embedder: googleAI.embedder('gemini-embedding-001'),
          },
        ]),
      ],
    });
    ```

    ### Configuration Options

    ```ts
    // Advanced configuration
    const ai = genkit({
      plugins: [
        googleAI(),
        lancedb([
          {
            dbUri: './vector-db', // Custom database directory
            tableName: 'my-documents',
            embedder: googleAI.embedder('gemini-embedding-001'),
            // Additional LanceDB options can be specified here
          },
        ]),
      ],
    });
    ```
</LanguageContent>

<LanguageContent lang="go">
For Go applications, you can use LanceDB through the Go client:

    ```bash
    go get github.com/lancedb/lancedb-go
    ```

    ```go
    package main

    import (
        "context"
        "github.com/firebase/genkit/go/genkit"
        "github.com/firebase/genkit/go/plugins/lancedb"
        "github.com/firebase/genkit/go/plugins/googleai"
    )

    func main() {
        ctx := context.Background()
        g, err := genkit.Init(ctx,
            genkit.WithPlugins(
                &googleai.GoogleAI{},
                &lancedb.LanceDB{
                    DatabaseURI: "./vector-db",
                    Tables: []lancedb.TableConfig{
                        {
                            Name:     "documents",
                            Embedder: "googleai/gemini-embedding-001",
                        },
                    },
                },
            ),
        )
        if err != nil {
            log.Fatal(err)
        }
    }
    ```
</LanguageContent>

<LanguageContent lang="python">
For Python applications, install the LanceDB client:

    ```bash
    pip install lancedb genkit-plugin-lancedb
    ```

    ```python
    from genkit.ai import Genkit
    from genkit.plugins.lancedb import LanceDB
    from genkit.plugins.google_genai import GoogleGenAI

    ai = Genkit(
        plugins=[
            GoogleGenAI(),
            LanceDB(
                database_uri="./vector-db",
                tables=[
                    {
                        "name": "documents",
                        "embedder": "googleai/gemini-embedding-001",
                    }
                ],
            ),
        ],
    )
    ```
</LanguageContent>

## Basic Usage

### Document Indexing

<LanguageContent lang="js">
Index documents for semantic search:

    ```ts
    import { lancedbIndexerRef, WriteMode } from 'genkitx-lancedb';
    import { Document } from 'genkit';

    // Create indexer reference
    const documentsIndexer = lancedbIndexerRef({
      tableName: 'documents',
    });

    // Prepare documents for indexing
    const documents: Document[] = [
      {
        content: 'LanceDB is an open-source vector database for AI applications.',
        metadata: {
          title: 'LanceDB Overview',
          category: 'database',
          source: 'documentation',
        },
      },
      {
        content: 'Embedded vector databases provide fast local search capabilities.',
        metadata: {
          title: 'Embedded Databases',
          category: 'technology',
          source: 'blog',
        },
      },
    ];

    // Index documents
    await ai.index({
      indexer: documentsIndexer,
      documents,
      options: {
        writeMode: WriteMode.Overwrite, // or WriteMode.Append
      },
    });

    // Batch indexing for better performance
    const batchSize = 100;
    for (let i = 0; i < largeDocumentSet.length; i += batchSize) {
      const batch = largeDocumentSet.slice(i, i + batchSize);
      await ai.index({
        indexer: documentsIndexer,
        documents: batch,
        options: {
          writeMode: i === 0 ? WriteMode.Overwrite : WriteMode.Append,
        },
      });
    }
    ```
</LanguageContent>

<LanguageContent lang="go">
Index documents for semantic search:

    ```go
    import (
        "context"
        "github.com/firebase/genkit/go/ai"
        "github.com/firebase/genkit/go/genkit"
    )

    func indexDocuments(ctx context.Context) error {
        documents := []ai.Document{
            {
                Content: "LanceDB is an open-source vector database for AI applications.",
                Metadata: map[string]interface{}{
                    "title":    "LanceDB Overview",
                    "category": "database",
                    "source":   "documentation",
                },
            },
            {
                Content: "Embedded vector databases provide fast local search capabilities.",
                Metadata: map[string]interface{}{
                    "title":    "Embedded Databases",
                    "category": "technology",
                    "source":   "blog",
                },
            },
        }

        // Index documents
        err := genkit.Index(ctx, g,
            ai.WithIndexer("lancedb/documents"),
            ai.WithDocuments(documents),
            ai.WithOptions(map[string]interface{}{
                "writeMode": "overwrite", // or "append"
            }),
        )
        if err != nil {
            return fmt.Errorf("failed to index documents: %w", err)
        }

        return nil
    }

    // Batch indexing
    func batchIndexDocuments(ctx context.Context, documents []ai.Document, batchSize int) error {
        for i := 0; i < len(documents); i += batchSize {
            end := i + batchSize
            if end > len(documents) {
                end = len(documents)
            }
            
            batch := documents[i:end]
            writeMode := "append"
            if i == 0 {
                writeMode = "overwrite"
            }

            err := genkit.Index(ctx, g,
                ai.WithIndexer("lancedb/documents"),
                ai.WithDocuments(batch),
                ai.WithOptions(map[string]interface{}{
                    "writeMode": writeMode,
                }),
            )
            if err != nil {
                return fmt.Errorf("failed to index batch: %w", err)
            }
        }
        return nil
    }
    ```
</LanguageContent>

<LanguageContent lang="python">
Index documents for semantic search:

    ```python
    from typing import List, Dict, Any

    # Prepare documents for indexing
    documents = [
        {
            "content": "LanceDB is an open-source vector database for AI applications.",
            "metadata": {
                "title": "LanceDB Overview",
                "category": "database",
                "source": "documentation",
            },
        },
        {
            "content": "Embedded vector databases provide fast local search capabilities.",
            "metadata": {
                "title": "Embedded Databases",
                "category": "technology",
                "source": "blog",
            },
        },
    ]

    # Index documents
    async def index_documents(docs: List[Dict[str, Any]], table_name: str = "documents"):
        try:
            indexer = f"lancedb/{table_name}"
            
            await ai.index(
                indexer=indexer,
                documents=docs,
                options={
                    "write_mode": "overwrite"  # or "append"
                }
            )
            
            return {"indexed": len(docs), "success": True}
        except Exception as error:
            print(f"Indexing failed: {error}")
            return {"indexed": 0, "success": False}

    # Batch indexing for better performance
    async def batch_index_documents(
        docs: List[Dict[str, Any]], 
        table_name: str = "documents",
        batch_size: int = 100
    ):
        total_indexed = 0
        
        for i in range(0, len(docs), batch_size):
            batch = docs[i:i + batch_size]
            write_mode = "overwrite" if i == 0 else "append"
            
            try:
                await ai.index(
                    indexer=f"lancedb/{table_name}",
                    documents=batch,
                    options={"write_mode": write_mode}
                )
                total_indexed += len(batch)
            except Exception as error:
                print(f"Batch indexing failed: {error}")
                break
        
        return {"indexed": total_indexed, "success": total_indexed == len(docs)}
    ```
</LanguageContent>

### Document Retrieval

<LanguageContent lang="js">
Retrieve relevant documents using semantic search:

    ```ts
    import { lancedbRetrieverRef } from 'genkitx-lancedb';

    // Create retriever reference
    const documentsRetriever = lancedbRetrieverRef({
      tableName: 'documents',
      displayName: 'Documents',
    });

    // Basic retrieval
    const query = "What is an embedded vector database?";
    const docs = await ai.retrieve({
      retriever: documentsRetriever,
      query,
      options: {
        k: 5, // Number of documents to retrieve
      },
    });

    console.log('Retrieved documents:', docs);

    // Advanced retrieval with filtering
    const filteredDocs = await ai.retrieve({
      retriever: documentsRetriever,
      query,
      options: {
        k: 3,
        filter: {
          category: 'database', // Metadata filtering
        },
      },
    });
    ```
</LanguageContent>

<LanguageContent lang="go">
Retrieve relevant documents using semantic search:

    ```go
    // Basic retrieval
    func retrieveDocuments(ctx context.Context, query string) ([]ai.Document, error) {
        docs, err := genkit.Retrieve(ctx, g,
            ai.WithRetriever("lancedb/documents"),
            ai.WithQuery(query),
            ai.WithOptions(map[string]interface{}{
                "k": 5,
            }),
        )
        if err != nil {
            return nil, fmt.Errorf("retrieval failed: %w", err)
        }

        return docs, nil
    }

    // Advanced retrieval with filtering
    func advancedRetrieve(ctx context.Context, query string, limit int, filter map[string]interface{}) ([]ai.Document, error) {
        docs, err := genkit.Retrieve(ctx, g,
            ai.WithRetriever("lancedb/documents"),
            ai.WithQuery(query),
            ai.WithOptions(map[string]interface{}{
                "k":      limit,
                "filter": filter,
            }),
        )
        if err != nil {
            return nil, fmt.Errorf("advanced retrieval failed: %w", err)
        }

        return docs, nil
    }

    // Usage example
    func searchDocuments(ctx context.Context) error {
        // Basic search
        docs, err := retrieveDocuments(ctx, "What is an embedded vector database?")
        if err != nil {
            return err
        }

        // Filtered search
        filteredDocs, err := advancedRetrieve(ctx, 
            "database concepts", 
            3, 
            map[string]interface{}{
                "category": "database",
            },
        )
        if err != nil {
            return err
        }

        fmt.Printf("Found %d documents, %d filtered\n", len(docs), len(filteredDocs))
        return nil
    }
    ```
</LanguageContent>

<LanguageContent lang="python">
Retrieve relevant documents using semantic search:

    ```python
    from typing import List, Dict, Any, Optional

    # Basic retrieval
    async def retrieve_documents(query: str, table_name: str = "documents", k: int = 5) -> List[Dict[str, Any]]:
        try:
            retriever = f"lancedb/{table_name}"
            docs = await ai.retrieve(
                retriever=retriever,
                query=query,
                options={"k": k}
            )
            return docs
        except Exception as error:
            print(f"Retrieval failed: {error}")
            return []

    # Advanced retrieval with filtering
    async def advanced_retrieve(
        query: str,
        table_name: str = "documents",
        k: int = 5,
        filter_criteria: Optional[Dict[str, Any]] = None
    ) -> List[Dict[str, Any]]:
        try:
            retriever = f"lancedb/{table_name}"
            
            options = {"k": k}
            if filter_criteria:
                options["filter"] = filter_criteria
            
            docs = await ai.retrieve(
                retriever=retriever,
                query=query,
                options=options
            )
            
            return docs
        except Exception as error:
            print(f"Advanced retrieval failed: {error}")
            return []

    # Usage examples
    async def search_examples():
        # Basic search
        docs = await retrieve_documents("What is an embedded vector database?", k=5)
        
        # Filtered search
        filtered_docs = await advanced_retrieve(
            query="database concepts",
            k=3,
            filter_criteria={"category": "database"}
        )
        
        print(f"Found {len(docs)} documents, {len(filtered_docs)} filtered")
        return docs, filtered_docs
    ```
</LanguageContent>

## Advanced Features

### Complete RAG Implementation

<LanguageContent lang="js">
Build a complete RAG system with document processing:

    ```ts
    import { lancedbIndexerRef, lancedbRetrieverRef, WriteMode } from 'genkitx-lancedb';
    import { chunk } from 'llm-chunk';
    import { readFile } from 'fs/promises';
    import pdf from 'pdf-parse';

    // Document processing configuration
    const chunkingConfig = {
      minLength: 1000,
      maxLength: 2000,
      splitter: 'sentence',
      overlap: 100,
    };

    // PDF text extraction
    async function extractTextFromPdf(filePath: string): Promise<string> {
      const dataBuffer = await readFile(filePath);
      const data = await pdf(dataBuffer);
      return data.text;
    }

    // Document indexing flow
    export const indexDocumentFlow = ai.defineFlow(
      {
        name: 'indexDocument',
        inputSchema: z.object({ 
          filePath: z.string(),
          tableName: z.string().optional().default('documents'),
        }),
        outputSchema: z.object({
          success: z.boolean(),
          documentsIndexed: z.number(),
          error: z.string().optional(),
        }),
      },
      async ({ filePath, tableName }) => {
        try {
          // Extract text from PDF
          const pdfText = await ai.run('extract-text', () =>
            extractTextFromPdf(filePath)
          );

          // Chunk the text
          const chunks = await ai.run('chunk-text', () =>
            chunk(pdfText, chunkingConfig)
          );

          // Convert to documents
          const documents = chunks.map((text, index) => ({
            content: text,
            metadata: {
              filePath,
              chunkIndex: index,
              source: 'pdf',
            },
          }));

          // Index documents
          const indexer = lancedbIndexerRef({ tableName });
          await ai.index({
            indexer,
            documents,
            options: { writeMode: WriteMode.Overwrite },
          });

          return {
            success: true,
            documentsIndexed: documents.length,
          };
        } catch (error) {
          return {
            success: false,
            documentsIndexed: 0,
            error: error instanceof Error ? error.message : String(error),
          };
        }
      },
    );

    // RAG query flow
    export const ragQueryFlow = ai.defineFlow(
      {
        name: 'ragQuery',
        inputSchema: z.object({ 
          query: z.string(),
          tableName: z.string().optional().default('documents'),
        }),
        outputSchema: z.object({ 
          answer: z.string(),
          sources: z.array(z.string()),
        }),
      },
      async ({ query, tableName }) => {
        // Retrieve relevant documents
        const retriever = lancedbRetrieverRef({ tableName });
        const docs = await ai.retrieve({
          retriever,
          query,
          options: { k: 3 },
        });

        // Generate answer using retrieved context
        const { text } = await ai.generate({
          model: googleAI.model('gemini-2.5-flash'),
          prompt: `
            Answer the following question using only the provided context.
            If you cannot answer based on the context, say so.

            Context:
            ${docs.map(doc => doc.content).join('\n\n')}

            Question: ${query}
          `,
        });

        return {
          answer: text,
          sources: docs.map(doc => doc.metadata?.filePath || 'unknown').filter(Boolean),
        };
      },
    );
    ```
</LanguageContent>

<LanguageContent lang="go">
Build a complete RAG system with document processing:

    ```go
    import (
        "context"
        "fmt"
        "strings"
        "github.com/firebase/genkit/go/ai"
        "github.com/firebase/genkit/go/genkit"
    )

    // Document processing and indexing
    func indexDocumentFromText(ctx context.Context, text, source string, tableName string) error {
        // Simple text chunking (in production, use a proper chunking library)
        chunks := chunkText(text, 1000, 100)
        
        var documents []ai.Document
        for i, chunk := range chunks {
            documents = append(documents, ai.Document{
                Content: chunk,
                Metadata: map[string]interface{}{
                    "source":     source,
                    "chunkIndex": i,
                },
            })
        }

        // Index documents
        err := genkit.Index(ctx, g,
            ai.WithIndexer(fmt.Sprintf("lancedb/%s", tableName)),
            ai.WithDocuments(documents),
            ai.WithOptions(map[string]interface{}{
                "writeMode": "overwrite",
            }),
        )
        if err != nil {
            return fmt.Errorf("failed to index documents: %w", err)
        }

        return nil
    }

    // Simple text chunking function
    func chunkText(text string, chunkSize, overlap int) []string {
        words := strings.Fields(text)
        var chunks []string
        
        for i := 0; i < len(words); i += chunkSize - overlap {
            end := i + chunkSize
            if end > len(words) {
                end = len(words)
            }
            
            chunk := strings.Join(words[i:end], " ")
            chunks = append(chunks, chunk)
            
            if end == len(words) {
                break
            }
        }
        
        return chunks
    }

    // RAG query function
    func performRAGQuery(ctx context.Context, query, tableName string) (string, []string, error) {
        // Retrieve relevant documents
        docs, err := genkit.Retrieve(ctx, g,
            ai.WithRetriever(fmt.Sprintf("lancedb/%s", tableName)),
            ai.WithQuery(query),
            ai.WithOptions(map[string]interface{}{
                "k": 3,
            }),
        )
        if err != nil {
            return "", nil, fmt.Errorf("retrieval failed: %w", err)
        }

        // Build context from retrieved documents
        var contextParts []string
        var sources []string
        for _, doc := range docs {
            contextParts = append(contextParts, doc.Content)
            if source, ok := doc.Metadata["source"].(string); ok {
                sources = append(sources, source)
            }
        }
        context := strings.Join(contextParts, "\n\n")

        // Generate answer
        prompt := fmt.Sprintf(`
            Answer the following question using only the provided context.
            If you cannot answer based on the context, say so.

            Context:
            %s

            Question: %s
        `, context, query)

        resp, err := genkit.Generate(ctx, g,
            ai.WithModel("googleai/gemini-2.5-flash"),
            ai.WithPrompt(prompt),
        )
        if err != nil {
            return "", nil, fmt.Errorf("generation failed: %w", err)
        }

        return resp.Text(), sources, nil
    }
    ```
</LanguageContent>

<LanguageContent lang="python">
Build a complete RAG system with document processing:

    ```python
    import re
    from typing import List, Dict, Any, Tuple

    # Simple text chunking function
    def chunk_text(text: str, chunk_size: int = 1000, overlap: int = 100) -> List[str]:
        words = text.split()
        chunks = []
        
        for i in range(0, len(words), chunk_size - overlap):
            end = min(i + chunk_size, len(words))
            chunk = ' '.join(words[i:end])
            chunks.append(chunk)
            
            if end == len(words):
                break
        
        return chunks

    # Document indexing function
    async def index_document_from_text(
        text: str, 
        source: str, 
        table_name: str = "documents"
    ) -> Dict[str, Any]:
        try:
            # Chunk the text
            chunks = chunk_text(text, chunk_size=1000, overlap=100)
            
            # Convert to documents
            documents = [
                {
                    "content": chunk,
                    "metadata": {
                        "source": source,
                        "chunk_index": i,
                    }
                }
                for i, chunk in enumerate(chunks)
            ]
            
            # Index documents
            await ai.index(
                indexer=f"lancedb/{table_name}",
                documents=documents,
                options={"write_mode": "overwrite"}
            )
            
            return {
                "success": True,
                "documents_indexed": len(documents),
            }
        except Exception as error:
            return {
                "success": False,
                "documents_indexed": 0,
                "error": str(error),
            }

    # RAG query function
    async def perform_rag_query(
        query: str, 
        table_name: str = "documents",
        k: int = 3
    ) -> Tuple[str, List[str]]:
        try:
            # Retrieve relevant documents
            docs = await ai.retrieve(
                retriever=f"lancedb/{table_name}",
                query=query,
                options={"k": k}
            )
            
            # Build context and collect sources
            context_parts = [doc["content"] for doc in docs]
            context = "\n\n".join(context_parts)
            
            sources = [
                doc.get("metadata", {}).get("source", "unknown") 
                for doc in docs
            ]
            
            # Generate answer
            prompt = f"""
            Answer the following question using only the provided context.
            If you cannot answer based on the context, say so.

            Context:
            {context}

            Question: {query}
            """
            
            response = await ai.generate(
                model="googleai/gemini-2.5-flash",
                prompt=prompt
            )
            
            return response.text, list(set(sources))  # Remove duplicates
            
        except Exception as error:
            print(f"RAG query failed: {error}")
            return "I'm sorry, I couldn't process your query.", []

    # Complete RAG workflow example
    async def rag_workflow_example():
        # Index a document
        sample_text = """
        LanceDB is an open-source vector database that provides embedded 
        vector storage capabilities. It's designed for AI applications 
        that need fast similarity search without managing a separate 
        database server.
        """
        
        index_result = await index_document_from_text(
            text=sample_text,
            source="lancedb_overview.txt",
            table_name="knowledge_base"
        )
        
        if index_result["success"]:
            # Query the indexed documents
            answer, sources = await perform_rag_query(
                query="What is LanceDB?",
                table_name="knowledge_base"
            )
            
            return {
                "answer": answer,
                "sources": sources,
                "indexed_documents": index_result["documents_indexed"]
            }
        else:
            return {"error": "Failed to index documents"}
    ```
</LanguageContent>

## Best Practices

### Performance Optimization

1. **Batch operations**: Index documents in batches for better performance
2. **Appropriate chunk sizes**: Balance between context and retrieval precision
3. **Embedding model selection**: Choose models that match your use case
4. **Database location**: Use local storage for development, consider cloud storage for production

### Data Management

1. **Write modes**: Use `Overwrite` for complete rebuilds, `Append` for incremental updates
2. **Metadata design**: Structure metadata for effective filtering
3. **Version control**: Track document versions and updates
4. **Backup strategies**: Regular backups of the database directory

### Production Deployment

1. **Database persistence**: Ensure database directory is persistent in containerized environments
2. **Resource allocation**: Allocate sufficient memory for large datasets
3. **Monitoring**: Track query performance and database size
4. **Scaling**: Consider partitioning large datasets across multiple tables

## Next Steps

- Learn about [RAG implementation](/unified-docs/rag) to build complete retrieval-augmented generation systems
- Explore [creating flows](/unified-docs/creating-flows) to build structured AI workflows with vector search
- See [deployment guides](/unified-docs/deployment) for production deployment strategies
- Check out other vector database options for different use cases
