---
title: OpenAI-Compatible APIs
description: Learn how to configure and use Genkit with OpenAI-compatible APIs across JavaScript, Go, and Python, including local servers like Ollama and custom model providers.
---

import LanguageSelector from '../../../../components/LanguageSelector.astro';
import LanguageContent from '../../../../components/LanguageContent.astro';
import CopyMarkdownButton from '../../../../components/CopyMarkdownButton.astro';

<div style="display: flex; justify-content: space-between; align-items: center; gap: 1rem; margin: 1rem 0 1rem 0;">
  <LanguageSelector />
  <CopyMarkdownButton />
</div>

The OpenAI-compatible plugin allows you to connect Genkit to any service that exposes an OpenAI-compatible API. This includes official OpenAI services, local servers like Ollama, and custom model providers that implement the OpenAI API specification.

## Overview

OpenAI-compatible APIs provide a standardized interface for interacting with language models, making it easy to switch between different providers or use local models without changing your application code.

**Supported use cases:**
- Local model servers (Ollama, LM Studio, etc.)
- Custom model deployments
- Alternative cloud providers with OpenAI-compatible endpoints
- Self-hosted models with OpenAI API wrappers

## Installation

<LanguageContent lang="js">
```bash
npm install @genkit-ai/compat-oai
```
</LanguageContent>

<LanguageContent lang="go">
```bash
go get github.com/firebase/genkit/go/plugins/openai
```
</LanguageContent>

<LanguageContent lang="python">
```bash
pip install genkit-plugin-openai
```
</LanguageContent>

## Basic configuration

<LanguageContent lang="js">
### JavaScript/TypeScript

```typescript
import { genkit } from 'genkit';
import { openAICompatible } from '@genkit-ai/compat-oai';

const ai = genkit({
  plugins: [
    openAICompatible({
      name: 'localLlama',
      apiKey: 'ollama', // Required, but can be a placeholder for local servers
      baseURL: 'http://localhost:11434/v1', // Example for Ollama
    }),
  ],
});
```

### Configuration options

- **`name`**: (Required) A unique name for the plugin instance
- **`apiKey`**: API key for the service (can be placeholder for local servers)
- **`baseURL`**: The base URL of the OpenAI-compatible API endpoint
- **`timeout`**: Request timeout in milliseconds
- **`defaultHeaders`**: Additional headers to include with requests
</LanguageContent>

<LanguageContent lang="go">
### Go

```go
import (
    "context"
    "github.com/firebase/genkit/go/genkit"
    "github.com/firebase/genkit/go/plugins/openai"
)

func main() {
    ctx := context.Background()
    
    g, err := genkit.Init(ctx,
        genkit.WithPlugins(&openai.OpenAI{
            APIKey:  "ollama", // Can be placeholder for local servers
            BaseURL: "http://localhost:11434/v1",
        }),
    )
    if err != nil {
        log.Fatalf("could not initialize Genkit: %v", err)
    }
}
```
</LanguageContent>

<LanguageContent lang="python">
### Python

```python
from genkit.ai import Genkit
from genkit.plugins.openai import OpenAI

ai = Genkit(
    plugins=[
        OpenAI(
            api_key="ollama",  # Can be placeholder for local servers
            base_url="http://localhost:11434/v1"
        )
    ]
)
```
</LanguageContent>

## Using models

<LanguageContent lang="js">
### Defining model references

```typescript
import { modelRef } from 'genkit';

// Define a reference to your model
export const localLlamaModel = modelRef({
  name: 'localLlama/llama3.2', // Format: pluginName/modelId
  // Optional: specify model capabilities
  info: {
    supports: {
      multiturn: true,
      tools: false,
      media: false,
      systemRole: true,
    },
  },
});
```

### Using in flows

```typescript
export const chatFlow = ai.defineFlow(
  {
    name: 'chatFlow',
    inputSchema: z.object({ message: z.string() }),
    outputSchema: z.object({ response: z.string() }),
  },
  async ({ message }) => {
    const llmResponse = await ai.generate({
      model: localLlamaModel,
      prompt: message,
      config: {
        temperature: 0.7,
        maxOutputTokens: 1000,
      },
    });
    
    return { response: llmResponse.text };
  }
);
```
</LanguageContent>

<LanguageContent lang="go">
### Using models

```go
import (
    "github.com/firebase/genkit/go/ai"
)

// Use the model in generation
resp, err := ai.Generate(ctx, g,
    ai.WithPrompt("Tell me about Go programming"),
    ai.WithModel("openai/llama3.2"), // Use the model ID from your provider
    ai.WithConfig(map[string]interface{}{
        "temperature":      0.7,
        "maxOutputTokens": 1000,
    }),
)
if err != nil {
    log.Fatalf("generation failed: %v", err)
}

fmt.Println(resp.Text())
```
</LanguageContent>

<LanguageContent lang="python">
### Using models

```python
# Generate content using the model
response = await ai.generate(
    prompt="Tell me about Python programming",
    model="openai/llama3.2",  # Use the model ID from your provider
    config={
        "temperature": 0.7,
        "max_output_tokens": 1000,
    }
)

print(response.text)
```
</LanguageContent>

## Common configurations

### Ollama (Local)

<LanguageContent lang="js">
```typescript
openAICompatible({
  name: 'ollama',
  apiKey: 'ollama',
  baseURL: 'http://localhost:11434/v1',
})
```
</LanguageContent>

<LanguageContent lang="go">
```go
&openai.OpenAI{
    APIKey:  "ollama",
    BaseURL: "http://localhost:11434/v1",
}
```
</LanguageContent>

<LanguageContent lang="python">
```python
OpenAI(
    api_key="ollama",
    base_url="http://localhost:11434/v1"
)
```
</LanguageContent>

### LM Studio

<LanguageContent lang="js">
```typescript
openAICompatible({
  name: 'lmstudio',
  apiKey: 'lm-studio',
  baseURL: 'http://localhost:1234/v1',
})
```
</LanguageContent>

<LanguageContent lang="go">
```go
&openai.OpenAI{
    APIKey:  "lm-studio",
    BaseURL: "http://localhost:1234/v1",
}
```
</LanguageContent>

<LanguageContent lang="python">
```python
OpenAI(
    api_key="lm-studio",
    base_url="http://localhost:1234/v1"
)
```
</LanguageContent>

### Custom cloud provider

<LanguageContent lang="js">
```typescript
openAICompatible({
  name: 'customProvider',
  apiKey: process.env.CUSTOM_API_KEY,
  baseURL: 'https://api.customprovider.com/v1',
  defaultHeaders: {
    'X-Custom-Header': 'value',
  },
})
```
</LanguageContent>

<LanguageContent lang="go">
```go
&openai.OpenAI{
    APIKey:  os.Getenv("CUSTOM_API_KEY"),
    BaseURL: "https://api.customprovider.com/v1",
    // Additional headers can be set via HTTP client configuration
}
```
</LanguageContent>

<LanguageContent lang="python">
```python
OpenAI(
    api_key=os.getenv("CUSTOM_API_KEY"),
    base_url="https://api.customprovider.com/v1",
    default_headers={
        "X-Custom-Header": "value"
    }
)
```
</LanguageContent>

## Advanced usage

### Multiple providers

You can configure multiple OpenAI-compatible providers in the same application:

<LanguageContent lang="js">
```typescript
const ai = genkit({
  plugins: [
    openAICompatible({
      name: 'ollama',
      apiKey: 'ollama',
      baseURL: 'http://localhost:11434/v1',
    }),
    openAICompatible({
      name: 'lmstudio',
      apiKey: 'lm-studio',
      baseURL: 'http://localhost:1234/v1',
    }),
  ],
});

// Use different models from different providers
const ollamaModel = modelRef({ name: 'ollama/llama3.2' });
const lmstudioModel = modelRef({ name: 'lmstudio/mistral-7b' });
```
</LanguageContent>

<LanguageContent lang="go">
```go
// Configure multiple providers by creating separate clients
ollamaClient := &openai.OpenAI{
    APIKey:  "ollama",
    BaseURL: "http://localhost:11434/v1",
}

lmstudioClient := &openai.OpenAI{
    APIKey:  "lm-studio", 
    BaseURL: "http://localhost:1234/v1",
}

g, err := genkit.Init(ctx,
    genkit.WithPlugins(ollamaClient, lmstudioClient),
)
```
</LanguageContent>

<LanguageContent lang="python">
```python
ai = Genkit(
    plugins=[
        OpenAI(
            api_key="ollama",
            base_url="http://localhost:11434/v1"
        ),
        OpenAI(
            api_key="lm-studio",
            base_url="http://localhost:1234/v1"
        )
    ]
)
```
</LanguageContent>

### Custom model configurations

<LanguageContent lang="js">
```typescript
const customModel = modelRef({
  name: 'ollama/custom-model',
  info: {
    supports: {
      multiturn: true,
      tools: true,
      media: false,
      systemRole: true,
    },
    versions: ['v1', 'v2'],
  },
  configSchema: z.object({
    temperature: z.number().min(0).max(2).optional(),
    topP: z.number().min(0).max(1).optional(),
    maxOutputTokens: z.number().positive().optional(),
    stopSequences: z.array(z.string()).optional(),
  }),
});
```
</LanguageContent>

### Error handling

<LanguageContent lang="js">
```typescript
try {
  const response = await ai.generate({
    model: localModel,
    prompt: "Hello, world!",
  });
  console.log(response.text);
} catch (error) {
  if (error.message.includes('ECONNREFUSED')) {
    console.error('Local server is not running');
  } else if (error.message.includes('401')) {
    console.error('Invalid API key');
  } else {
    console.error('Generation failed:', error.message);
  }
}
```
</LanguageContent>

<LanguageContent lang="go">
```go
resp, err := ai.Generate(ctx, g,
    ai.WithPrompt("Hello, world!"),
    ai.WithModel("openai/llama3.2"),
)
if err != nil {
    if strings.Contains(err.Error(), "connection refused") {
        log.Println("Local server is not running")
    } else if strings.Contains(err.Error(), "401") {
        log.Println("Invalid API key")
    } else {
        log.Printf("Generation failed: %v", err)
    }
    return
}

fmt.Println(resp.Text())
```
</LanguageContent>

<LanguageContent lang="python">
```python
try:
    response = await ai.generate(
        prompt="Hello, world!",
        model="openai/llama3.2"
    )
    print(response.text)
except ConnectionError:
    print("Local server is not running")
except Exception as error:
    if "401" in str(error):
        print("Invalid API key")
    else:
        print(f"Generation failed: {error}")
```
</LanguageContent>

## Streaming responses

<LanguageContent lang="js">
```typescript
const stream = await ai.generateStream({
  model: localModel,
  prompt: "Write a long story about space exploration",
});

for await (const chunk of stream) {
  process.stdout.write(chunk.text);
}
```
</LanguageContent>

<LanguageContent lang="go">
```go
stream, err := ai.GenerateStream(ctx, g,
    ai.WithPrompt("Write a long story about space exploration"),
    ai.WithModel("openai/llama3.2"),
)
if err != nil {
    log.Fatalf("streaming failed: %v", err)
}

for chunk := range stream {
    if chunk.Error != nil {
        log.Printf("Stream error: %v", chunk.Error)
        break
    }
    fmt.Print(chunk.Text)
}
```
</LanguageContent>

<LanguageContent lang="python">
```python
async for chunk in ai.generate_stream(
    prompt="Write a long story about space exploration",
    model="openai/llama3.2"
):
    print(chunk.text, end="", flush=True)
```
</LanguageContent>

## Best practices

### Local development

1. **Start your local server first**: Ensure Ollama, LM Studio, or your custom server is running
2. **Use appropriate timeouts**: Local models may be slower than cloud APIs
3. **Monitor resource usage**: Local models can be resource-intensive

### Production deployment

1. **Use environment variables**: Store API keys and URLs in environment variables
2. **Implement fallbacks**: Have backup providers in case your primary service is unavailable
3. **Monitor performance**: Track response times and error rates
4. **Rate limiting**: Respect rate limits of your chosen provider

### Model selection

1. **Test compatibility**: Verify that your chosen model supports the features you need
2. **Benchmark performance**: Compare different models for your specific use case
3. **Consider costs**: Balance performance requirements with API costs

## Troubleshooting

### Common issues

**Connection refused errors:**
- Verify the local server is running
- Check the correct port and URL
- Ensure no firewall blocking

**Authentication errors:**
- Verify API key is correct
- Check if the service requires authentication
- Ensure proper headers are set

**Model not found errors:**
- Verify the model name matches what's available on the server
- Check if the model needs to be downloaded first (for local servers)

**Timeout errors:**
- Increase timeout values for slower models
- Consider using streaming for long responses
- Check server resource availability

## Next steps

- Learn about [model configuration](/unified-docs/generating-content) for fine-tuning behavior
- Explore [tool calling](/unified-docs/tool-calling) with compatible models
- Set up [observability](/unified-docs/local-observability) to monitor performance
- Review [deployment patterns](/unified-docs/deployment) for production use
