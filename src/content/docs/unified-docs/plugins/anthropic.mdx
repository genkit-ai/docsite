---
title: Anthropic (Claude) Plugin
description: Learn how to use Anthropic's Claude models with Genkit across JavaScript, Go, and Python for advanced reasoning, analysis, and conversational AI.
---

import LanguageSelector from '../../../../components/LanguageSelector.astro';
import CopyMarkdownButton from '../../../../components/CopyMarkdownButton.astro';
import LanguageContent from '../../../../components/LanguageContent.astro';

<div style="display: flex; justify-content: space-between; align-items: center; gap: 1rem; margin: 1rem 0 1rem 0;">
  <LanguageSelector />
  <CopyMarkdownButton />
</div>

Anthropic's Claude models are known for their advanced reasoning capabilities, safety features, and nuanced understanding of complex topics. Claude excels at analysis, writing, math, coding, and thoughtful conversation while maintaining helpful, harmless, and honest interactions.

## Installation and Setup

<LanguageContent lang="js">
Claude models are available in JavaScript through Vertex AI Model Garden. You'll need access to Claude models in your Google Cloud project.

    Install the Vertex AI plugin:

    ```bash
    npm install @genkit-ai/vertexai
    ```

    Configure the plugin with Claude models:

    ```ts
    import { genkit } from 'genkit';
    import { vertexAI } from '@genkit-ai/vertexai';

    const ai = genkit({
      plugins: [
        vertexAI({
          projectId: 'your-project-id',
          location: 'us-central1',
          models: ['claude-3-haiku', 'claude-3-sonnet', 'claude-3-opus'],
        }),
      ],
    });
    ```

    ### Prerequisites

    1. **Google Cloud Project**: Set up a Google Cloud project with Vertex AI enabled
    2. **Claude Model Access**: Request access to Claude models in [Vertex AI Model Garden](https://console.cloud.google.com/vertex-ai/publishers/anthropic/model-garden)
    3. **Authentication**: Configure Google Cloud authentication

    ```bash
    # Set up authentication
    gcloud auth application-default login
    export GOOGLE_CLOUD_PROJECT=your-project-id
    ```

    ### Available Models via Vertex AI

    - **claude-3-haiku**: Fast and efficient for simple tasks
    - **claude-3-sonnet**: Balanced performance and capability
    - **claude-3-opus**: Most capable for complex reasoning
</LanguageContent>

<LanguageContent lang="go">
Claude models are available in Go through the OpenAI-compatible Anthropic plugin.

    Install the required packages:

    ```bash
    go get github.com/firebase/genkit/go/plugins/compat_oai/anthropic
    go get github.com/openai/openai-go/option
    ```

    Configure the Anthropic plugin:

    ```go
    package main

    import (
        "context"
        "github.com/firebase/genkit/go/genkit"
        "github.com/firebase/genkit/go/plugins/compat_oai/anthropic"
        "github.com/openai/openai-go/option"
    )

    func main() {
        ctx := context.Background()
        g, err := genkit.Init(ctx,
            genkit.WithPlugins(&anthropic.Anthropic{
                Opts: []option.RequestOption{
                    option.WithAPIKey(os.Getenv("ANTHROPIC_API_KEY")),
                },
            }),
        )
        if err != nil {
            log.Fatal(err)
        }
    }
    ```

    ### API Key Configuration

    ```bash
    export ANTHROPIC_API_KEY=your_anthropic_api_key
    ```

    Get your API key from [Anthropic Console](https://console.anthropic.com/).

    ### Available Models

    - **claude-3-7-sonnet-20250219**: Latest Claude 3.7 Sonnet with advanced capabilities
    - **claude-3-5-haiku-20241022**: Fast and efficient Claude 3.5 Haiku
    - **claude-3-5-sonnet-20240620**: Balanced Claude 3.5 Sonnet
    - **claude-3-opus-20240229**: Most capable Claude 3 model
    - **claude-3-haiku-20240307**: Fastest Claude 3 model
</LanguageContent>

<LanguageContent lang="python">
Claude models are currently not directly supported in Python Genkit. However, you can access Claude through:

    1. **Vertex AI Model Garden** (if available in your region)
    2. **Custom OpenAI-compatible wrapper** using the Anthropic API

    For Vertex AI access (if available):

    ```python
    from genkit.ai import Genkit
    from genkit.plugins.vertex_ai import VertexAI

    ai = Genkit(
        plugins=[
            VertexAI(
                project_id="your-project-id",
                location="us-central1",
                models=["claude-3-haiku", "claude-3-sonnet", "claude-3-opus"],
            ),
        ],
    )
    ```

    For direct Anthropic API access, you would need to create a custom plugin or use the Anthropic Python SDK directly alongside Genkit.

    ### Environment Configuration

    ```bash
    export GOOGLE_CLOUD_PROJECT=your-project-id
    # or
    export ANTHROPIC_API_KEY=your_anthropic_api_key
    ```
</LanguageContent>

## Basic Usage

### Text Generation

<LanguageContent lang="js">
Use Claude models for text generation through Vertex AI:

    ```ts
    import { genkit, z } from 'genkit';
    import { vertexAI } from '@genkit-ai/vertexai';

    const ai = genkit({
      plugins: [
        vertexAI({
          projectId: 'your-project-id',
          location: 'us-central1',
          models: ['claude-3-sonnet'],
        }),
      ],
    });

    // Basic text generation
    const response = await ai.generate({
      model: 'claude-3-sonnet',
      prompt: 'Explain the concept of quantum entanglement in simple terms.',
    });

    console.log(response.text);

    // Flow with Claude
    export const claudeAnalysisFlow = ai.defineFlow(
      {
        name: 'claudeAnalysisFlow',
        inputSchema: z.object({ 
          text: z.string(),
          analysisType: z.enum(['sentiment', 'summary', 'critique']),
        }),
        outputSchema: z.object({ analysis: z.string() }),
      },
      async ({ text, analysisType }) => {
        const prompts = {
          sentiment: `Analyze the sentiment of this text: "${text}"`,
          summary: `Provide a concise summary of this text: "${text}"`,
          critique: `Provide a thoughtful critique of this text: "${text}"`,
        };

        const response = await ai.generate({
          model: 'claude-3-sonnet',
          prompt: prompts[analysisType],
          config: {
            temperature: 0.3,
            maxTokens: 500,
          },
        });

        return { analysis: response.text };
      },
    );
    ```
</LanguageContent>

<LanguageContent lang="go">
Use Claude models with the Anthropic plugin:

    ```go
    import (
        "context"
        "github.com/firebase/genkit/go/ai"
        "github.com/firebase/genkit/go/genkit"
        "github.com/firebase/genkit/go/plugins/compat_oai/anthropic"
    )

    func main() {
        ctx := context.Background()
        
        // Initialize Anthropic plugin
        claude := &anthropic.Anthropic{
            Opts: []option.RequestOption{
                option.WithAPIKey(os.Getenv("ANTHROPIC_API_KEY")),
            },
        }
        g, err := genkit.Init(ctx, genkit.WithPlugins(claude))
        if err != nil {
            log.Fatal(err)
        }

        // Basic text generation
        model := claude.Model(g, "claude-3-7-sonnet-20250219")
        resp, err := genkit.Generate(ctx, g,
            ai.WithModel(model),
            ai.WithPrompt("Explain the concept of quantum entanglement in simple terms."),
        )
        if err != nil {
            log.Fatal(err)
        }

        fmt.Println(resp.Text())

        // Advanced reasoning task
        reasoningResp, err := genkit.Generate(ctx, g,
            ai.WithModel(model),
            ai.WithPrompt("Analyze the ethical implications of AI in healthcare decision-making."),
            ai.WithConfig(map[string]interface{}{
                "temperature": 0.3,
                "max_tokens":  1000,
            }),
        )
        if err != nil {
            log.Fatal(err)
        }

        fmt.Println(reasoningResp.Text())
    }
    ```
</LanguageContent>

<LanguageContent lang="python">
Use Claude models through available integrations:

    ```python
    # If using Vertex AI Model Garden
    from genkit.ai import Genkit
    from genkit.plugins.vertex_ai import VertexAI

    ai = Genkit(
        plugins=[
            VertexAI(
                project_id="your-project-id",
                location="us-central1",
                models=["claude-3-sonnet"],
            ),
        ],
    )

    # Basic text generation
    response = await ai.generate(
        model="claude-3-sonnet",
        prompt="Explain the concept of quantum entanglement in simple terms."
    )
    print(response.text)

    # Analysis task
    async def analyze_text(text: str, analysis_type: str) -> str:
        prompts = {
            "sentiment": f"Analyze the sentiment of this text: \"{text}\"",
            "summary": f"Provide a concise summary of this text: \"{text}\"",
            "critique": f"Provide a thoughtful critique of this text: \"{text}\"",
        }

        response = await ai.generate(
            model="claude-3-sonnet",
            prompt=prompts[analysis_type],
            config={
                "temperature": 0.3,
                "max_tokens": 500,
            }
        )
        return response.text
    ```
</LanguageContent>

## Advanced Features

### Complex Reasoning

<LanguageContent lang="js">
Leverage Claude's reasoning capabilities:

    ```ts
    // Complex analysis flow
    export const complexAnalysisFlow = ai.defineFlow(
      {
        name: 'complexAnalysisFlow',
        inputSchema: z.object({
          problem: z.string(),
          context: z.string().optional(),
        }),
        outputSchema: z.object({
          analysis: z.string(),
          reasoning: z.string(),
          recommendations: z.array(z.string()),
        }),
      },
      async ({ problem, context }) => {
        const prompt = context
          ? `Given this context: ${context}\n\nAnalyze this problem step by step: ${problem}`
          : `Analyze this problem step by step: ${problem}`;

        const response = await ai.generate({
          model: 'claude-3-opus', // Use most capable model for complex reasoning
          prompt: `${prompt}

Please provide:
1. A thorough analysis
2. Your reasoning process
3. Specific recommendations

Format your response clearly with sections.`,
          config: {
            temperature: 0.2, // Lower temperature for analytical tasks
            maxTokens: 2000,
          },
        });

        // Parse the structured response
        const sections = response.text.split('\n\n');
        const analysis = sections[0] || '';
        const reasoning = sections[1] || '';
        const recommendations = sections.slice(2)
          .filter(section => section.includes('-'))
          .flatMap(section => 
            section.split('\n')
              .filter(line => line.trim().startsWith('-'))
              .map(line => line.replace(/^-\s*/, '').trim())
          );

        return { analysis, reasoning, recommendations };
      },
    );

    // Ethical reasoning flow
    export const ethicalAnalysisFlow = ai.defineFlow(
      {
        name: 'ethicalAnalysisFlow',
        inputSchema: z.object({ scenario: z.string() }),
        outputSchema: z.object({
          ethicalConsiderations: z.array(z.string()),
          stakeholders: z.array(z.string()),
          recommendations: z.string(),
        }),
      },
      async ({ scenario }) => {
        const response = await ai.generate({
          model: 'claude-3-sonnet',
          prompt: `Analyze the ethical implications of this scenario: ${scenario}

Please identify:
1. Key ethical considerations
2. Affected stakeholders
3. Recommended approach

Be thorough and consider multiple perspectives.`,
          config: {
            temperature: 0.3,
            maxTokens: 1500,
          },
        });

        // Extract structured information from response
        const lines = response.text.split('\n').filter(line => line.trim());
        const ethicalConsiderations = lines
          .filter(line => line.includes('ethical') || line.includes('moral'))
          .slice(0, 5);
        const stakeholders = lines
          .filter(line => line.includes('stakeholder') || line.includes('affected'))
          .slice(0, 5);
        const recommendations = lines
          .filter(line => line.includes('recommend') || line.includes('suggest'))
          .join(' ');

        return { ethicalConsiderations, stakeholders, recommendations };
      },
    );
    ```
</LanguageContent>

<LanguageContent lang="go">
Leverage Claude's reasoning capabilities:

    ```go
    // Complex analysis function
    func performComplexAnalysis(ctx context.Context, problem, context string) (map[string]interface{}, error) {
        prompt := problem
        if context != "" {
            prompt = fmt.Sprintf("Given this context: %s\n\nAnalyze this problem step by step: %s", context, problem)
        }

        fullPrompt := fmt.Sprintf(`%s

Please provide:
1. A thorough analysis
2. Your reasoning process
3. Specific recommendations

Format your response clearly with sections.`, prompt)

        model := claude.Model(g, "claude-3-opus-20240229") // Most capable model
        resp, err := genkit.Generate(ctx, g,
            ai.WithModel(model),
            ai.WithPrompt(fullPrompt),
            ai.WithConfig(map[string]interface{}{
                "temperature": 0.2,
                "max_tokens":  2000,
            }),
        )
        if err != nil {
            return nil, fmt.Errorf("analysis failed: %w", err)
        }

        // Parse structured response
        sections := strings.Split(resp.Text(), "\n\n")
        analysis := ""
        reasoning := ""
        var recommendations []string

        if len(sections) > 0 {
            analysis = sections[0]
        }
        if len(sections) > 1 {
            reasoning = sections[1]
        }
        if len(sections) > 2 {
            for _, section := range sections[2:] {
                lines := strings.Split(section, "\n")
                for _, line := range lines {
                    if strings.HasPrefix(strings.TrimSpace(line), "-") {
                        recommendations = append(recommendations, strings.TrimSpace(strings.TrimPrefix(line, "-")))
                    }
                }
            }
        }

        return map[string]interface{}{
            "analysis":        analysis,
            "reasoning":       reasoning,
            "recommendations": recommendations,
        }, nil
    }

    // Ethical analysis function
    func performEthicalAnalysis(ctx context.Context, scenario string) (map[string]interface{}, error) {
        prompt := fmt.Sprintf(`Analyze the ethical implications of this scenario: %s

Please identify:
1. Key ethical considerations
2. Affected stakeholders
3. Recommended approach

Be thorough and consider multiple perspectives.`, scenario)

        model := claude.Model(g, "claude-3-7-sonnet-20250219")
        resp, err := genkit.Generate(ctx, g,
            ai.WithModel(model),
            ai.WithPrompt(prompt),
            ai.WithConfig(map[string]interface{}{
                "temperature": 0.3,
                "max_tokens":  1500,
            }),
        )
        if err != nil {
            return nil, fmt.Errorf("ethical analysis failed: %w", err)
        }

        // Extract structured information
        lines := strings.Split(resp.Text(), "\n")
        var ethicalConsiderations, stakeholders []string
        var recommendations string

        for _, line := range lines {
            line = strings.TrimSpace(line)
            if strings.Contains(strings.ToLower(line), "ethical") || strings.Contains(strings.ToLower(line), "moral") {
                ethicalConsiderations = append(ethicalConsiderations, line)
            }
            if strings.Contains(strings.ToLower(line), "stakeholder") || strings.Contains(strings.ToLower(line), "affected") {
                stakeholders = append(stakeholders, line)
            }
            if strings.Contains(strings.ToLower(line), "recommend") || strings.Contains(strings.ToLower(line), "suggest") {
                recommendations += line + " "
            }
        }

        return map[string]interface{}{
            "ethicalConsiderations": ethicalConsiderations[:min(len(ethicalConsiderations), 5)],
            "stakeholders":          stakeholders[:min(len(stakeholders), 5)],
            "recommendations":       strings.TrimSpace(recommendations),
        }, nil
    }
    ```
</LanguageContent>

<LanguageContent lang="python">
Leverage Claude's reasoning capabilities:

    ```python
    from typing import List, Dict, Any, Optional

    # Complex analysis function
    async def perform_complex_analysis(
        problem: str, 
        context: Optional[str] = None
    ) -> Dict[str, Any]:
        prompt = problem
        if context:
            prompt = f"Given this context: {context}\n\nAnalyze this problem step by step: {problem}"

        full_prompt = f"""{prompt}

Please provide:
1. A thorough analysis
2. Your reasoning process
3. Specific recommendations

Format your response clearly with sections."""

        try:
            response = await ai.generate(
                model="claude-3-opus",  # Most capable model
                prompt=full_prompt,
                config={
                    "temperature": 0.2,
                    "max_tokens": 2000,
                }
            )

            # Parse structured response
            sections = response.text.split('\n\n')
            analysis = sections[0] if sections else ""
            reasoning = sections[1] if len(sections) > 1 else ""
            
            recommendations = []
            for section in sections[2:]:
                lines = section.split('\n')
                for line in lines:
                    if line.strip().startswith('-'):
                        recommendations.append(line.replace('-', '').strip())

            return {
                "analysis": analysis,
                "reasoning": reasoning,
                "recommendations": recommendations,
            }
        except Exception as error:
            print(f"Analysis failed: {error}")
            return {"analysis": "", "reasoning": "", "recommendations": []}

    # Ethical analysis function
    async def perform_ethical_analysis(scenario: str) -> Dict[str, Any]:
        prompt = f"""Analyze the ethical implications of this scenario: {scenario}

Please identify:
1. Key ethical considerations
2. Affected stakeholders
3. Recommended approach

Be thorough and consider multiple perspectives."""

        try:
            response = await ai.generate(
                model="claude-3-sonnet",
                prompt=prompt,
                config={
                    "temperature": 0.3,
                    "max_tokens": 1500,
                }
            )

            # Extract structured information
            lines = [line.strip() for line in response.text.split('\n') if line.strip()]
            
            ethical_considerations = [
                line for line in lines 
                if 'ethical' in line.lower() or 'moral' in line.lower()
            ][:5]
            
            stakeholders = [
                line for line in lines 
                if 'stakeholder' in line.lower() or 'affected' in line.lower()
            ][:5]
            
            recommendations = ' '.join([
                line for line in lines 
                if 'recommend' in line.lower() or 'suggest' in line.lower()
            ])

            return {
                "ethical_considerations": ethical_considerations,
                "stakeholders": stakeholders,
                "recommendations": recommendations,
            }
        except Exception as error:
            print(f"Ethical analysis failed: {error}")
            return {"ethical_considerations": [], "stakeholders": [], "recommendations": ""}
    ```
</LanguageContent>

### Conversational AI

<LanguageContent lang="js">
Build sophisticated conversational applications:

    ```ts
    // Advanced conversational flow
    export const claudeConversationFlow = ai.defineFlow(
      {
        name: 'claudeConversationFlow',
        inputSchema: z.object({
          message: z.string(),
          history: z.array(z.object({
            role: z.enum(['user', 'assistant']),
            content: z.string(),
          })).optional(),
          personality: z.enum(['analytical', 'creative', 'supportive', 'professional']).optional(),
        }),
        outputSchema: z.object({ response: z.string() }),
      },
      async ({ message, history = [], personality = 'analytical' }) => {
        const personalityPrompts = {
          analytical: 'You are a thoughtful analyst who provides detailed, logical responses.',
          creative: 'You are a creative thinker who offers imaginative and innovative perspectives.',
          supportive: 'You are a supportive companion who provides encouragement and understanding.',
          professional: 'You are a professional consultant who gives clear, actionable advice.',
        };

        const messages = [
          { role: 'system', content: personalityPrompts[personality] },
          ...history,
          { role: 'user', content: message },
        ];

        const response = await ai.generate({
          model: 'claude-3-sonnet',
          messages,
          config: {
            temperature: personality === 'creative' ? 0.8 : 0.6,
            maxTokens: 1000,
          },
        });

        return { response: response.text };
      },
    );
    ```
</LanguageContent>

<LanguageContent lang="go">
Build sophisticated conversational applications:

    ```go
    type ChatMessage struct {
        Role    string `json:"role"`
        Content string `json:"content"`
    }

    func handleClaudeConversation(ctx context.Context, message string, history []ChatMessage, personality string) (string, error) {
        personalityPrompts := map[string]string{
            "analytical":   "You are a thoughtful analyst who provides detailed, logical responses.",
            "creative":     "You are a creative thinker who offers imaginative and innovative perspectives.",
            "supportive":   "You are a supportive companion who provides encouragement and understanding.",
            "professional": "You are a professional consultant who gives clear, actionable advice.",
        }

        systemPrompt, exists := personalityPrompts[personality]
        if !exists {
            systemPrompt = personalityPrompts["analytical"]
        }

        messages := []ChatMessage{
            {Role: "system", Content: systemPrompt},
        }
        messages = append(messages, history...)
        messages = append(messages, ChatMessage{Role: "user", Content: message})

        temperature := 0.6
        if personality == "creative" {
            temperature = 0.8
        }

        model := claude.Model(g, "claude-3-7-sonnet-20250219")
        resp, err := genkit.Generate(ctx, g,
            ai.WithModel(model),
            ai.WithMessages(messages),
            ai.WithConfig(map[string]interface{}{
                "temperature": temperature,
                "max_tokens":  1000,
            }),
        )
        if err != nil {
            return "", fmt.Errorf("conversation failed: %w", err)
        }

        return resp.Text(), nil
    }
    ```
</LanguageContent>

<LanguageContent lang="python">
Build sophisticated conversational applications:

    ```python
    from typing import List, Dict, Optional

    async def handle_claude_conversation(
        message: str,
        history: List[Dict[str, str]] = None,
        personality: str = "analytical"
    ) -> str:
        if history is None:
            history = []

        personality_prompts = {
            "analytical": "You are a thoughtful analyst who provides detailed, logical responses.",
            "creative": "You are a creative thinker who offers imaginative and innovative perspectives.",
            "supportive": "You are a supportive companion who provides encouragement and understanding.",
            "professional": "You are a professional consultant who gives clear, actionable advice.",
        }

        system_prompt = personality_prompts.get(personality, personality_prompts["analytical"])

        messages = [
            {"role": "system", "content": system_prompt},
            *history,
            {"role": "user", "content": message},
        ]

        temperature = 0.8 if personality == "creative" else 0.6

        try:
            response = await ai.generate(
                model="claude-3-sonnet",
                messages=messages,
                config={
                    "temperature": temperature,
                    "max_tokens": 1000,
                }
            )
            return response.text
        except Exception as error:
            print(f"Conversation failed: {error}")
            return "I'm sorry, I couldn't process your message at the moment."
    ```
</LanguageContent>

## Model Comparison

### Available Models

| Model | Capabilities | Best For | Context Window |
|-------|-------------|----------|----------------|
| **Claude 3 Haiku** | Fast, efficient | Simple tasks, quick responses | 200K tokens |
| **Claude 3 Sonnet** | Balanced performance | General-purpose tasks, analysis | 200K tokens |
| **Claude 3 Opus** | Most capable | Complex reasoning, research | 200K tokens |
| **Claude 3.5 Sonnet** | Enhanced reasoning | Advanced analysis, coding | 200K tokens |
| **Claude 3.7 Sonnet** | Latest capabilities | Cutting-edge reasoning tasks | 200K tokens |

## Best Practices

### Optimizing for Different Tasks

1. **Analysis and reasoning**: Use Claude 3 Opus or 3.7 Sonnet with low temperature (0.2-0.3)
2. **Creative writing**: Use Claude 3.5 Sonnet with higher temperature (0.7-0.8)
3. **Quick responses**: Use Claude 3 Haiku for speed
4. **Ethical considerations**: Claude models excel at nuanced ethical reasoning

### Prompt Engineering

1. **Be specific**: Claude responds well to detailed, structured prompts
2. **Use examples**: Provide examples of desired output format
3. **Request reasoning**: Ask Claude to explain its thinking process
4. **Set context**: Provide relevant background information

### Safety and Alignment

1. **Built-in safety**: Claude has strong safety guardrails
2. **Helpful responses**: Models are trained to be helpful, harmless, and honest
3. **Nuanced understanding**: Excellent at understanding context and intent
4. **Ethical reasoning**: Strong capability for ethical analysis and decision-making

## Next Steps

- Learn about [generating content](/unified-docs/models) to understand how to use these models effectively
- Explore [tool calling](/unified-docs/tool-calling) to add interactive capabilities (Note: Tool calling may have limitations with Claude models)
- See [creating flows](/unified-docs/flows) to build structured AI workflows with advanced reasoning
- Check out [deployment guides](/unified-docs/deployment) for production deployment strategies
