---
title: OpenAI plugin
description: Learn how to use OpenAI models with Genkit across JavaScript, Go, and Python, including GPT models, DALL-E image generation, Whisper transcription, and text-to-speech capabilities.
---

import { Tabs, TabItem } from '@astrojs/starlight/components';

The OpenAI plugin provides access to OpenAI's powerful AI models, including GPT for text generation, DALL-E for image generation, Whisper for speech transcription, and text-to-speech models.

## Installation and Setup

<Tabs syncKey="language">
  <TabItem label="JavaScript" icon="seti:javascript">
    Install the OpenAI plugin:

    ```bash
    npm install @genkit-ai/compat-oai
    ```

    Configure the plugin when initializing Genkit:

    ```ts
    import { genkit } from 'genkit';
    import { openAI } from '@genkit-ai/compat-oai/openai';

    const ai = genkit({
      plugins: [openAI()],
    });
    ```

    :::note
    The OpenAI plugin is built on top of the `openAICompatible` plugin and is pre-configured for OpenAI's API endpoints.
    :::
  </TabItem>
  <TabItem label="Go" icon="seti:go">
    The OpenAI plugin is available through the OpenAI-compatible plugin:

    ```go
    import (
        "context"
        "github.com/firebase/genkit/go/genkit"
        "github.com/firebase/genkit/go/plugins/openai"
    )

    func main() {
        ctx := context.Background()
        g, err := genkit.Init(ctx,
            genkit.WithPlugins(&openai.OpenAI{
                APIKey: "your-api-key", // or use environment variable
            }),
        )
        if err != nil {
            log.Fatal(err)
        }
    }
    ```
  </TabItem>
  <TabItem label="Python" icon="seti:python">
    Install the OpenAI plugin:

    ```bash
    pip install genkit-plugin-openai
    ```

    Configure the plugin when initializing Genkit:

    ```python
    from genkit.ai import Genkit
    from genkit.plugins.openai import OpenAI

    ai = Genkit(
        plugins=[OpenAI()],
    )
    ```
  </TabItem>
</Tabs>

## API Key Configuration

The plugin requires an API key for the OpenAI API, which you can get from the [OpenAI Platform](https://platform.openai.com/api-keys).

<Tabs syncKey="language">
  <TabItem label="JavaScript" icon="seti:javascript">
    Configure your API key by doing one of the following:

    - Set the `OPENAI_API_KEY` environment variable:
      ```bash
      export OPENAI_API_KEY=your_api_key_here
      ```

    - Specify the API key when initializing the plugin:
      ```ts
      openAI({ apiKey: yourKey });
      ```

    :::caution
    Don't embed your API key directly in code! Use environment variables or a service like Google Cloud Secret Manager.
    :::
  </TabItem>
  <TabItem label="Go" icon="seti:go">
    Set the `OPENAI_API_KEY` environment variable:

    ```bash
    export OPENAI_API_KEY=your_api_key_here
    ```

    Or specify it in the plugin configuration:

    ```go
    g, err := genkit.Init(ctx,
        genkit.WithPlugins(&openai.OpenAI{
            APIKey: os.Getenv("OPENAI_API_KEY"),
        }),
    )
    ```
  </TabItem>
  <TabItem label="Python" icon="seti:python">
    Set the `OPENAI_API_KEY` environment variable:

    ```bash
    export OPENAI_API_KEY=your_api_key_here
    ```

    The plugin will automatically use this environment variable.
  </TabItem>
</Tabs>

## Text Generation

<Tabs syncKey="language">
  <TabItem label="JavaScript" icon="seti:javascript">
    Use OpenAI's GPT models for text generation:

    ```ts
    import { openAI } from '@genkit-ai/compat-oai/openai';

    const ai = genkit({
      plugins: [openAI()],
    });

    // Basic text generation
    const llmResponse = await ai.generate({
      prompt: 'Tell me a joke about programming',
      model: openAI.model('gpt-4o'),
    });

    // With configuration
    const configuredResponse = await ai.generate({
      prompt: 'Write a creative story about AI',
      model: openAI.model('gpt-4o'),
      config: {
        temperature: 0.7,
        maxTokens: 1000,
        topP: 0.9,
      },
    });

    // Using in a flow
    export const jokeFlow = ai.defineFlow(
      {
        name: 'jokeFlow',
        inputSchema: z.object({ subject: z.string() }),
        outputSchema: z.object({ joke: z.string() }),
      },
      async ({ subject }) => {
        const llmResponse = await ai.generate({
          prompt: `tell me a joke about ${subject}`,
          model: openAI.model('gpt-4o'),
        });
        return { joke: llmResponse.text };
      },
    );
    ```
  </TabItem>
  <TabItem label="Go" icon="seti:go">
    Use OpenAI models with the generation API:

    ```go
    import (
        "context"
        "github.com/firebase/genkit/go/ai"
        "github.com/firebase/genkit/go/genkit"
        "github.com/firebase/genkit/go/plugins/openai"
    )

    func main() {
        ctx := context.Background()
        g, err := genkit.Init(ctx,
            genkit.WithPlugins(&openai.OpenAI{}),
            genkit.WithDefaultModel("openai/gpt-4o"),
        )
        if err != nil {
            log.Fatal(err)
        }

        // Generate content
        resp, err := genkit.Generate(ctx, g,
            ai.WithPrompt("Tell me a joke about programming"),
        )
        if err != nil {
            log.Fatal(err)
        }

        fmt.Println(resp.Text())
    }
    ```
  </TabItem>
  <TabItem label="Python" icon="seti:python">
    Use OpenAI models with the generation API:

    ```python
    from genkit.ai import Genkit
    from genkit.plugins.openai import OpenAI, openai_name

    ai = Genkit(
        plugins=[OpenAI()],
        model=openai_name('gpt-4o'),
    )

    # Generate content
    response = await ai.generate('Tell me a joke about programming')
    print(response.text)

    # With configuration
    response = await ai.generate(
        prompt='Write a creative story about AI',
        model=openai_name('gpt-4o'),
        config={
            'temperature': 0.7,
            'max_tokens': 1000,
            'top_p': 0.9,
        }
    )
    ```
  </TabItem>
</Tabs>

## Image Generation

<Tabs syncKey="language">
  <TabItem label="JavaScript" icon="seti:javascript">
    Generate images using DALL-E models:

    ```ts
    // Basic image generation
    const imageResponse = await ai.generate({
      model: openAI.model('dall-e-3'),
      prompt: 'A photorealistic image of a cat programming a computer.',
      config: {
        size: '1024x1024',
        style: 'vivid',
        quality: 'hd',
      },
    });

    const imageUrl = imageResponse.media()?.url;

    // DALL-E 2 for faster generation
    const quickImage = await ai.generate({
      model: openAI.model('dall-e-2'),
      prompt: 'A simple cartoon of a robot',
      config: {
        size: '512x512',
        n: 2, // Generate 2 variations
      },
    });
    ```
  </TabItem>
  <TabItem label="Go" icon="seti:go">
    Image generation requires custom implementation using the OpenAI API:

    ```go
    // Image generation requires custom implementation
    // Use the OpenAI Go SDK directly for DALL-E functionality
    ```
  </TabItem>
  <TabItem label="Python" icon="seti:python">
    Image generation requires custom implementation using the OpenAI API:

    ```python
    # Image generation requires custom implementation
    # Use the OpenAI Python SDK directly for DALL-E functionality
    ```
  </TabItem>
</Tabs>

## Text Embeddings

<Tabs syncKey="language">
  <TabItem label="JavaScript" icon="seti:javascript">
    Generate text embeddings for vector search and similarity:

    ```ts
    // Generate embeddings
    const embedding = await ai.embed({
      embedder: openAI.embedder('text-embedding-ada-002'),
      content: 'This is a sample text for embedding',
    });

    // Using in a flow
    export const embedFlow = ai.defineFlow(
      {
        name: 'embedFlow',
        inputSchema: z.object({ text: z.string() }),
        outputSchema: z.object({ embedding: z.string() }),
      },
      async ({ text }) => {
        const embedding = await ai.embed({
          embedder: openAI.embedder('text-embedding-ada-002'),
          content: text,
        });

        return { embedding: JSON.stringify(embedding) };
      },
    );

    // Use with vector databases
    const ai = genkit({
      plugins: [
        openAI(),
        chroma([
          {
            embedder: openAI.embedder('text-embedding-ada-002'),
            collectionName: 'my-collection',
          },
        ]),
      ],
    });
    ```
  </TabItem>
  <TabItem label="Go" icon="seti:go">
    Generate embeddings using OpenAI models:

    ```go
    // Generate embeddings
    embeddings, err := genkit.Embed(ctx, g,
        ai.WithEmbedder("openai/text-embedding-ada-002"),
        ai.WithEmbedContent("This is a sample text for embedding"),
    )
    if err != nil {
        log.Fatal(err)
    }

    fmt.Printf("Generated %d-dimensional embedding\n", len(embeddings))
    ```
  </TabItem>
  <TabItem label="Python" icon="seti:python">
    Generate embeddings using OpenAI models:

    ```python
    # Generate embeddings
    embeddings = await ai.embed(
        embedder=openai_name('text-embedding-ada-002'),
        content='This is a sample text for embedding',
    )

    print(f"Generated {len(embeddings)}-dimensional embedding")
    ```
  </TabItem>
</Tabs>

## Audio Processing

### Speech-to-Text (Whisper)

<Tabs syncKey="language">
  <TabItem label="JavaScript" icon="seti:javascript">
    Transcribe audio files using Whisper:

    ```ts
    import * as fs from 'fs';

    const whisper = openAI.model('whisper-1');
    const audioFile = fs.readFileSync('path/to/your/audio.mp3');

    const transcription = await ai.generate({
      model: whisper,
      prompt: [
        {
          media: {
            contentType: 'audio/mp3',
            url: `data:audio/mp3;base64,${audioFile.toString('base64')}`,
          },
        },
      ],
      config: {
        language: 'en', // Optional: specify language
        temperature: 0, // For more deterministic results
      },
    });

    console.log('Transcription:', transcription.text());

    // With additional context
    const contextualTranscription = await ai.generate({
      model: whisper,
      prompt: [
        { text: 'This is a recording of a technical meeting about AI.' },
        {
          media: {
            contentType: 'audio/wav',
            url: `data:audio/wav;base64,${audioFile.toString('base64')}`,
          },
        },
      ],
    });
    ```
  </TabItem>
  <TabItem label="Go" icon="seti:go">
    Audio transcription requires custom implementation using the OpenAI API:

    ```go
    // Audio transcription requires custom implementation
    // Use the OpenAI Go SDK directly for Whisper functionality
    ```
  </TabItem>
  <TabItem label="Python" icon="seti:python">
    Audio transcription requires custom implementation using the OpenAI API:

    ```python
    # Audio transcription requires custom implementation
    # Use the OpenAI Python SDK directly for Whisper functionality
    ```
  </TabItem>
</Tabs>

### Text-to-Speech

<Tabs syncKey="language">
  <TabItem label="JavaScript" icon="seti:javascript">
    Generate speech from text:

    ```ts
    import * as fs from 'fs';

    const tts = openAI.model('tts-1');
    const speechResponse = await ai.generate({
      model: tts,
      prompt: 'Hello, world! This is a test of text-to-speech.',
      config: {
        voice: 'alloy', // Options: alloy, echo, fable, onyx, nova, shimmer
        response_format: 'mp3', // Options: mp3, opus, aac, flac
        speed: 1.0, // 0.25 to 4.0
      },
    });

    const audioData = speechResponse.media();
    if (audioData) {
      fs.writeFileSync('output.mp3', Buffer.from(audioData.url.split(',')[1], 'base64'));
    }

    // High-quality TTS
    const hqSpeech = await ai.generate({
      model: openAI.model('tts-1-hd'),
      prompt: 'This is high-quality text-to-speech.',
      config: {
        voice: 'nova',
        response_format: 'wav',
      },
    });
    ```
  </TabItem>
  <TabItem label="Go" icon="seti:go">
    Text-to-speech requires custom implementation using the OpenAI API:

    ```go
    // Text-to-speech requires custom implementation
    // Use the OpenAI Go SDK directly for TTS functionality
    ```
  </TabItem>
  <TabItem label="Python" icon="seti:python">
    Text-to-speech requires custom implementation using the OpenAI API:

    ```python
    # Text-to-speech requires custom implementation
    # Use the OpenAI Python SDK directly for TTS functionality
    ```
  </TabItem>
</Tabs>

## Advanced Features

### Web Search Integration

<Tabs syncKey="language">
  <TabItem label="JavaScript" icon="seti:javascript">
    Some OpenAI models support web search capabilities:

    ```ts
    const llmResponse = await ai.generate({
      prompt: 'What was a positive news story from today?',
      model: openAI.model('gpt-4o-search-preview'),
      config: {
        web_search_options: {
          max_results: 5,
        },
      },
    });
    ```
  </TabItem>
  <TabItem label="Go" icon="seti:go">
    Web search integration requires custom implementation:

    ```go
    // Web search requires custom implementation
    // Use the OpenAI API directly for search-enabled models
    ```
  </TabItem>
  <TabItem label="Python" icon="seti:python">
    Web search integration requires custom implementation:

    ```python
    # Web search requires custom implementation
    # Use the OpenAI API directly for search-enabled models
    ```
  </TabItem>
</Tabs>

### Function Calling

<Tabs syncKey="language">
  <TabItem label="JavaScript" icon="seti:javascript">
    OpenAI models support function calling for tool integration:

    ```ts
    const weatherTool = ai.defineTool({
      name: 'getWeather',
      description: 'Get the current weather for a location',
      inputSchema: z.object({
        location: z.string().describe('The city and state'),
      }),
      outputSchema: z.object({
        temperature: z.number(),
        condition: z.string(),
      }),
    }, async ({ location }) => {
      // Implementation here
      return { temperature: 72, condition: 'sunny' };
    });

    const response = await ai.generate({
      prompt: 'What\'s the weather like in San Francisco?',
      model: openAI.model('gpt-4o'),
      tools: [weatherTool],
    });
    ```
  </TabItem>
  <TabItem label="Go" icon="seti:go">
    Function calling is supported through Genkit's tool system:

    ```go
    // Define tools and use with OpenAI models
    // See tool calling documentation for implementation details
    ```
  </TabItem>
  <TabItem label="Python" icon="seti:python">
    Function calling is supported through Genkit's tool system:

    ```python
    # Define tools and use with OpenAI models
    # See tool calling documentation for implementation details
    ```
  </TabItem>
</Tabs>

### Passthrough Configuration

<Tabs syncKey="language">
  <TabItem label="JavaScript" icon="seti:javascript">
    Access new models and features without updating Genkit:

    ```ts
    const llmResponse = await ai.generate({
      prompt: 'Tell me a cool story',
      model: openAI.model('gpt-4-new'), // hypothetical new model
      config: {
        seed: 123,
        new_feature_parameter: 'value', // hypothetical config for new model
        logprobs: true,
        top_logprobs: 5,
      },
    });
    ```

    Genkit passes this config as-is to the OpenAI API, giving you access to new model features.
  </TabItem>
  <TabItem label="Go" icon="seti:go">
    Passthrough configuration allows access to new OpenAI features:

    ```go
    // Custom configuration can be passed through to the OpenAI API
    // See OpenAI Go SDK documentation for available options
    ```
  </TabItem>
  <TabItem label="Python" icon="seti:python">
    Passthrough configuration allows access to new OpenAI features:

    ```python
    # Custom configuration can be passed through to the OpenAI API
    # See OpenAI Python SDK documentation for available options
    ```
  </TabItem>
</Tabs>

## Available Models

### Text Generation
- **GPT-4 family**: `gpt-4o`, `gpt-4o-mini`, `gpt-4-turbo`, `gpt-4`
- **GPT-3.5**: `gpt-3.5-turbo`
- **Search-enabled**: `gpt-4o-search-preview`

### Image Generation
- **DALL-E 3**: `dall-e-3` (high quality, 1024x1024, 1024x1792, 1792x1024)
- **DALL-E 2**: `dall-e-2` (faster, 256x256, 512x512, 1024x1024)

### Embeddings
- **Text embeddings**: `text-embedding-ada-002`, `text-embedding-3-small`, `text-embedding-3-large`

### Audio
- **Speech-to-text**: `whisper-1`
- **Text-to-speech**: `tts-1`, `tts-1-hd`

## Configuration Options

### Text Generation
- `temperature`: Randomness (0-2)
- `max_tokens`: Maximum response length
- `top_p`: Nucleus sampling (0-1)
- `frequency_penalty`: Reduce repetition (-2 to 2)
- `presence_penalty`: Encourage new topics (-2 to 2)
- `seed`: Deterministic outputs
- `logprobs`: Return log probabilities

### Image Generation
- `size`: Image dimensions
- `style`: `vivid` or `natural`
- `quality`: `standard` or `hd`
- `n`: Number of images (1-10 for DALL-E 2)

### Audio
- `voice`: TTS voice selection
- `response_format`: Audio format
- `speed`: Speech rate (0.25-4.0)
- `language`: Whisper language hint
- `temperature`: Whisper randomness

## Next Steps

- Learn about [generating content](/unified-docs/generating-content) to understand how to use these models effectively
- Explore [tool calling](/unified-docs/tool-calling) to integrate OpenAI's function calling capabilities
- See [creating flows](/unified-docs/creating-flows) to build structured AI workflows
- Check out [RAG](/unified-docs/rag) to implement retrieval-augmented generation with OpenAI embeddings
