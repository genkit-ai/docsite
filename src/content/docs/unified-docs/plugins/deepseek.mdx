---
title: DeepSeek Plugin
description: Learn how to use DeepSeek's advanced AI models with Genkit across JavaScript, Go, and Python, including reasoning models, code generation, and cost-effective solutions.
---

import { Tabs, TabItem } from '@astrojs/starlight/components';

The DeepSeek plugin provides access to DeepSeek's powerful AI models, including their advanced reasoning models and cost-effective solutions. DeepSeek models are known for their strong performance in coding, mathematics, and reasoning tasks.

## Installation and Setup

<Tabs syncKey="language">
  <TabItem label="JavaScript" icon="seti:javascript">
    Install the DeepSeek plugin:

    ```bash
    npm install @genkit-ai/compat-oai
    ```

    Configure the plugin when initializing Genkit:

    ```ts
    import { genkit } from 'genkit';
    import { deepSeek } from '@genkit-ai/compat-oai/deepseek';

    const ai = genkit({
      plugins: [deepSeek()],
    });
    ```

    ### API Key Configuration

    Set your DeepSeek API key using one of these methods:

    ```bash
    # Environment variable (recommended)
    export DEEPSEEK_API_KEY=your_deepseek_api_key
    ```

    ```ts
    // Or pass directly to plugin (not recommended for production)
    const ai = genkit({
      plugins: [deepSeek({ apiKey: 'your_deepseek_api_key' })],
    });
    ```

    Get your API key from [DeepSeek Platform](https://platform.deepseek.com/).
  </TabItem>
  <TabItem label="Go" icon="seti:go">
    For Go applications, use the OpenAI-compatible client with DeepSeek endpoints:

    ```go
    package main

    import (
        "context"
        "github.com/firebase/genkit/go/genkit"
        "github.com/firebase/genkit/go/plugins/openai"
    )

    func main() {
        ctx := context.Background()
        g, err := genkit.Init(ctx,
            genkit.WithPlugins(&openai.OpenAI{
                APIKey:  os.Getenv("DEEPSEEK_API_KEY"),
                BaseURL: "https://api.deepseek.com/v1",
                Models: []openai.ModelConfig{
                    {Name: "deepseek-chat", Type: "chat"},
                    {Name: "deepseek-coder", Type: "chat"},
                    {Name: "deepseek-reasoner", Type: "chat"},
                },
            }),
        )
        if err != nil {
            log.Fatal(err)
        }
    }
    ```

    ### Environment Configuration

    ```bash
    export DEEPSEEK_API_KEY=your_deepseek_api_key
    ```
  </TabItem>
  <TabItem label="Python" icon="seti:python">
    For Python applications, use the OpenAI-compatible client:

    ```bash
    pip install genkit-plugin-openai
    ```

    ```python
    from genkit.ai import Genkit
    from genkit.plugins.openai import OpenAI

    ai = Genkit(
        plugins=[OpenAI(
            api_key=os.getenv("DEEPSEEK_API_KEY"),
            base_url="https://api.deepseek.com/v1",
            models=[
                {"name": "deepseek-chat", "type": "chat"},
                {"name": "deepseek-coder", "type": "chat"},
                {"name": "deepseek-reasoner", "type": "chat"},
            ],
        )],
    )
    ```

    ### Environment Configuration

    ```bash
    export DEEPSEEK_API_KEY=your_deepseek_api_key
    ```
  </TabItem>
</Tabs>

## Basic Usage

### Text Generation

<Tabs syncKey="language">
  <TabItem label="JavaScript" icon="seti:javascript">
    Use DeepSeek models for text generation:

    ```ts
    import { genkit, z } from 'genkit';
    import { deepSeek } from '@genkit-ai/compat-oai/deepseek';

    const ai = genkit({
      plugins: [deepSeek()],
    });

    // Basic text generation
    const response = await ai.generate({
      model: deepSeek.model('deepseek-chat'),
      prompt: 'Explain the concept of machine learning',
    });

    console.log(response.text);

    // Flow with DeepSeek
    export const deepseekFlow = ai.defineFlow(
      {
        name: 'deepseekFlow',
        inputSchema: z.object({ subject: z.string() }),
        outputSchema: z.object({ information: z.string() }),
      },
      async ({ subject }) => {
        const llmResponse = await ai.generate({
          model: deepSeek.model('deepseek-chat'),
          prompt: `Tell me something about ${subject}.`,
        });
        return { information: llmResponse.text };
      },
    );

    // Advanced reasoning tasks
    const reasoningResponse = await ai.generate({
      model: deepSeek.model('deepseek-reasoner'),
      prompt: 'Solve this step by step: If a train travels 120 km in 2 hours, and then 180 km in 3 hours, what is the average speed for the entire journey?',
      config: {
        temperature: 0.1, // Lower temperature for reasoning tasks
        maxTokens: 1000,
      },
    });
    ```
  </TabItem>
  <TabItem label="Go" icon="seti:go">
    Use DeepSeek models with the generation API:

    ```go
    import (
        "context"
        "github.com/firebase/genkit/go/ai"
        "github.com/firebase/genkit/go/genkit"
    )

    func main() {
        ctx := context.Background()
        
        // Basic text generation
        resp, err := genkit.Generate(ctx, g,
            ai.WithModel("deepseek-chat"),
            ai.WithPrompt("Explain the concept of machine learning"),
        )
        if err != nil {
            log.Fatal(err)
        }

        fmt.Println(resp.Text())

        // Advanced reasoning tasks
        reasoningResp, err := genkit.Generate(ctx, g,
            ai.WithModel("deepseek-reasoner"),
            ai.WithPrompt("Solve this step by step: If a train travels 120 km in 2 hours, and then 180 km in 3 hours, what is the average speed for the entire journey?"),
            ai.WithConfig(map[string]interface{}{
                "temperature": 0.1,
                "max_tokens":  1000,
            }),
        )
        if err != nil {
            log.Fatal(err)
        }

        fmt.Println(reasoningResp.Text())
    }
    ```
  </TabItem>
  <TabItem label="Python" icon="seti:python">
    Use DeepSeek models with the generation API:

    ```python
    from genkit.ai import Genkit
    from genkit.plugins.openai import OpenAI, openai_name

    ai = Genkit(
        plugins=[OpenAI(
            api_key=os.getenv("DEEPSEEK_API_KEY"),
            base_url="https://api.deepseek.com/v1",
            models=[
                {"name": "deepseek-chat", "type": "chat"},
                {"name": "deepseek-reasoner", "type": "chat"},
            ],
        )],
    )

    # Basic text generation
    response = await ai.generate(
        model=openai_name('deepseek-chat'),
        prompt='Explain the concept of machine learning'
    )
    print(response.text)

    # Advanced reasoning tasks
    reasoning_response = await ai.generate(
        model=openai_name('deepseek-reasoner'),
        prompt='Solve this step by step: If a train travels 120 km in 2 hours, and then 180 km in 3 hours, what is the average speed for the entire journey?',
        config={
            'temperature': 0.1,
            'max_tokens': 1000,
        }
    )
    print(reasoning_response.text)
    ```
  </TabItem>
</Tabs>

### Code Generation

<Tabs syncKey="language">
  <TabItem label="JavaScript" icon="seti:javascript">
    Use DeepSeek for code generation and programming tasks:

    ```ts
    // Code generation flow
    export const codeGenerationFlow = ai.defineFlow(
      {
        name: 'codeGenerationFlow',
        inputSchema: z.object({ 
          task: z.string(),
          language: z.string(),
        }),
        outputSchema: z.object({ code: z.string() }),
      },
      async ({ task, language }) => {
        const response = await ai.generate({
          model: deepSeek.model('deepseek-coder'),
          prompt: `Write ${language} code to ${task}. Include comments and error handling.`,
          config: {
            temperature: 0.2, // Lower temperature for code generation
            maxTokens: 2000,
          },
        });
        return { code: response.text };
      },
    );

    // Code review and optimization
    export const codeReviewFlow = ai.defineFlow(
      {
        name: 'codeReviewFlow',
        inputSchema: z.object({ code: z.string() }),
        outputSchema: z.object({ 
          review: z.string(),
          suggestions: z.array(z.string()),
        }),
      },
      async ({ code }) => {
        const response = await ai.generate({
          model: deepSeek.model('deepseek-coder'),
          prompt: `Review this code and provide suggestions for improvement:\n\n${code}`,
          config: {
            temperature: 0.3,
            maxTokens: 1500,
          },
        });

        // Parse the response to extract review and suggestions
        const lines = response.text.split('\n');
        const review = lines.slice(0, 5).join('\n');
        const suggestions = lines.slice(5).filter(line => line.trim().startsWith('-'));

        return { review, suggestions };
      },
    );
    ```
  </TabItem>
  <TabItem label="Go" icon="seti:go">
    Use DeepSeek for code generation and programming tasks:

    ```go
    // Code generation
    func generateCode(ctx context.Context, task, language string) (string, error) {
        prompt := fmt.Sprintf("Write %s code to %s. Include comments and error handling.", language, task)
        
        resp, err := genkit.Generate(ctx, g,
            ai.WithModel("deepseek-coder"),
            ai.WithPrompt(prompt),
            ai.WithConfig(map[string]interface{}{
                "temperature": 0.2,
                "max_tokens":  2000,
            }),
        )
        if err != nil {
            return "", err
        }
        
        return resp.Text(), nil
    }

    // Code review and optimization
    func reviewCode(ctx context.Context, code string) (string, error) {
        prompt := fmt.Sprintf("Review this code and provide suggestions for improvement:\n\n%s", code)
        
        resp, err := genkit.Generate(ctx, g,
            ai.WithModel("deepseek-coder"),
            ai.WithPrompt(prompt),
            ai.WithConfig(map[string]interface{}{
                "temperature": 0.3,
                "max_tokens":  1500,
            }),
        )
        if err != nil {
            return "", err
        }
        
        return resp.Text(), nil
    }
    ```
  </TabItem>
  <TabItem label="Python" icon="seti:python">
    Use DeepSeek for code generation and programming tasks:

    ```python
    # Code generation
    async def generate_code(task: str, language: str) -> str:
        prompt = f"Write {language} code to {task}. Include comments and error handling."
        
        response = await ai.generate(
            model=openai_name('deepseek-coder'),
            prompt=prompt,
            config={
                'temperature': 0.2,
                'max_tokens': 2000,
            }
        )
        return response.text

    # Code review and optimization
    async def review_code(code: str) -> dict:
        prompt = f"Review this code and provide suggestions for improvement:\n\n{code}"
        
        response = await ai.generate(
            model=openai_name('deepseek-coder'),
            prompt=prompt,
            config={
                'temperature': 0.3,
                'max_tokens': 1500,
            }
        )
        
        # Parse the response to extract review and suggestions
        lines = response.text.split('\n')
        review = '\n'.join(lines[:5])
        suggestions = [line for line in lines[5:] if line.strip().startswith('-')]
        
        return {'review': review, 'suggestions': suggestions}
    ```
  </TabItem>
</Tabs>

## Advanced Features

### Mathematical Reasoning

<Tabs syncKey="language">
  <TabItem label="JavaScript" icon="seti:javascript">
    Leverage DeepSeek's mathematical reasoning capabilities:

    ```ts
    // Mathematical problem solving
    export const mathSolverFlow = ai.defineFlow(
      {
        name: 'mathSolverFlow',
        inputSchema: z.object({ problem: z.string() }),
        outputSchema: z.object({ 
          solution: z.string(),
          steps: z.array(z.string()),
        }),
      },
      async ({ problem }) => {
        const response = await ai.generate({
          model: deepSeek.model('deepseek-reasoner'),
          prompt: `Solve this mathematical problem step by step: ${problem}`,
          config: {
            temperature: 0.1, // Very low temperature for mathematical accuracy
            maxTokens: 1500,
          },
        });

        // Parse the response to extract solution and steps
        const lines = response.text.split('\n').filter(line => line.trim());
        const solution = lines[lines.length - 1];
        const steps = lines.slice(0, -1);

        return { solution, steps };
      },
    );

    // Statistical analysis
    export const statisticsFlow = ai.defineFlow(
      {
        name: 'statisticsFlow',
        inputSchema: z.object({ 
          data: z.array(z.number()),
          analysisType: z.enum(['descriptive', 'inferential', 'regression']),
        }),
        outputSchema: z.object({ analysis: z.string() }),
      },
      async ({ data, analysisType }) => {
        const dataStr = data.join(', ');
        const prompt = `Perform ${analysisType} statistical analysis on this data: [${dataStr}]. Provide detailed calculations and interpretations.`;

        const response = await ai.generate({
          model: deepSeek.model('deepseek-reasoner'),
          prompt,
          config: {
            temperature: 0.2,
            maxTokens: 2000,
          },
        });

        return { analysis: response.text };
      },
    );
    ```
  </TabItem>
  <TabItem label="Go" icon="seti:go">
    Leverage DeepSeek's mathematical reasoning capabilities:

    ```go
    // Mathematical problem solving
    func solveMathProblem(ctx context.Context, problem string) (string, []string, error) {
        prompt := fmt.Sprintf("Solve this mathematical problem step by step: %s", problem)
        
        resp, err := genkit.Generate(ctx, g,
            ai.WithModel("deepseek-reasoner"),
            ai.WithPrompt(prompt),
            ai.WithConfig(map[string]interface{}{
                "temperature": 0.1,
                "max_tokens":  1500,
            }),
        )
        if err != nil {
            return "", nil, err
        }
        
        lines := strings.Split(resp.Text(), "\n")
        var steps []string
        var solution string
        
        for _, line := range lines {
            if strings.TrimSpace(line) != "" {
                steps = append(steps, line)
            }
        }
        
        if len(steps) > 0 {
            solution = steps[len(steps)-1]
            steps = steps[:len(steps)-1]
        }
        
        return solution, steps, nil
    }

    // Statistical analysis
    func performStatisticalAnalysis(ctx context.Context, data []float64, analysisType string) (string, error) {
        dataStr := make([]string, len(data))
        for i, v := range data {
            dataStr[i] = fmt.Sprintf("%.2f", v)
        }
        
        prompt := fmt.Sprintf("Perform %s statistical analysis on this data: [%s]. Provide detailed calculations and interpretations.",
            analysisType, strings.Join(dataStr, ", "))
        
        resp, err := genkit.Generate(ctx, g,
            ai.WithModel("deepseek-reasoner"),
            ai.WithPrompt(prompt),
            ai.WithConfig(map[string]interface{}{
                "temperature": 0.2,
                "max_tokens":  2000,
            }),
        )
        if err != nil {
            return "", err
        }
        
        return resp.Text(), nil
    }
    ```
  </TabItem>
  <TabItem label="Python" icon="seti:python">
    Leverage DeepSeek's mathematical reasoning capabilities:

    ```python
    from typing import List

    # Mathematical problem solving
    async def solve_math_problem(problem: str) -> dict:
        prompt = f"Solve this mathematical problem step by step: {problem}"
        
        response = await ai.generate(
            model=openai_name('deepseek-reasoner'),
            prompt=prompt,
            config={
                'temperature': 0.1,
                'max_tokens': 1500,
            }
        )
        
        lines = [line for line in response.text.split('\n') if line.strip()]
        solution = lines[-1] if lines else ""
        steps = lines[:-1] if len(lines) > 1 else []
        
        return {'solution': solution, 'steps': steps}

    # Statistical analysis
    async def perform_statistical_analysis(data: List[float], analysis_type: str) -> str:
        data_str = ', '.join([f"{x:.2f}" for x in data])
        prompt = f"Perform {analysis_type} statistical analysis on this data: [{data_str}]. Provide detailed calculations and interpretations."
        
        response = await ai.generate(
            model=openai_name('deepseek-reasoner'),
            prompt=prompt,
            config={
                'temperature': 0.2,
                'max_tokens': 2000,
            }
        )
        
        return response.text
    ```
  </TabItem>
</Tabs>

### Conversational AI

<Tabs syncKey="language">
  <TabItem label="JavaScript" icon="seti:javascript">
    Build conversational applications with DeepSeek:

    ```ts
    // Conversational chat flow
    export const chatFlow = ai.defineFlow(
      {
        name: 'chatFlow',
        inputSchema: z.object({
          message: z.string(),
          history: z.array(z.object({
            role: z.enum(['user', 'assistant']),
            content: z.string(),
          })).optional(),
          mode: z.enum(['general', 'coding', 'reasoning']).optional(),
        }),
        outputSchema: z.object({ response: z.string() }),
      },
      async ({ message, history = [], mode = 'general' }) => {
        // Select model based on conversation mode
        const modelMap = {
          general: 'deepseek-chat',
          coding: 'deepseek-coder',
          reasoning: 'deepseek-reasoner',
        };

        // Build conversation context
        const messages = [
          { role: 'system', content: `You are a helpful AI assistant specialized in ${mode} tasks.` },
          ...history,
          { role: 'user', content: message },
        ];

        const response = await ai.generate({
          model: deepSeek.model(modelMap[mode]),
          messages,
          config: {
            temperature: mode === 'reasoning' ? 0.1 : 0.7,
            maxTokens: 1500,
          },
        });

        return { response: response.text };
      },
    );

    // Multi-turn reasoning conversation
    export const reasoningChatFlow = ai.defineFlow(
      {
        name: 'reasoningChatFlow',
        inputSchema: z.object({
          question: z.string(),
          context: z.string().optional(),
        }),
        outputSchema: z.object({ 
          answer: z.string(),
          reasoning: z.string(),
        }),
      },
      async ({ question, context }) => {
        const prompt = context 
          ? `Given this context: ${context}\n\nAnswer this question with detailed reasoning: ${question}`
          : `Answer this question with detailed reasoning: ${question}`;

        const response = await ai.generate({
          model: deepSeek.model('deepseek-reasoner'),
          prompt,
          config: {
            temperature: 0.2,
            maxTokens: 2000,
          },
        });

        // Split response into answer and reasoning
        const parts = response.text.split('\n\n');
        const answer = parts[parts.length - 1];
        const reasoning = parts.slice(0, -1).join('\n\n');

        return { answer, reasoning };
      },
    );
    ```
  </TabItem>
  <TabItem label="Go" icon="seti:go">
    Build conversational applications with DeepSeek:

    ```go
    type ChatMessage struct {
        Role    string `json:"role"`
        Content string `json:"content"`
    }

    func handleChat(ctx context.Context, message string, history []ChatMessage, mode string) (string, error) {
        // Select model based on conversation mode
        modelMap := map[string]string{
            "general":   "deepseek-chat",
            "coding":    "deepseek-coder",
            "reasoning": "deepseek-reasoner",
        }
        
        model, exists := modelMap[mode]
        if !exists {
            model = "deepseek-chat"
        }

        // Build conversation context
        messages := []ChatMessage{
            {Role: "system", Content: fmt.Sprintf("You are a helpful AI assistant specialized in %s tasks.", mode)},
        }
        messages = append(messages, history...)
        messages = append(messages, ChatMessage{Role: "user", Content: message})

        temperature := 0.7
        if mode == "reasoning" {
            temperature = 0.1
        }

        resp, err := genkit.Generate(ctx, g,
            ai.WithModel(model),
            ai.WithMessages(messages),
            ai.WithConfig(map[string]interface{}{
                "temperature": temperature,
                "max_tokens":  1500,
            }),
        )
        if err != nil {
            return "", err
        }

        return resp.Text(), nil
    }
    ```
  </TabItem>
  <TabItem label="Python" icon="seti:python">
    Build conversational applications with DeepSeek:

    ```python
    from typing import List, Dict, Optional

    async def handle_chat(
        message: str, 
        history: List[Dict[str, str]] = None, 
        mode: str = 'general'
    ) -> str:
        if history is None:
            history = []
        
        # Select model based on conversation mode
        model_map = {
            'general': 'deepseek-chat',
            'coding': 'deepseek-coder',
            'reasoning': 'deepseek-reasoner',
        }
        
        model = model_map.get(mode, 'deepseek-chat')
        
        # Build conversation context
        messages = [
            {"role": "system", "content": f"You are a helpful AI assistant specialized in {mode} tasks."},
            *history,
            {"role": "user", "content": message},
        ]

        temperature = 0.1 if mode == 'reasoning' else 0.7

        response = await ai.generate(
            model=openai_name(model),
            messages=messages,
            config={
                'temperature': temperature,
                'max_tokens': 1500,
            }
        )

        return response.text
    ```
  </TabItem>
</Tabs>

## Model Comparison

### Available Models

| Model | Capabilities | Best For | Context Window |
|-------|-------------|----------|----------------|
| **deepseek-chat** | General conversation, reasoning | General-purpose tasks, Q&A | 32K tokens |
| **deepseek-coder** | Code generation, programming | Software development, code review | 32K tokens |
| **deepseek-reasoner** | Advanced reasoning, mathematics | Complex problem solving, analysis | 32K tokens |

### Performance Characteristics

<Tabs syncKey="language">
  <TabItem label="JavaScript" icon="seti:javascript">
    ```ts
    // Performance comparison example
    const performanceTest = async () => {
      const prompt = "Explain the time complexity of quicksort algorithm";
      
      // General model
      const startGeneral = Date.now();
      const generalResponse = await ai.generate({
        model: deepSeek.model('deepseek-chat'),
        prompt,
      });
      const generalTime = Date.now() - startGeneral;
      
      // Specialized coder model
      const startCoder = Date.now();
      const coderResponse = await ai.generate({
        model: deepSeek.model('deepseek-coder'),
        prompt,
      });
      const coderTime = Date.now() - startCoder;
      
      console.log(`General: ${generalTime}ms, Coder: ${coderTime}ms`);
      console.log(`General length: ${generalResponse.text.length}, Coder length: ${coderResponse.text.length}`);
    };
    ```
  </TabItem>
  <TabItem label="Go" icon="seti:go">
    ```go
    func performanceTest(ctx context.Context) {
        prompt := "Explain the time complexity of quicksort algorithm"
        
        // General model
        startGeneral := time.Now()
        generalResp, _ := genkit.Generate(ctx, g,
            ai.WithModel("deepseek-chat"),
            ai.WithPrompt(prompt),
        )
        generalTime := time.Since(startGeneral)
        
        // Specialized coder model
        startCoder := time.Now()
        coderResp, _ := genkit.Generate(ctx, g,
            ai.WithModel("deepseek-coder"),
            ai.WithPrompt(prompt),
        )
        coderTime := time.Since(startCoder)
        
        fmt.Printf("General: %v, Coder: %v\n", generalTime, coderTime)
        fmt.Printf("General length: %d, Coder length: %d\n", 
            len(generalResp.Text()), len(coderResp.Text()))
    }
    ```
  </TabItem>
  <TabItem label="Python" icon="seti:python">
    ```python
    import time

    async def performance_test():
        prompt = "Explain the time complexity of quicksort algorithm"
        
        # General model
        start_general = time.time()
        general_response = await ai.generate(
            model=openai_name('deepseek-chat'),
            prompt=prompt
        )
        general_time = time.time() - start_general
        
        # Specialized coder model
        start_coder = time.time()
        coder_response = await ai.generate(
            model=openai_name('deepseek-coder'),
            prompt=prompt
        )
        coder_time = time.time() - start_coder
        
        print(f"General: {general_time:.2f}s, Coder: {coder_time:.2f}s")
        print(f"General length: {len(general_response.text)}, Coder length: {len(coder_response.text)}")
    ```
  </TabItem>
</Tabs>

## Advanced Configuration

### Custom Model Configuration

<Tabs syncKey="language">
  <TabItem label="JavaScript" icon="seti:javascript">
    ```ts
    // Advanced configuration with passthrough options
    const response = await ai.generate({
      model: deepSeek.model('deepseek-chat'),
      prompt: 'Analyze the latest developments in AI',
      config: {
        temperature: 0.7,
        maxTokens: 2000,
        topP: 0.9,
        frequencyPenalty: 0.1,
        presencePenalty: 0.1,
        // Passthrough configuration for new features
        stream: true,
        logprobs: true,
        top_logprobs: 5,
      },
    });

    // Environment-specific configuration
    const environmentConfig = {
      development: {
        model: deepSeek.model('deepseek-chat'),
        temperature: 0.8,
        maxTokens: 1000,
      },
      production: {
        model: deepSeek.model('deepseek-reasoner'),
        temperature: 0.3,
        maxTokens: 2000,
      },
    };

    const config = environmentConfig[process.env.NODE_ENV || 'development'];
    ```
  </TabItem>
  <TabItem label="Go" icon="seti:go">
    ```go
    // Advanced configuration
    resp, err := genkit.Generate(ctx, g,
        ai.WithModel("deepseek-chat"),
        ai.WithPrompt("Analyze the latest developments in AI"),
        ai.WithConfig(map[string]interface{}{
            "temperature":        0.7,
            "max_tokens":        2000,
            "top_p":             0.9,
            "frequency_penalty": 0.1,
            "presence_penalty":  0.1,
            "stream":            true,
            "logprobs":          true,
            "top_logprobs":      5,
        }),
    )
    ```
  </TabItem>
  <TabItem label="Python" icon="seti:python">
    ```python
    # Advanced configuration
    response = await ai.generate(
        model=openai_name('deepseek-chat'),
        prompt='Analyze the latest developments in AI',
        config={
            'temperature': 0.7,
            'max_tokens': 2000,
            'top_p': 0.9,
            'frequency_penalty': 0.1,
            'presence_penalty': 0.1,
            'stream': True,
            'logprobs': True,
            'top_logprobs': 5,
        }
    )
    ```
  </TabItem>
</Tabs>

## Best Practices

### Optimizing for Different Tasks

1. **General conversation**: Use `deepseek-chat` with moderate temperature (0.7)
2. **Code generation**: Use `deepseek-coder` with low temperature (0.2)
3. **Mathematical reasoning**: Use `deepseek-reasoner` with very low temperature (0.1)
4. **Creative writing**: Use `deepseek-chat` with higher temperature (0.8-0.9)

### Cost Optimization

1. **Choose the right model**: Use specialized models for their intended tasks
2. **Optimize token usage**: Be specific in prompts and set appropriate `maxTokens`
3. **Cache responses**: Cache frequently requested computations
4. **Batch similar requests**: Group related queries when possible

### Error Handling

<Tabs syncKey="language">
  <TabItem label="JavaScript" icon="seti:javascript">
    ```ts
    const robustDeepSeekFlow = ai.defineFlow(
      {
        name: 'robustDeepSeekFlow',
        inputSchema: z.object({ query: z.string() }),
        outputSchema: z.object({ response: z.string() }),
      },
      async ({ query }) => {
        try {
          const response = await ai.generate({
            model: deepSeek.model('deepseek-chat'),
            prompt: query,
            config: {
              temperature: 0.7,
              maxTokens: 1000,
            },
          });
          return { response: response.text };
        } catch (error) {
          if (error.message.includes('rate_limit')) {
            // Fallback to reasoning model with lower token limit
            const fallbackResponse = await ai.generate({
              model: deepSeek.model('deepseek-reasoner'),
              prompt: query,
              config: {
                maxTokens: 500,
              },
            });
            return { response: fallbackResponse.text };
          }
          throw error;
        }
      },
    );
    ```
  </TabItem>
  <TabItem label="Go" icon="seti:go">
    ```go
    func robustDeepSeekGenerate(ctx context.Context, query string) (string, error) {
        resp, err := genkit.Generate(ctx, g,
            ai.WithModel("deepseek-chat"),
            ai.WithPrompt(query),
            ai.WithConfig(map[string]interface{}{
                "temperature": 0.7,
                "max_tokens":  1000,
            }),
        )
        
        if err != nil {
            if strings.Contains(err.Error(), "rate_limit") {
                // Fallback to reasoning model
                fallbackResp, fallbackErr := genkit.Generate(ctx, g,
                    ai.WithModel("deepseek-reasoner"),
                    ai.WithPrompt(query),
                    ai.WithConfig(map[string]interface{}{
                        "max_tokens": 500,
                    }),
                )
                if fallbackErr != nil {
                    return "", fallbackErr
                }
                return fallbackResp.Text(), nil
            }
            return "", err
        }
        
        return resp.Text(), nil
    }
    ```
  </TabItem>
  <TabItem label="Python" icon="seti:python">
    ```python
    async def robust_deepseek_generate(query: str) -> str:
        try:
            response = await ai.generate(
                model=openai_name('deepseek-chat'),
                prompt=query,
                config={
                    'temperature': 0.7,
                    'max_tokens': 1000,
                }
            )
            return response.text
        except Exception as error:
            if 'rate_limit' in str(error):
                # Fallback to reasoning model
                fallback_response = await ai.generate(
                    model=openai_name('deepseek-reasoner'),
                    prompt=query,
                    config={
                        'max_tokens': 500,
                    }
                )
                return fallback_response.text
            raise error
    ```
  </TabItem>
</Tabs>

## Next Steps

- Learn about [generating content](/unified-docs/generating-content) to understand how to use these models effectively
- Explore [tool calling](/unified-docs/tool-calling) to add interactive capabilities to your DeepSeek applications
- See [creating flows](/unified-docs/creating-flows) to build structured AI workflows with reasoning capabilities
- Check out [deployment guides](/unified-docs/deployment) for production deployment strategies
