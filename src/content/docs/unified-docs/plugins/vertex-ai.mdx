---
title: Vertex AI plugin
description: Learn how to use Google Cloud Vertex AI with Genkit across JavaScript, Go, and Python, including Gemini models, Imagen image generation, evaluation metrics, Vector Search, and text-to-speech capabilities.
---

import LanguageSelector from '../../../../components/LanguageSelector.astro';
import LanguageContent from '../../../../components/LanguageContent.astro';

<LanguageSelector />

The Vertex AI plugin provides interfaces to several Google Cloud AI services, offering enterprise-grade AI capabilities with advanced features like grounding, evaluation metrics, and vector search.

## Available Services

The Vertex AI plugin provides access to:

- **Google generative AI models**: Gemini text generation, Imagen image generation, text and multimodal embeddings
- **Evaluation metrics**: BLEU, ROUGE, Fluency, Safety, Groundedness, and Summarization metrics through Vertex AI Rapid Evaluation API
- **Vector Search**: Enterprise-grade vector database service
- **Text-to-Speech**: Advanced speech synthesis capabilities
- **Model Garden**: Access to third-party models like Claude 3, Llama 3.1, and Mistral

## Installation and Setup

<LanguageContent lang="js">
Install the Vertex AI plugin:

    ```bash
    npm install @genkit-ai/vertexai
    ```

    If you want to locally run flows that use this plugin, you also need the [Google Cloud CLI tool](https://cloud.google.com/sdk/docs/install) installed.

    Configure the plugin when initializing Genkit:

    ```ts
    import { genkit } from 'genkit';
    import { vertexAI } from '@genkit-ai/vertexai';

    const ai = genkit({
      plugins: [vertexAI({ location: 'us-central1' })],
    });
    ```
</LanguageContent>

<LanguageContent lang="go">
The Vertex AI plugin is included with the Genkit Go package:

    ```go
    import (
        "context"
        "github.com/firebase/genkit/go/genkit"
        "github.com/firebase/genkit/go/plugins/googlecloud"
    )

    func main() {
        ctx := context.Background()
        g, err := genkit.Init(ctx,
            genkit.WithPlugins(&googlecloud.GoogleCloud{
                ProjectID: "your-project-id",
                Location:  "us-central1",
            }),
        )
        if err != nil {
            log.Fatal(err)
        }
    }
    ```
</LanguageContent>

<LanguageContent lang="python">
Vertex AI support in Python is available through the Google Cloud plugin:

    ```bash
    pip install genkit-plugin-google-cloud
    ```

    Configure the plugin when initializing Genkit:

    ```python
    from genkit.ai import Genkit
    from genkit.plugins.google_cloud import GoogleCloud

    ai = Genkit(
        plugins=[GoogleCloud(
            project_id="your-project-id",
            location="us-central1",
        )],
    )
    ```
</LanguageContent>

## Authentication and Configuration

<LanguageContent lang="js">
The plugin requires:

    1. **Google Cloud project ID**: Set via `projectId` in configuration or `GCLOUD_PROJECT` environment variable
    2. **API location**: Set via `location` in configuration or `GCLOUD_LOCATION` environment variable  
    3. **Authentication**: Set up Google Cloud Application Default Credentials

    **Local development:**
    ```bash
    gcloud auth application-default login --project YOUR_PROJECT_ID
    ```

    **Production environments:**
    - Google Cloud environments (Cloud Functions, Cloud Run) are automatically authenticated
    - Other environments: see [Application Default Credentials](https://cloud.google.com/docs/authentication/provide-credentials-adc) docs

    **Required IAM role:** Vertex AI User (`roles/aiplatform.user`)
</LanguageContent>

<LanguageContent lang="go">
Configure authentication and project settings:

    ```go
    g, err := genkit.Init(ctx,
        genkit.WithPlugins(&googlecloud.GoogleCloud{
            ProjectID: "your-project-id",
            Location:  "us-central1",
        }),
    )
    ```

    **Authentication setup:**
    ```bash
    gcloud auth application-default login --project YOUR_PROJECT_ID
    ```

    **Required IAM role:** Vertex AI User (`roles/aiplatform.user`)
</LanguageContent>

<LanguageContent lang="python">
Configure the plugin with your project details:

    ```python
    ai = Genkit(
        plugins=[GoogleCloud(
            project_id="your-project-id",
            location="us-central1",
        )],
    )
    ```

    **Authentication setup:**
    ```bash
    gcloud auth application-default login --project YOUR_PROJECT_ID
    ```

    **Required IAM role:** Vertex AI User (`roles/aiplatform.user`)
</LanguageContent>

## Basic Usage

<LanguageContent lang="js">
Use Vertex AI models for text generation:

    ```ts
    import { vertexAI } from '@genkit-ai/vertexai';

    const ai = genkit({
      plugins: [vertexAI({ location: 'us-central1' })],
    });

    const llmResponse = await ai.generate({
      model: vertexAI.model('gemini-2.5-flash'),
      prompt: 'What should I do when I visit Melbourne?',
    });

    // Generate embeddings
    const embeddings = await ai.embed({
      embedder: vertexAI.embedder('gemini-embedding-001'),
      content: 'How many widgets do you have in stock?',
    });
    ```
</LanguageContent>

<LanguageContent lang="go">
Use Vertex AI models with the generation API:

    ```go
    import (
        "context"
        "github.com/firebase/genkit/go/ai"
        "github.com/firebase/genkit/go/genkit"
        "github.com/firebase/genkit/go/plugins/googlecloud"
    )

    func main() {
        ctx := context.Background()
        g, err := genkit.Init(ctx,
            genkit.WithPlugins(&googlecloud.GoogleCloud{
                ProjectID: "your-project-id",
                Location:  "us-central1",
            }),
            genkit.WithDefaultModel("vertexai/gemini-2.5-flash"),
        )
        if err != nil {
            log.Fatal(err)
        }

        // Generate content
        resp, err := genkit.Generate(ctx, g,
            ai.WithPrompt("What should I do when I visit Melbourne?"),
        )
        if err != nil {
            log.Fatal(err)
        }

        fmt.Println(resp.Text())
    }
    ```
</LanguageContent>

<LanguageContent lang="python">
Use Vertex AI models with the generation API:

    ```python
    from genkit.ai import Genkit
    from genkit.plugins.google_cloud import GoogleCloud, vertex_ai_name

    ai = Genkit(
        plugins=[GoogleCloud(
            project_id="your-project-id",
            location="us-central1",
        )],
        model=vertex_ai_name('gemini-2.5-flash'),
    )

    # Generate content
    response = await ai.generate('What should I do when I visit Melbourne?')
    print(response.text)

    # Generate embeddings
    embeddings = await ai.embed(
        embedder=vertex_ai_name('gemini-embedding-001'),
        content='How many widgets do you have in stock?',
    )
    ```
</LanguageContent>

## Advanced Features

### Grounding with Google Search and Private Data

<LanguageContent lang="js">
Ground Gemini responses using Google Search or your own data:

    ```ts
    // Google Search grounding
    await ai.generate({
      model: vertexAI.model('gemini-2.5-flash'),
      prompt: 'What are the latest developments in AI?',
      config: {
        googleSearchRetrieval: {
          disableAttribution: true,
        }
      }
    });

    // Private data grounding
    await ai.generate({
      model: vertexAI.model('gemini-2.5-flash'),
      prompt: 'What does our company policy say about remote work?',
      config: {
        vertexRetrieval: {
          datastore: {
            projectId: 'your-cloud-project',
            location: 'us-central1',
            collection: 'your-collection',
          },
          disableAttribution: true,
        }
      }
    });
    ```

    :::caution[Pricing]
    Vertex AI charges additional fees for grounding requests. See [Vertex AI pricing](https://cloud.google.com/vertex-ai/generative-ai/pricing) for details.
    :::
</LanguageContent>

<LanguageContent lang="go">
Grounding functionality is available through the Vertex AI API. Implement using the Google Cloud SDK directly or through custom configuration:

    ```go
    // Grounding requires custom implementation using the Vertex AI API
    // See Google Cloud documentation for grounding configuration
    ```
</LanguageContent>

<LanguageContent lang="python">
Grounding functionality is available through the Vertex AI API. Implement using the Google Cloud SDK directly:

    ```python
    # Grounding requires custom implementation using the Vertex AI API
    # See Google Cloud documentation for grounding configuration
    ```
</LanguageContent>

### Image Generation with Imagen

<LanguageContent lang="js">
Generate images from text prompts:

    ```ts
    // Basic image generation
    const response = await ai.generate({
      model: vertexAI.model('imagen-3.0-generate-002'),
      output: { format: 'media' },
      prompt: 'a banana riding a bicycle',
    });

    // Advanced image editing
    const baseImg = fs.readFileSync('base.png', { encoding: 'base64' });
    const maskImg = fs.readFileSync('mask.png', { encoding: 'base64' });

    const editResponse = await ai.generate({
      model: vertexAI.model('imagen-3.0-generate-002'),
      output: { format: 'media' },
      prompt: [
        { media: { url: `data:image/png;base64,${baseImg}` } },
        {
          media: { url: `data:image/png;base64,${maskImg}` },
          metadata: { type: 'mask' },
        },
        { text: 'replace the background with a sunset' },
      ],
      config: {
        editConfig: {
          editMode: 'outpainting',
        },
      },
    });
    ```
</LanguageContent>

<LanguageContent lang="go">
Image generation is available through the Vertex AI API. Implement using the Google Cloud SDK:

    ```go
    // Image generation requires custom implementation using the Vertex AI API
    // See Vertex AI Imagen documentation for implementation details
    ```
</LanguageContent>

<LanguageContent lang="python">
Image generation is available through the Vertex AI API. Implement using the Google Cloud SDK:

    ```python
    # Image generation requires custom implementation using the Vertex AI API
    # See Vertex AI Imagen documentation for implementation details
    ```
</LanguageContent>

### Multimodal Embeddings

<LanguageContent lang="js">
Generate embeddings from text, images, and video:

    ```ts
    // Text embeddings
    const textEmbeddings = await ai.embed({
      embedder: vertexAI.embedder('gemini-embedding-001'),
      content: 'How many widgets do you have in stock?',
    });

    // Multimodal embeddings
    const multimodalEmbeddings = await ai.embed({
      embedder: vertexAI.embedder('multimodal-embedding-001'),
      content: {
        content: [
          {
            media: {
              url: 'gs://cloud-samples-data/generative-ai/video/pixel8.mp4',
              contentType: 'video/mp4',
            },
          },
        ],
      },
    });
    ```
</LanguageContent>

<LanguageContent lang="go">
Generate embeddings using Vertex AI models:

    ```go
    // Text embeddings
    embeddings, err := genkit.Embed(ctx, g,
        ai.WithEmbedder("vertexai/gemini-embedding-001"),
        ai.WithEmbedContent("How many widgets do you have in stock?"),
    )
    if err != nil {
        log.Fatal(err)
    }
    ```
</LanguageContent>

<LanguageContent lang="python">
Generate embeddings using Vertex AI models:

    ```python
    # Text embeddings
    embeddings = await ai.embed(
        embedder=vertex_ai_name('gemini-embedding-001'),
        content='How many widgets do you have in stock?',
    )
    ```
</LanguageContent>

## Model Garden Integration

<LanguageContent lang="js">
Access third-party models through Vertex AI Model Garden:

    ### Claude 3 Models

    ```ts
    import { vertexAIModelGarden } from '@genkit-ai/vertexai/modelgarden';

    const ai = genkit({
      plugins: [
        vertexAIModelGarden({
          location: 'us-central1',
          models: ['claude-3-haiku', 'claude-3-sonnet', 'claude-3-opus'],
        }),
      ],
    });

    const response = await ai.generate({
      model: 'claude-3-sonnet',
      prompt: 'What should I do when I visit Melbourne?',
    });
    ```

    ### Llama 3.1 405b

    ```ts
    const ai = genkit({
      plugins: [
        vertexAIModelGarden({
          location: 'us-central1',
          models: ['llama3-405b-instruct-maas'],
        }),
      ],
    });

    const response = await ai.generate({
      model: 'llama3-405b-instruct-maas',
      prompt: 'Write a function that adds two numbers together',
    });
    ```

    ### Mistral Models

    ```ts
    const ai = genkit({
      plugins: [
        vertexAIModelGarden({
          location: 'us-central1',
          models: ['mistral-large', 'mistral-nemo', 'codestral'],
        }),
      ],
    });

    const response = await ai.generate({
      model: 'mistral-large',
      prompt: 'Write a function that adds two numbers together',
      config: {
        version: 'mistral-large-2411',
        temperature: 0.7,
        maxOutputTokens: 1024,
        topP: 0.9,
        stopSequences: ['###'],
      },
    });
    ```
</LanguageContent>

<LanguageContent lang="go">
Model Garden integration requires custom implementation using the Vertex AI API:

    ```go
    // Model Garden models require custom implementation
    // See Vertex AI Model Garden documentation for setup
    ```
</LanguageContent>

<LanguageContent lang="python">
Model Garden integration requires custom implementation using the Vertex AI API:

    ```python
    # Model Garden models require custom implementation
    # See Vertex AI Model Garden documentation for setup
    ```
</LanguageContent>

## Evaluation Metrics

<LanguageContent lang="js">
Use Vertex AI Rapid Evaluation API for model evaluation:

    ```ts
    import { vertexAIEvaluation, VertexAIEvaluationMetricType } from '@genkit-ai/vertexai/evaluation';

    const ai = genkit({
      plugins: [
        vertexAIEvaluation({
          location: 'us-central1',
          metrics: [
            VertexAIEvaluationMetricType.SAFETY,
            {
              type: VertexAIEvaluationMetricType.ROUGE,
              metricSpec: {
                rougeType: 'rougeLsum',
              },
            },
          ],
        }),
      ],
    });
    ```

    Available metrics:
    - **BLEU**: Translation quality
    - **ROUGE**: Summarization quality  
    - **Fluency**: Text fluency
    - **Safety**: Content safety
    - **Groundedness**: Factual accuracy
    - **Summarization Quality/Helpfulness/Verbosity**: Summary evaluation

    Run evaluations:
    ```bash
    genkit eval:run
    genkit eval:flow -e vertexai/safety
    ```
</LanguageContent>

<LanguageContent lang="go">
Evaluation metrics are available through the Vertex AI API:

    ```go
    // Evaluation requires custom implementation using the Vertex AI API
    // See Vertex AI Rapid Evaluation documentation
    ```
</LanguageContent>

<LanguageContent lang="python">
Evaluation metrics are available through the Vertex AI API:

    ```python
    # Evaluation requires custom implementation using the Vertex AI API
    # See Vertex AI Rapid Evaluation documentation
    ```
</LanguageContent>

## Vector Search

<LanguageContent lang="js">
Use Vertex AI Vector Search for enterprise-grade vector operations:

    ### Setup

    1. Create a Vector Search index in the [Google Cloud Console](https://console.cloud.google.com/vertex-ai/matching-engine/indexes)
    2. Configure dimensions based on your embedding model:
       - `gemini-embedding-001`: 768 dimensions
       - `text-multilingual-embedding-002`: 768 dimensions  
       - `multimodalEmbedding001`: 128, 256, 512, or 1408 dimensions
    3. Deploy the index to a standard endpoint

    ### Configuration

    ```ts
    import { vertexAIVectorSearch } from '@genkit-ai/vertexai/vectorsearch';
    import { getFirestoreDocumentIndexer, getFirestoreDocumentRetriever } from '@genkit-ai/vertexai/vectorsearch';

    const ai = genkit({
      plugins: [
        vertexAIVectorSearch({
          projectId: 'your-project-id',
          location: 'us-central1',
          vectorSearchOptions: [
            {
              indexId: 'your-index-id',
              indexEndpointId: 'your-endpoint-id',
              deployedIndexId: 'your-deployed-index-id',
              publicDomainName: 'your-domain-name',
              documentRetriever: firestoreDocumentRetriever,
              documentIndexer: firestoreDocumentIndexer,
              embedder: vertexAI.embedder('gemini-embedding-001'),
            },
          ],
        }),
      ],
    });
    ```

    ### Usage

    ```ts
    import { vertexAiIndexerRef, vertexAiRetrieverRef } from '@genkit-ai/vertexai/vectorsearch';

    // Index documents
    await ai.index({
      indexer: vertexAiIndexerRef({
        indexId: 'your-index-id',
      }),
      documents,
    });

    // Retrieve similar documents
    const results = await ai.retrieve({
      retriever: vertexAiRetrieverRef({
        indexId: 'your-index-id',
      }),
      query: queryDocument,
    });
    ```

    :::caution[Pricing]
    Vector Search has both ingestion and hosting costs. See [Vertex AI pricing](https://cloud.google.com/vertex-ai/pricing#vectorsearch) for details.
    :::
</LanguageContent>

<LanguageContent lang="go">
Vector Search integration requires custom implementation using the Vertex AI API:

    ```go
    // Vector Search requires custom implementation
    // See Vertex AI Vector Search documentation for setup
    ```
</LanguageContent>

<LanguageContent lang="python">
Vector Search integration requires custom implementation using the Vertex AI API:

    ```python
    # Vector Search requires custom implementation
    # See Vertex AI Vector Search documentation for setup
    ```
</LanguageContent>

## Text-to-Speech

<LanguageContent lang="js">
Generate high-quality speech from text:

    ```ts
    import { writeFile } from 'node:fs/promises';

    const response = await ai.generate({
      model: vertexAI.model('gemini-2.5-flash-preview-tts'),
      config: {
        responseModalities: ['AUDIO'],
        speechConfig: {
          voiceConfig: {
            prebuiltVoiceConfig: { voiceName: 'Algenib' },
          },
        },
      },
      prompt: 'Say that Genkit is an amazing Gen AI library',
    });

    if (response.media?.url) {
      const audioBuffer = Buffer.from(
        response.media.url.substring(response.media.url.indexOf(',') + 1),
        'base64'
      );
      await writeFile('output.wav', audioBuffer);
    }
    ```

    ### Multi-speaker Audio

    ```ts
    const response = await ai.generate({
      model: vertexAI.model('gemini-2.5-flash-preview-tts'),
      config: {
        responseModalities: ['AUDIO'],
        speechConfig: {
          multiSpeakerVoiceConfig: {
            speakerVoiceConfigs: [
              {
                speaker: 'Speaker1',
                voiceConfig: {
                  prebuiltVoiceConfig: { voiceName: 'Algenib' },
                },
              },
              {
                speaker: 'Speaker2',
                voiceConfig: {
                  prebuiltVoiceConfig: { voiceName: 'Achernar' },
                },
              },
            ],
          },
        },
      },
      prompt: `Here's the dialog:
        Speaker1: "Genkit is an amazing Gen AI library!"
        Speaker2: "I thought it was a framework."`,
    });
    ```
</LanguageContent>

<LanguageContent lang="go">
Text-to-speech functionality requires custom implementation using the Vertex AI API:

    ```go
    // TTS requires custom implementation using the Vertex AI API
    // See Vertex AI Speech Generation documentation
    ```
</LanguageContent>

<LanguageContent lang="python">
Text-to-speech functionality requires custom implementation using the Vertex AI API:

    ```python
    # TTS requires custom implementation using the Vertex AI API
    # See Vertex AI Speech Generation documentation
    ```
</LanguageContent>

## Context Caching

<LanguageContent lang="js">
Optimize performance with context caching for large inputs:

    ```ts
    const llmResponse = await ai.generate({
      messages: [
        {
          role: 'user',
          content: [{ text: 'Here is the relevant text from War and Peace.' }],
        },
        {
          role: 'model',
          content: [
            {
              text: "Based on War and Peace, here is some analysis of Pierre Bezukhov's character.",
            },
          ],
          metadata: {
            cache: {
              ttlSeconds: 300, // Cache for 5 minutes
            },
          },
        },
      ],
      model: vertexAI.model('gemini-2.5-flash'),
      prompt: "Describe Pierre's transformation throughout the novel.",
    });
    ```

    **Supported models**: `gemini-2.5-flash-001`, `gemini-2.0-pro-001`
</LanguageContent>

<LanguageContent lang="go">
Context caching requires custom implementation using the Vertex AI API:

    ```go
    // Context caching requires custom implementation
    // See Vertex AI Context Caching documentation
    ```
</LanguageContent>

<LanguageContent lang="python">
Context caching requires custom implementation using the Vertex AI API:

    ```python
    # Context caching requires custom implementation
    # See Vertex AI Context Caching documentation
    ```
</LanguageContent>

## Available Models

### Text Generation
- `gemini-2.5-flash`, `gemini-2.5-flash-lite`, `gemini-1.5-pro`
- `claude-3-haiku`, `claude-3-sonnet`, `claude-3-opus` (Model Garden)
- `llama3-405b-instruct-maas` (Model Garden)
- `mistral-large`, `mistral-nemo`, `codestral` (Model Garden)

### Embeddings
- `gemini-embedding-001` (768 dimensions)
- `text-multilingual-embedding-002` (768 dimensions)
- `multimodal-embedding-001` (128-1408 dimensions)

### Image Generation
- `imagen-3.0-generate-002`, `imagen-2.0-generate-001`

### Text-to-Speech
- `gemini-2.5-flash-preview-tts`

## Next Steps

- Learn about [generating content](/unified-docs/generating-content) to understand how to use these models effectively
- Explore [evaluation](/unified-docs/evaluation) to leverage Vertex AI's evaluation metrics
- See [RAG](/unified-docs/rag) to implement retrieval-augmented generation with Vector Search
- Check out [creating flows](/unified-docs/creating-flows) to build structured AI workflows
