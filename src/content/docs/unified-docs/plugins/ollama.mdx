---
title: Ollama plugin
description: Learn how to use Ollama for local AI models with Genkit across JavaScript, Go, and Python, including setup, configuration, and usage for both text generation and embeddings.
---

import LanguageSelector from '../../../../components/LanguageSelector.astro';
import LanguageContent from '../../../../components/LanguageContent.astro';

<LanguageSelector />

The Ollama plugin provides interfaces to local LLMs supported by [Ollama](https://ollama.com/), enabling you to run powerful AI models locally without requiring cloud API keys or internet connectivity.

## Prerequisites

Before using the Ollama plugin, you need to install and run the Ollama server locally:

1. **Download and install Ollama** from [ollama.com/download](https://ollama.com/download)
2. **Download models** using the Ollama CLI:
   ```bash
   ollama pull gemma
   ollama pull llama2
   ollama pull nomic-embed-text  # for embeddings
   ```
3. **Start the Ollama server** (usually starts automatically after installation)

## Installation and Setup

<LanguageContent lang="js">
Install the Ollama plugin:

    ```bash
    npm install genkitx-ollama
    ```

    Configure the plugin when initializing Genkit:

    ```ts
    import { genkit } from 'genkit';
    import { ollama } from 'genkitx-ollama';

    const ai = genkit({
      plugins: [
        ollama({
          models: [
            {
              name: 'gemma',
              type: 'generate', // 'chat' | 'generate' | undefined
            },
            {
              name: 'llama2',
              type: 'chat',
            },
          ],
          serverAddress: 'http://127.0.0.1:11434', // default local address
        }),
      ],
    });
    ```
</LanguageContent>

<LanguageContent lang="go">
The Ollama plugin is available through the Ollama package:

    ```go
    import (
        "context"
        "github.com/firebase/genkit/go/genkit"
        "github.com/firebase/genkit/go/plugins/ollama"
    )

    func main() {
        ctx := context.Background()
        g, err := genkit.Init(ctx,
            genkit.WithPlugins(&ollama.Ollama{
                ServerAddress: "http://127.0.0.1:11434",
                Models: []ollama.ModelConfig{
                    {Name: "gemma", Type: "generate"},
                    {Name: "llama2", Type: "chat"},
                },
            }),
        )
        if err != nil {
            log.Fatal(err)
        }
    }
    ```
</LanguageContent>

<LanguageContent lang="python">
Install the Ollama plugin:

    ```bash
    pip install genkit-plugin-ollama
    ```

    Configure the plugin when initializing Genkit:

    ```python
    from genkit.ai import Genkit
    from genkit.plugins.ollama import Ollama

    ai = Genkit(
        plugins=[Ollama(
            server_address="http://127.0.0.1:11434",
            models=[
                {"name": "gemma", "type": "generate"},
                {"name": "llama2", "type": "chat"},
            ],
        )],
    )
    ```
</LanguageContent>

## Basic Usage

<LanguageContent lang="js">
Use Ollama models for text generation:

    ```ts
    // Basic text generation
    const llmResponse = await ai.generate({
      model: 'ollama/gemma',
      prompt: 'Tell me a joke about programming.',
    });

    console.log(llmResponse.text);

    // Chat-style interaction
    const chatResponse = await ai.generate({
      model: 'ollama/llama2',
      prompt: 'What are the benefits of using local AI models?',
      config: {
        temperature: 0.7,
        maxTokens: 500,
      },
    });

    // Using in a flow
    export const localAIFlow = ai.defineFlow(
      {
        name: 'localAIFlow',
        inputSchema: z.object({ question: z.string() }),
        outputSchema: z.object({ answer: z.string() }),
      },
      async ({ question }) => {
        const response = await ai.generate({
          model: 'ollama/gemma',
          prompt: `Answer this question: ${question}`,
        });
        return { answer: response.text };
      },
    );
    ```
</LanguageContent>

<LanguageContent lang="go">
Use Ollama models with the generation API:

    ```go
    import (
        "context"
        "github.com/firebase/genkit/go/ai"
        "github.com/firebase/genkit/go/genkit"
        "github.com/firebase/genkit/go/plugins/ollama"
    )

    func main() {
        ctx := context.Background()
        g, err := genkit.Init(ctx,
            genkit.WithPlugins(&ollama.Ollama{
                ServerAddress: "http://127.0.0.1:11434",
                Models: []ollama.ModelConfig{
                    {Name: "gemma", Type: "generate"},
                },
            }),
            genkit.WithDefaultModel("ollama/gemma"),
        )
        if err != nil {
            log.Fatal(err)
        }

        // Generate content
        resp, err := genkit.Generate(ctx, g,
            ai.WithPrompt("Tell me a joke about programming."),
        )
        if err != nil {
            log.Fatal(err)
        }

        fmt.Println(resp.Text())
    }
    ```
</LanguageContent>

<LanguageContent lang="python">
Use Ollama models with the generation API:

    ```python
    from genkit.ai import Genkit
    from genkit.plugins.ollama import Ollama, ollama_name

    ai = Genkit(
        plugins=[Ollama(
            server_address="http://127.0.0.1:11434",
            models=[
                {"name": "gemma", "type": "generate"},
                {"name": "llama2", "type": "chat"},
            ],
        )],
        model=ollama_name('gemma'),
    )

    # Generate content
    response = await ai.generate('Tell me a joke about programming.')
    print(response.text)

    # With configuration
    response = await ai.generate(
        prompt='What are the benefits of using local AI models?',
        model=ollama_name('llama2'),
        config={
            'temperature': 0.7,
            'max_tokens': 500,
        }
    )
    ```
</LanguageContent>

## Embeddings

<LanguageContent lang="js">
Use Ollama for text embeddings:

    ```ts
    const ai = genkit({
      plugins: [
        ollama({
          serverAddress: 'http://localhost:11434',
          embedders: [
            { name: 'nomic-embed-text', dimensions: 768 },
            { name: 'all-minilm', dimensions: 384 },
          ],
        }),
      ],
    });

    // Generate embeddings
    const embeddings = await ai.embed({
      embedder: 'ollama/nomic-embed-text',
      content: 'Some text to embed!',
    });

    console.log('Embedding dimensions:', embeddings.length);

    // Use with vector databases
    const ai = genkit({
      plugins: [
        ollama({
          embedders: [{ name: 'nomic-embed-text', dimensions: 768 }],
        }),
        chroma([
          {
            embedder: 'ollama/nomic-embed-text',
            collectionName: 'local-embeddings',
          },
        ]),
      ],
    });

    // Embedding flow
    export const embedFlow = ai.defineFlow(
      {
        name: 'embedFlow',
        inputSchema: z.object({ text: z.string() }),
        outputSchema: z.object({ embedding: z.array(z.number()) }),
      },
      async ({ text }) => {
        const embedding = await ai.embed({
          embedder: 'ollama/nomic-embed-text',
          content: text,
        });
        return { embedding };
      },
    );
    ```
</LanguageContent>

<LanguageContent lang="go">
Generate embeddings using Ollama models:

    ```go
    g, err := genkit.Init(ctx,
        genkit.WithPlugins(&ollama.Ollama{
            ServerAddress: "http://127.0.0.1:11434",
            Embedders: []ollama.EmbedderConfig{
                {Name: "nomic-embed-text", Dimensions: 768},
            },
        }),
    )

    // Generate embeddings
    embeddings, err := genkit.Embed(ctx, g,
        ai.WithEmbedder("ollama/nomic-embed-text"),
        ai.WithEmbedContent("Some text to embed!"),
    )
    if err != nil {
        log.Fatal(err)
    }

    fmt.Printf("Generated %d-dimensional embedding\n", len(embeddings))
    ```
</LanguageContent>

<LanguageContent lang="python">
Generate embeddings using Ollama models:

    ```python
    ai = Genkit(
        plugins=[Ollama(
            server_address="http://127.0.0.1:11434",
            embedders=[
                {"name": "nomic-embed-text", "dimensions": 768},
                {"name": "all-minilm", "dimensions": 384},
            ],
        )],
    )

    # Generate embeddings
    embeddings = await ai.embed(
        embedder=ollama_name('nomic-embed-text'),
        content='Some text to embed!',
    )

    print(f"Generated {len(embeddings)}-dimensional embedding")
    ```
</LanguageContent>

## Authentication and Remote Deployments

<LanguageContent lang="js">
For remote Ollama deployments that require authentication:

    ### Static Headers

    ```ts
    const ai = genkit({
      plugins: [
        ollama({
          models: [{ name: 'gemma' }],
          serverAddress: 'https://my-ollama-deployment.com',
          requestHeaders: {
            'api-key': 'your-api-key-here',
            'authorization': 'Bearer your-token',
          },
        }),
      ],
    });
    ```

    ### Dynamic Headers

    ```ts
    import { GoogleAuth } from 'google-auth-library';

    const ai = genkit({
      plugins: [
        ollama({
          models: [{ name: 'gemma' }],
          serverAddress: 'https://my-ollama-deployment.com',
          requestHeaders: async (params) => {
            const headers = await fetchWithAuthHeader(params.serverAddress);
            return { Authorization: headers['Authorization'] };
          },
        }),
      ],
    });

    // Function to fetch auth headers
    async function fetchWithAuthHeader(url: string) {
      const auth = new GoogleAuth();
      const client = await auth.getIdTokenClient(url);
      const headers = await client.getRequestHeaders(url);
      return headers;
    }
    ```

    ### Environment-based Configuration

    ```ts
    const ollamaConfig = process.env.NODE_ENV === 'production'
      ? {
          models: [{ name: 'gemma' }],
          serverAddress: 'https://my-ollama-deployment.com',
          requestHeaders: { 'api-key': process.env.OLLAMA_API_KEY },
        }
      : {
          models: [{ name: 'gemma' }],
          serverAddress: 'http://127.0.0.1:11434',
        };

    const ai = genkit({
      plugins: [ollama(ollamaConfig)],
    });
    ```
</LanguageContent>

<LanguageContent lang="go">
For remote Ollama deployments with authentication:

    ```go
    g, err := genkit.Init(ctx,
        genkit.WithPlugins(&ollama.Ollama{
            ServerAddress: "https://my-ollama-deployment.com",
            Models: []ollama.ModelConfig{
                {Name: "gemma", Type: "generate"},
            },
            RequestHeaders: map[string]string{
                "api-key":       os.Getenv("OLLAMA_API_KEY"),
                "authorization": "Bearer " + os.Getenv("OLLAMA_TOKEN"),
            },
        }),
    )
    ```
</LanguageContent>

<LanguageContent lang="python">
For remote Ollama deployments with authentication:

    ```python
    import os

    ai = Genkit(
        plugins=[Ollama(
            server_address="https://my-ollama-deployment.com",
            models=[{"name": "gemma", "type": "generate"}],
            request_headers={
                "api-key": os.getenv("OLLAMA_API_KEY"),
                "authorization": f"Bearer {os.getenv('OLLAMA_TOKEN')}",
            },
        )],
    )
    ```
</LanguageContent>

## Model Configuration

### Model Types

<LanguageContent lang="js">
Configure different model types for different use cases:

    ```ts
    const ai = genkit({
      plugins: [
        ollama({
          models: [
            // Chat models for conversational AI
            { name: 'llama2', type: 'chat' },
            { name: 'mistral', type: 'chat' },
            
            // Generate models for text completion
            { name: 'gemma', type: 'generate' },
            { name: 'codellama', type: 'generate' },
            
            // Auto-detect type (default)
            { name: 'phi' }, // type will be auto-detected
          ],
          serverAddress: 'http://127.0.0.1:11434',
        }),
      ],
    });
    ```
</LanguageContent>

<LanguageContent lang="go">
Configure different model types:

    ```go
    g, err := genkit.Init(ctx,
        genkit.WithPlugins(&ollama.Ollama{
            ServerAddress: "http://127.0.0.1:11434",
            Models: []ollama.ModelConfig{
                {Name: "llama2", Type: "chat"},
                {Name: "mistral", Type: "chat"},
                {Name: "gemma", Type: "generate"},
                {Name: "codellama", Type: "generate"},
                {Name: "phi"}, // auto-detect type
            },
        }),
    )
    ```
</LanguageContent>

<LanguageContent lang="python">
Configure different model types:

    ```python
    ai = Genkit(
        plugins=[Ollama(
            server_address="http://127.0.0.1:11434",
            models=[
                {"name": "llama2", "type": "chat"},
                {"name": "mistral", "type": "chat"},
                {"name": "gemma", "type": "generate"},
                {"name": "codellama", "type": "generate"},
                {"name": "phi"},  # auto-detect type
            ],
        )],
    )
    ```
</LanguageContent>

## Popular Models

Here are some popular models you can use with Ollama:

### Text Generation Models
- **Llama 2**: `llama2` (7B, 13B, 70B variants)
- **Gemma**: `gemma` (2B, 7B variants)
- **Mistral**: `mistral` (7B)
- **Code Llama**: `codellama` (7B, 13B, 34B variants)
- **Phi**: `phi` (3B)
- **Qwen**: `qwen` (various sizes)

### Embedding Models
- **Nomic Embed Text**: `nomic-embed-text` (768 dimensions)
- **All-MiniLM**: `all-minilm` (384 dimensions)
- **BGE**: `bge-large` (1024 dimensions)

### Specialized Models
- **Llava**: `llava` (multimodal - text and images)
- **Dolphin**: `dolphin-mistral` (uncensored variant)
- **Orca**: `orca-mini` (smaller, efficient model)

## Configuration Options

### Generation Parameters
- `temperature`: Randomness (0.0-2.0)
- `top_p`: Nucleus sampling (0.0-1.0)
- `top_k`: Top-k sampling
- `repeat_penalty`: Repetition penalty
- `seed`: Random seed for reproducible outputs
- `num_predict`: Maximum tokens to generate
- `stop`: Stop sequences

### Performance Tuning
- `num_ctx`: Context window size
- `num_batch`: Batch size for processing
- `num_gpu`: Number of GPU layers to use
- `num_thread`: Number of CPU threads

## Advantages of Local Models

### Privacy and Security
- **Data stays local**: No data sent to external APIs
- **No API keys required**: No risk of key exposure
- **Offline capability**: Works without internet connection
- **Full control**: Complete control over model behavior

### Cost and Performance
- **No usage fees**: No per-token or per-request charges
- **Predictable costs**: Only hardware and electricity costs
- **Low latency**: No network round-trips
- **Customizable**: Fine-tune models for specific use cases

### Development Benefits
- **Rapid prototyping**: No API rate limits
- **Consistent availability**: No service outages
- **Version control**: Pin specific model versions
- **Experimentation**: Try different models easily

## Next Steps

- Learn about [generating content](/unified-docs/generating-content) to understand how to use these models effectively
- Explore [RAG](/unified-docs/rag) to implement retrieval-augmented generation with local embeddings
- See [creating flows](/unified-docs/creating-flows) to build structured AI workflows with local models
- Check out [tool calling](/unified-docs/tool-calling) to add interactive capabilities to your local AI applications
