---
title: Tool calling
description: Learn how to enable LLMs to interact with external applications and data using Genkit's tool calling feature across JavaScript, Go, and Python, covering tool definition, usage, and advanced scenarios.
---

import ExampleLink from '@/components/ExampleLink.astro';
import LanguageSelector from '../../../components/LanguageSelector.astro';
import LanguageContent from '../../../components/LanguageContent.astro';

<LanguageSelector />

_Tool calling_, also known as _function calling_, is a structured way to give
LLMs the ability to make requests back to the application that called it. You
define the tools you want to make available to the model, and the model will
make tool requests to your app as necessary to fulfill the prompts you give it.

The use cases of tool calling generally fall into a few themes:

**Giving an LLM access to information it wasn't trained with**

- Frequently changing information, such as a stock price or the current
  weather.
- Information specific to your app domain, such as product information or user
  profiles.

Note the overlap with [retrieval augmented generation](/unified-docs/rag) (RAG), which is also
a way to let an LLM integrate factual information into its generations. RAG is a
heavier solution that is most suited when you have a large amount of information
or the information that's most relevant to a prompt is ambiguous. On the other
hand, if retrieving the information the LLM needs is a simple function call or
database lookup, tool calling is more appropriate.

**Introducing a degree of determinism into an LLM workflow**

- Performing calculations that the LLM cannot reliably complete itself.
- Forcing an LLM to generate verbatim text under certain circumstances, such
  as when responding to a question about an app's terms of service.

**Performing an action when initiated by an LLM**

- Turning on and off lights in an LLM-powered home assistant
- Reserving table reservations in an LLM-powered restaurant agent

## Before you begin

If you want to run the code examples on this page, first complete the steps in
the Getting started guide for your language. All of the examples assume that you
have already set up a project with Genkit dependencies installed.

<LanguageContent lang="js">
Complete the [Getting started](/docs/get-started) guide.
</LanguageContent>

<LanguageContent lang="go">
Complete the [Get started](/go/docs/get-started-go) guide.
</LanguageContent>

<LanguageContent lang="python">
Complete the [Get started](/python/docs/get-started) guide.
</LanguageContent>

This page discusses one of the advanced features of Genkit model abstraction, so
before you dive too deeply, you should be familiar with the content on the
[Generating content with AI models](/unified-docs/generating-content) page. You should also be familiar
with Genkit's system for defining input and output schemas, which is discussed
on the [Defining AI workflows](/unified-docs/creating-flows) page.

## Overview of tool calling

<ExampleLink
  title="Tool Calling"
  description="See how Genkit can enable rich UI for tool calling in a live demo."
  example="tool-calling"
/>

At a high level, this is what a typical tool-calling interaction with an LLM
looks like:

1. The calling application prompts the LLM with a request and also includes in
   the prompt a list of tools the LLM can use to generate a response.
2. The LLM either generates a complete response or generates a tool call request
   in a specific format.
3. If the caller receives a complete response, the request is fulfilled and the
   interaction ends; but if the caller receives a tool call, it performs
   whatever logic is appropriate and sends a new request to the LLM containing
   the original prompt or some variation of it as well as the result of the tool
   call.
4. The LLM handles the new prompt as in Step 2.

For this to work, several requirements must be met:

- The model must be trained to make tool requests when it's needed to complete
  a prompt. Most of the larger models provided through web APIs, such as
  Gemini and Claude, can do this, but smaller and more specialized models
  often cannot. Genkit will throw an error if you try to provide tools to a
  model that doesn't support it.
- The calling application must provide tool definitions to the model in the
  format it expects.
- The calling application must prompt the model to generate tool calling
  requests in the format the application expects.

## Tool calling with Genkit

Genkit provides a single interface for tool calling with models that support it.
Each model plugin ensures that the last two of the above criteria are met, and
the Genkit instance's `generate()` function automatically carries out the tool
calling loop described earlier.

### Model support

Tool calling support depends on the model, the model API, and the Genkit plugin.
Consult the relevant documentation to determine if tool calling is likely to be
supported. In addition:

- Genkit will throw an error if you try to provide tools to a model that
  doesn't support it.
- If the plugin exports model references, the model info will indicate if it supports tool calling.

<LanguageContent lang="js">
Check the `info.supports.tools` property on model references:

    ```ts
    import { googleAI } from '@genkit-ai/googleai';
    
    const model = googleAI.model('gemini-2.5-flash');
    console.log(model.info.supports.tools); // true/false
    ```
</LanguageContent>

<LanguageContent lang="go">
Check the `ModelInfo.Supports.Tools` property:

    ```go
    // Model support information is available through the plugin
    // Check plugin documentation for specific model capabilities
    ```
</LanguageContent>

<LanguageContent lang="python">
Check the `info.supports.tools` property:

    ```python
    # Model support information is available through the plugin
    # Check plugin documentation for specific model capabilities
    ```
</LanguageContent>

### Defining tools

Use the appropriate method for your language to define tools:

<LanguageContent lang="js">
Use the Genkit instance's `defineTool()` function:

    ```ts
    import { genkit, z } from 'genkit';
    import { googleAI } from '@genkit-ai/googleai';

    const ai = genkit({
      plugins: [googleAI()],
      model: googleAI.model('gemini-2.5-flash'),
    });

    const getWeather = ai.defineTool(
      {
        name: 'getWeather',
        description: 'Gets the current weather in a given location',
        inputSchema: z.object({
          location: z.string().describe('The location to get the current weather for'),
        }),
        outputSchema: z.string(),
      },
      async (input) => {
        // Here, we would typically make an API call or database query. For this
        // example, we just return a fixed value.
        return `The current weather in ${input.location} is 63°F and sunny.`;
      },
    );
    ```

    The syntax here looks just like the `defineFlow()` syntax; however, `name`,
    `description`, and `inputSchema` parameters are required. When writing a tool
    definition, take special care with the wording and descriptiveness of these
    parameters. They are vital for the LLM to make effective use of the
    available tools.
</LanguageContent>

<LanguageContent lang="go">
Use the `genkit.DefineTool()` function:

    ```go
    package main

    import (
        "context"
        "fmt"
        "log"

        "github.com/firebase/genkit/go/ai"
        "github.com/firebase/genkit/go/genkit"
        "github.com/firebase/genkit/go/plugins/googlegenai"
    )

    // Define the input structure for the tool
    type WeatherInput struct {
        Location string `json:"location" jsonschema_description:"Location to get weather for"`
    }

    func main() {
        ctx := context.Background()

        g, err := genkit.Init(ctx,
            genkit.WithPlugins(&googlegenai.GoogleAI{}),
            genkit.WithDefaultModel("googleai/gemini-2.5-flash"),
        )
        if err != nil {
            log.Fatalf("Genkit initialization failed: %v", err)
        }

        getWeatherTool := genkit.DefineTool(
            g, "getWeather", "Gets the current weather in a given location",
            func(ctx context.Context, input WeatherInput) (string, error) {
                // Here, we would typically make an API call or database query. For this
                // example, we just return a fixed value.
                log.Printf("Tool 'getWeather' called for location: %s", input.Location)
                return fmt.Sprintf("The current weather in %s is 63°F and sunny.", input.Location), nil
            })
    }
    ```

    The syntax here looks just like the `genkit.DefineFlow()` syntax; however, you
    must write a description. Take special care with the wording and descriptiveness
    of the description as it is vital for the LLM to decide to use it appropriately.
</LanguageContent>

<LanguageContent lang="python">
Use the Genkit instance's `tool()` decorator:

    ```python
    from pydantic import BaseModel, Field
    from genkit.ai import Genkit
    from genkit.plugins.google_genai import GoogleGenai

    ai = Genkit(
        plugins=[GoogleGenai()],
        model='googleai/gemini-2.5-flash',
    )

    class WeatherInput(BaseModel):
        location: str = Field(description='The location to get the current weather for')

    @ai.tool()
    def get_weather(input: WeatherInput) -> str:
        """Gets the current weather in a given location"""
        # Replace with actual weather fetching logic
        return f'The current weather in {input.location} is 63°F and sunny.'
    ```

    The syntax here looks just like the `flow()` syntax; however `description`
    parameter is required. When writing a tool definition, take special care
    with the wording and descriptiveness of these parameters. They are vital
    for the LLM to make effective use of the available tools.
</LanguageContent>

### Using tools

Include defined tools in your prompts to generate content:

<LanguageContent lang="js">
**Using `generate()`:**

    ```ts
    const response = await ai.generate({
      prompt: "What is the weather in Baltimore?",
      tools: [getWeather],
    });
    ```

    **Using `definePrompt()`:**

    ```ts
    const weatherPrompt = ai.definePrompt(
      {
        name: "weatherPrompt",
        tools: [getWeather],
      },
      "What is the weather in {{location}}?"
    );

    const response = await weatherPrompt({ location: "Baltimore" });
    ```

    **Using Prompt files:**

    ```dotprompt
    ---
    tools: [getWeather]
    input:
      schema:
        location: string
    ---

    What is the weather in {{location}}?
    ```

    Then you can execute the prompt in your code as follows:

    ```ts
    // assuming prompt file is named weatherPrompt.prompt
    const weatherPrompt = ai.prompt("weatherPrompt");

    const response = await weatherPrompt({ location: "Baltimore" });
    ```

    **Using Chat:**

    ```ts
    const chat = ai.chat({
      system: "Answer questions using the tools you have.",
      tools: [getWeather],
    });

    const response = await chat.send("What is the weather in Baltimore?");

    // Or, specify tools that are message-specific
    const response = await chat.send({
      prompt: "What is the weather in Baltimore?",
      tools: [getWeather],
    });
    ```
</LanguageContent>

<LanguageContent lang="go">
**Using `genkit.Generate()`:**

    ```go
    resp, err := genkit.Generate(ctx, g,
        ai.WithPrompt("What is the weather in San Francisco?"),
        ai.WithTools(getWeatherTool),
    )
    ```

    **Using `genkit.DefinePrompt()`:**

    ```go
    weatherPrompt, err := genkit.DefinePrompt(g, "weatherPrompt",
        ai.WithPrompt("What is the weather in {{location}}?"),
        ai.WithTools(getWeatherTool),
    )
    if err != nil {
        log.Fatal(err)
    }

    resp, err := weatherPrompt.Execute(ctx,
        ai.WithInput(map[string]any{"location": "San Francisco"}),
    )
    ```

    **Using a `.prompt` file:**

    Create a file named `prompts/weatherPrompt.prompt`:

    ```dotprompt
    ---
    system: "Answer questions using the tools you have."
    tools: [getWeather]
    input:
      schema:
        location: string
    ---

    What is the weather in {{location}}?
    ```

    Then execute it in your Go code:

    ```go
    // Assuming prompt file named weatherPrompt.prompt exists in ./prompts dir.
    weatherPrompt := genkit.LookupPrompt("weatherPrompt")
    if weatherPrompt == nil {
        log.Fatal("no prompt named 'weatherPrompt' found")
    }

    resp, err := weatherPrompt.Execute(ctx,
        ai.WithInput(map[string]any{"location": "San Francisco"}),
    )
    ```
</LanguageContent>

<LanguageContent lang="python">
**Using `generate()`:**

    ```python
    result = await ai.generate(
        prompt='What is the weather in Baltimore?',
        tools=['get_weather'],
    )
    ```

    **Using flows with tools:**

    ```python
    @ai.flow()
    async def weather_flow(location: str):
        result = await ai.generate(
            prompt=f'What is the weather in {location}?',
            tools=['get_weather'],
        )
        return result.text
    ```
</LanguageContent>

Genkit will automatically handle the tool call if the LLM needs to use the tool to answer the prompt.

### Streaming and Tool Calling

When combining tool calling with streaming responses, you will receive `toolRequest` and `toolResponse` content parts in the chunks of the stream:

<LanguageContent lang="js">
```ts
    const { stream } = ai.generateStream({
      prompt: "What is the weather in Baltimore?",
      tools: [getWeather],
    });

    for await (const chunk of stream) {
      console.log(chunk);
    }
    ```

    This might produce a sequence of chunks similar to:

    ```ts
    {index: 0, role: "model", content: [{text: "Okay, I'll check the weather"}]}
    {index: 0, role: "model", content: [{text: "for Baltimore."}]}
    // toolRequests will be emitted as a single chunk by most models
    {index: 0, role: "model", content: [{toolRequest: {name: "getWeather", input: {location: "Baltimore"}}}]}
    // when streaming multiple messages, Genkit increments the index and indicates the new role
    {index: 1, role: "tool", content: [{toolResponse: {name: "getWeather", output: "Temperature: 68 degrees\nStatus: Cloudy."}}]}
    {index: 2, role: "model", content: [{text: "The weather in Baltimore is 68 degrees and cloudy."}]}
    ```

    You can use these chunks to dynamically construct the full generated message sequence.
</LanguageContent>

<LanguageContent lang="go">
```go
    resp, err := genkit.Generate(ctx, g,
        ai.WithPrompt("What is the weather in San Francisco?"),
        ai.WithTools(getWeatherTool),
        ai.WithStreaming(func(ctx context.Context, chunk *ai.ModelResponseChunk) error {
            // Handle streaming chunks here
            log.Println("Chunk:", chunk.Text())
            return nil
        }),
    )
    ```
</LanguageContent>

<LanguageContent lang="python">
```python
    stream, response = ai.generate_stream(
        prompt='What is the weather in Baltimore?',
        tools=['get_weather'],
    )

    async for chunk in stream:
        print(chunk)
    ```
</LanguageContent>

### Limiting Tool Call Iterations with `maxTurns`

When working with tools that might trigger multiple sequential calls, you can control resource usage and prevent runaway execution using the `maxTurns` parameter. This sets a hard limit on how many back-and-forth interactions the model can have with your tools in a single generation cycle.

**Why use maxTurns?**
- **Cost Control**: Prevents unexpected API usage charges from excessive tool calls
- **Performance**: Ensures responses complete within reasonable timeframes
- **Safety**: Guards against infinite loops in complex tool interactions
- **Predictability**: Makes your application behavior more deterministic

The default value is 5 turns, which works well for most scenarios. Each "turn" represents one complete cycle where the model can make tool calls and receive responses.

<LanguageContent lang="js">
**Example: Web Research Agent**

    Consider a research agent that might need to search multiple times to find comprehensive information:

    ```ts
    const webSearch = ai.defineTool(
      {
        name: 'webSearch',
        description: 'Search the web for current information',
        inputSchema: z.object({
          query: z.string().describe('Search query'),
        }),
        outputSchema: z.string(),
      },
      async (input) => {
        // Simulate web search API call
        return `Search results for "${input.query}": [relevant information here]`;
      },
    );

    const response = await ai.generate({
      prompt: 'Research the latest developments in quantum computing, including recent breakthroughs, key companies, and future applications.',
      tools: [webSearch],
      maxTurns: 8, // Allow up to 8 research iterations
    });
    ```

    **Example: Financial Calculator**

    ```ts
    const calculator = ai.defineTool(
      {
        name: 'calculator',
        description: 'Perform mathematical calculations',
        inputSchema: z.object({
          expression: z.string().describe('Mathematical expression to evaluate'),
        }),
        outputSchema: z.number(),
      },
      async (input) => {
        // Safe evaluation of mathematical expressions
        return eval(input.expression); // In production, use a safe math parser
      },
    );

    const response = await ai.generate({
      prompt: 'Calculate the total value of my portfolio: 100 shares of AAPL, 50 shares of GOOGL, and 200 shares of MSFT. Also calculate what percentage each holding represents.',
      tools: [calculator, stockAnalyzer],
      maxTurns: 12, // Multiple stock lookups + calculations needed
    });
    ```
</LanguageContent>

<LanguageContent lang="go">
```go
    resp, err := genkit.Generate(ctx, g,
        ai.WithPrompt("Research the latest developments in quantum computing"),
        ai.WithTools(webSearchTool),
        ai.WithMaxTurns(8), // Allow up to 8 research iterations
    )
    ```
</LanguageContent>

<LanguageContent lang="python">
```python
    result = await ai.generate(
        prompt='Research the latest developments in quantum computing',
        tools=['web_search'],
        max_turns=8,  # Allow up to 8 research iterations
    )
    ```
</LanguageContent>

**What happens when maxTurns is reached?**

When the limit is hit, Genkit stops the tool-calling loop and returns the model's current response, even if it was in the middle of using tools. The model will typically provide a partial answer or explain that it couldn't complete all the requested operations.

### Dynamically defining tools at runtime

<LanguageContent lang="js">
As most things in Genkit tools need to be predefined during your app's
    initialization. This is necessary so that you would be able interact with your
    tools from the Genkit Dev UI. This is typically the recommended way. However
    there are scenarios when the tool must be defined dynamically per user request.

    You can dynamically define tools using `ai.dynamicTool` function. It is very
    similar to `ai.defineTool` method, however dynamic tools are not tracked by
    Genkit runtime, so cannot be interacted with from Genkit Dev UI and must be
    passed to the `ai.generate` call by reference (for regular tools you can also
    use a string tool name).

    ```ts
    import { genkit, z } from 'genkit';
    import { googleAI } from '@genkit-ai/googleai';

    const ai = genkit({
      plugins: [googleAI()],
      model: googleAI.model('gemini-2.5-flash'),
    });

    ai.defineFlow('weatherFlow', async () => {
      const getWeather = ai.dynamicTool(
        {
          name: 'getWeather',
          description: 'Gets the current weather in a given location',
          inputSchema: z.object({
            location: z.string().describe('The location to get the current weather for'),
          }),
          outputSchema: z.string(),
        },
        async (input) => {
          return `The current weather in ${input.location} is 63°F and sunny.`;
        },
      );

      const { text } = await ai.generate({
        prompt: 'What is the weather in Baltimore?',
        tools: [getWeather],
      });

      return text;
    });
    ```

    When defining dynamic tools, to specify input and output schemas you can either
    use Zod as shown in the previous example, or you can pass in manually
    constructed JSON Schema.

    ```ts
    const getWeather = ai.dynamicTool(
      {
        name: 'getWeather',
        description: 'Gets the current weather in a given location',
        inputJsonSchema: myInputJsonSchema,
        outputJsonSchema: myOutputJsonSchema,
      },
      async (input) => {
        /* ... */
      },
    );
    ```

    Dynamic tools don't require the implementation function. If you don't pass in
    the function the tool will behave like an [interrupt](/docs/interrupts) and you can
    do manual tool call handling:

    ```ts
    const getWeather = ai.dynamicTool({
      name: 'getWeather',
      description: 'Gets the current weather in a given location',
      inputJsonSchema: myInputJsonSchema,
      outputJsonSchema: myOutputJsonSchema,
    });
    ```
</LanguageContent>

<LanguageContent lang="go">
```go
    // Dynamic tool definition in Go
    // Check Go documentation for specific implementation details
    ```
</LanguageContent>

<LanguageContent lang="python">
```python
    # Dynamic tool definition in Python
    # Check Python documentation for specific implementation details
    ```
</LanguageContent>

### Pause the tool loop by using interrupts

By default, Genkit repeatedly calls the LLM until every tool call has been
resolved. You can conditionally pause execution in situations where you want
to, for example:

- Ask the user a question or display UI.
- Confirm a potentially risky action with the user.
- Request out-of-band approval for an action.

**Interrupts** are special tools that can halt the loop and return control
to your code so that you can handle more advanced scenarios. Visit the
interrupts guide to learn how to use them.

### Explicitly handling tool calls

If you want full control over this tool-calling loop, for example to
apply more complicated logic, you can handle tool calls explicitly:

<LanguageContent lang="js">
Set the `returnToolRequests` parameter to `true`. Now it's your responsibility to ensure all of the tool requests are fulfilled:

    ```ts
    const getWeather = ai.defineTool(
      {
        // ... tool definition ...
      },
      async ({ location }) => {
        // ... tool implementation ...
      },
    );

    const generateOptions: GenerateOptions = {
      prompt: "What's the weather like in Baltimore?",
      tools: [getWeather],
      returnToolRequests: true,
    };

    let llmResponse;
    while (true) {
      llmResponse = await ai.generate(generateOptions);
      const toolRequests = llmResponse.toolRequests;
      if (toolRequests.length < 1) {
        break;
      }
      const toolResponses: ToolResponsePart[] = await Promise.all(
        toolRequests.map(async (part) => {
          switch (part.toolRequest.name) {
            case 'getWeather':
              return {
                toolResponse: {
                  name: part.toolRequest.name,
                  ref: part.toolRequest.ref,
                  output: await getWeather(part.toolRequest.input),
                },
              };
            default:
              throw Error('Tool not found');
          }
        }),
      );
      generateOptions.messages = llmResponse.messages;
      generateOptions.prompt = toolResponses;
    }
    ```
</LanguageContent>

<LanguageContent lang="go">
Set the `WithReturnToolRequests()` option to `true`. Now it's your responsibility to ensure all of the tool requests are fulfilled:

    ```go
    getWeatherTool := genkit.DefineTool(
        g, "getWeather", "Gets the current weather in a given location",
        func(ctx context.Context, location WeatherInput) (string, error) {
            // Tool implementation...
            return "sunny", nil
        },
    )

    resp, err := genkit.Generate(ctx, g,
        ai.WithPrompt("What is the weather in San Francisco?"),
        ai.WithTools(getWeatherTool),
        ai.WithReturnToolRequests(true),
    )
    if err != nil {
        log.Fatal(err)
    }

    parts := []*ai.Part{}
    for _, req := range resp.ToolRequests() {
        tool := genkit.LookupTool(g, req.Name)
        if tool == nil {
            log.Fatalf("tool %q not found", req.Name)
        }

        output, err := tool.RunRaw(ctx, req.Input)
        if err != nil {
            log.Fatalf("tool %q execution failed: %v", tool.Name(), err)
        }

        parts = append(parts,
            ai.NewToolResponsePart(&ai.ToolResponse{
                Name:   req.Name,
                Ref:    req.Ref,
                Output: output,
            }))
    }

    resp, err = genkit.Generate(ctx, g,
        ai.WithMessages(append(resp.History(), ai.NewMessage(ai.RoleTool, nil, parts...))...),
    )
    if err != nil {
        log.Fatal(err)
    }
    ```
</LanguageContent>

<LanguageContent lang="python">
Set the `return_tool_requests` parameter to `True`. Now it's your responsibility to ensure all of the tool requests are fulfilled:

    ```python
    llm_response = await ai.generate(
        prompt='What is the weather in Baltimore?',
        tools=['get_weather'],
        return_tool_requests=True,
    )

    tool_request_parts = llm_response.tool_requests

    if len(tool_request_parts) == 0:
        print(llm_response.text)
    else:
        for part in tool_request_parts:
            await handle_tool(part.name, part.input)
    ```
</LanguageContent>

## Extending Tool Capabilities with MCP

The [Model Context Protocol (MCP)](/unified-docs/model-context-protocol) provides a powerful way to extend your tool-calling capabilities by connecting to external MCP servers. With MCP, you can:

- **Access pre-built tools** from the MCP ecosystem without implementing them yourself
- **Connect to external services** like databases, APIs, and file systems
- **Share tools** between different AI applications
- **Build extensible workflows** that leverage community-maintained tools

MCP tools work seamlessly with Genkit's tool-calling system, allowing you to mix custom tools with external MCP tools in the same generation request.

## Next steps

- Learn about [Model Context Protocol (MCP)](/unified-docs/model-context-protocol) to extend your tool capabilities with external servers
- Explore [interrupts](/docs/interrupts) to pause tool execution for user interaction
- See [retrieval-augmented generation (RAG)](/unified-docs/rag) for handling large amounts of contextual information
- Check out [multi-agent systems](/docs/multi-agent) for coordinating multiple AI agents with tools
- Browse the [tool calling example](https://github.com/firebase/genkit/tree/main/js/testapps/tool-calling) for a complete implementation
