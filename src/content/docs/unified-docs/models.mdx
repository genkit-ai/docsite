---
title: Generating content with AI models
description: Learn how to generate content with AI models using Genkit's unified interface across JavaScript, Go, and Python, covering basic usage, configuration, structured output, streaming, and multimodal input/output.
---

import LLMSummary from '@/components/llm-summary.astro';
import ExampleLink from '@/components/ExampleLink.astro';
import LanguageSelector from '../../../components/LanguageSelector.astro';
import CopyMarkdownButton from '../../../components/CopyMarkdownButton.astro';
import LanguageContent from '../../../components/LanguageContent.astro';

<div style="display: flex; justify-content: space-between; align-items: center; gap: 1rem; margin: 1rem 0 1rem 0;">
  <LanguageSelector />
  <CopyMarkdownButton />
</div>

<LLMSummary>
Genkit provides a unified interface to interact with various generative AI models (LLMs, image generation) across JavaScript, Go, and Python.

**Core Function:** `ai.generate()` (JS), `genkit.Generate()` (Go), `ai.generate()` (Python)

**Basic Usage:**

<LanguageContent lang="js">
```typescript
import { googleAI } from '@genkit-ai/googleai';
import { genkit } from 'genkit';

const ai = genkit({
  plugins: [googleAI()],
  model: googleAI.model('gemini-2.5-flash'), // Default model
});

// Generate with default model
const response1 = await ai.generate('prompt text');
console.log(response1.text);

// Generate with specific model reference
const response2 = await ai.generate({
  model: googleAI.model('gemini-2.5-flash'),
  prompt: 'prompt text',
});
console.log(response2.text);

// Generate with model string ID
const response3 = await ai.generate({
  model: 'googleai/gemini-2.5-flash',
  prompt: 'prompt text',
});
console.log(response3.text);
```
</LanguageContent>

<LanguageContent lang="go">
```go
import (
    "context"
    "log"
    "github.com/firebase/genkit/go/ai"
    "github.com/firebase/genkit/go/genkit"
    "github.com/firebase/genkit/go/plugins/googlegenai"
)

func main() {
    ctx := context.Background()
    g, err := genkit.Init(ctx,
        genkit.WithPlugins(&googlegenai.GoogleAI{}),
        genkit.WithDefaultModel("googleai/gemini-2.5-flash"),
    )
    if err != nil {
        log.Fatalf("could not initialize Genkit: %v", err)
    }

    // Generate with default model
    resp1, err := genkit.Generate(ctx, g,
        ai.WithPrompt("prompt text"),
    )
    if err != nil {
        log.Fatalf("could not generate: %v", err)
    }
    log.Println(resp1.Text())

    // Generate with specific model
    resp2, err := genkit.Generate(ctx, g,
        ai.WithModelName("googleai/gemini-2.5-flash"),
        ai.WithPrompt("prompt text"),
    )
    if err != nil {
        log.Fatalf("could not generate: %v", err)
    }
    log.Println(resp2.Text())
}
```
</LanguageContent>

<LanguageContent lang="python">
```python
import asyncio
from genkit.ai import Genkit
from genkit.plugins.google_genai import GoogleGenai

ai = Genkit(
    plugins=[GoogleGenai()],
    model='googleai/gemini-2.5-flash',
)

async def main() -> None:
    # Generate with default model
    result1 = await ai.generate(
        prompt='prompt text',
    )
    print(result1.text)

    # Generate with specific model
    result2 = await ai.generate(
        prompt='prompt text',
        model='googleai/gemini-2.5-flash',
    )
    print(result2.text)

ai.run_main(main())
```
</LanguageContent>

**Configuration:**

- **System Prompt:** `system: "Instruction for the model"`
- **Model Parameters:** `config: { maxOutputTokens: 512, temperature: 1.0, topP: 0.95, topK: 40, stopSequences: ["\n"] }`

**Key Concepts:**

- **Flexibility:** Easily swap models (`model` parameter).
- **Schema validation:** For defining and validating structured output schemas.
- **Streaming:** For real-time output using `generateStream`.
- **Multimodality:** Handle text, image, video, audio inputs (model-dependent).
- **Media Generation:** Create images, etc. (model-dependent).

</LLMSummary>

Genkit provides a unified interface for working with AI models across different providers. This allows you to easily switch between models or use multiple models in your application without changing your code structure.

### Before you begin

If you want to run the code examples on this page, first complete the steps in
the Getting started guide for your language. All of the examples assume that you
have already installed Genkit as a dependency in your project.

Complete the [Getting started](/unified-docs/get-started) guide.

### Supported AI providers

Genkit supports many AI providers including Google AI, OpenAI, Anthropic, xAI, DeepSeek, and Ollama. See the [AI Provider Overview](/unified-docs/plugins/overview) for a complete list of officially supported and community providers across all languages.

### Setup

Before using `generate()`, you need to configure an AI provider plugin. See the [Getting Started guide](/unified-docs/get-started) for setup instructions.

### The generate() method

In Genkit, the primary interface through which you interact with generative AI
models is the `generate()` method.

The simplest `generate()` call specifies the model you want to use and a text
prompt:

<LanguageContent lang="js">
```ts
import { googleAI } from '@genkit-ai/googleai';
import { genkit } from 'genkit';

const ai = genkit({
  plugins: [googleAI()],
  // Optional. Specify a default model.
  model: googleAI.model('gemini-2.5-flash'),
});

async function run() {
  const response = await ai.generate('Invent a menu item for a restaurant with a pirate theme.');
  console.log(response.text);
}

run();
```
</LanguageContent>

<LanguageContent lang="go">
```go
package main

import (
    "context"
    "log"

    "github.com/firebase/genkit/go/ai"
    "github.com/firebase/genkit/go/genkit"
    "github.com/firebase/genkit/go/plugins/googlegenai"
)

func main() {
    ctx := context.Background()

    g, err := genkit.Init(ctx,
        genkit.WithPlugins(&googlegenai.GoogleAI{}),
        genkit.WithDefaultModel("googleai/gemini-2.5-flash"),
    )
    if err != nil {
        log.Fatalf("could not initialize Genkit: %v", err)
    }

    resp, err := genkit.Generate(ctx, g,
        ai.WithPrompt("Invent a menu item for a pirate themed restaurant."),
    )
    if err != nil {
        log.Fatalf("could not generate model response: %v", err)
    }

    log.Println(resp.Text())
}
```
</LanguageContent>

<LanguageContent lang="python">
```python
import asyncio
from genkit.ai import Genkit
from genkit.plugins.google_genai import GoogleGenai

ai = Genkit(
    plugins=[GoogleGenai()],
    model='googleai/gemini-2.5-flash',
)

async def main() -> None:
    result = await ai.generate(
        prompt='Invent a menu item for a pirate themed restaurant.',
    )
    print(result.text)

ai.run_main(main())
```
</LanguageContent>

When you run this brief example, it will print out some debugging information
followed by the output of the `generate()` call, which will usually be Markdown
text as in the following example:

```md
## The Blackheart's Bounty

**A hearty stew of slow-cooked beef, spiced with rum and molasses, served in a
hollowed-out cannonball with a side of crusty bread and a dollop of tangy
pineapple salsa.**

**Description:** This dish is a tribute to the hearty meals enjoyed by pirates
on the high seas. The beef is tender and flavorful, infused with the warm spices
of rum and molasses. The pineapple salsa adds a touch of sweetness and acidity,
balancing the richness of the stew. The cannonball serving vessel adds a fun and
thematic touch, making this dish a perfect choice for any pirate-themed
adventure.
```

Run the script again and you'll get a different output.

The preceding code sample sent the generation request to the default model,
which you specified when you configured the Genkit instance.

You can also specify a model for a single `generate()` call:

<LanguageContent lang="js">
```ts
import { googleAI } from '@genkit-ai/googleai';

const response = await ai.generate({
  model: googleAI.model('gemini-2.5-flash'),
  prompt: 'Invent a menu item for a restaurant with a pirate theme.',
});
```

This example uses a model reference function provided by the model plugin. You can also specify the model using a string identifier:

```ts
const response = await ai.generate({
  model: 'googleai/gemini-2.5-flash-001',
  prompt: 'Invent a menu item for a restaurant with a pirate theme.',
});
```
</LanguageContent>

<LanguageContent lang="go">
```go
resp, err := genkit.Generate(ctx, g,
    ai.WithModelName("googleai/gemini-2.5-pro"),
    ai.WithPrompt("Invent a menu item for a pirate themed restaurant."),
)
```
</LanguageContent>

<LanguageContent lang="python">
```python
result = await ai.generate(
    prompt='Invent a menu item for a pirate themed restaurant.',
    model='googleai/gemini-2.0-pro',
)
```
</LanguageContent>

A model string identifier looks like `providerid/modelid`, where the provider ID
(in this case, `googleai`) identifies the plugin, and the model ID is a
plugin-specific string identifier for a specific version of a model.

These examples also illustrate an important point: when you use
`generate()` to make generative AI model calls, changing the model you want to
use is simply a matter of passing a different value to the model parameter. By
using `generate()` instead of the native model SDKs, you give yourself the
flexibility to more easily use several different models in your app and change
models in the future.

So far you have only seen examples of the simplest `generate()` calls. However,
`generate()` also provides an interface for more advanced interactions with
generative models, which you will see in the sections that follow.

### System prompts

Some models support providing a _system prompt_, which gives the model
instructions as to how you want it to respond to messages from the user. You can
use the system prompt to specify a persona you want the model to adopt, the tone
of its responses, the format of its responses, and so on.

If the model you're using supports system prompts, you can provide one:

<LanguageContent lang="js">
```ts
const response = await ai.generate({
  prompt: 'What is your quest?',
  system: "You are a knight from Monty Python's Flying Circus.",
});
```
</LanguageContent>

<LanguageContent lang="go">
```go
resp, err := genkit.Generate(ctx, g,
    ai.WithSystem("You are a food industry marketing consultant."),
    ai.WithPrompt("Invent a menu item for a pirate themed restaurant."),
)
```

For models that don't support system prompts, `ai.WithSystem()` simulates it by
modifying the request to appear _like_ a system prompt.
</LanguageContent>

<LanguageContent lang="python">
```python
result = await ai.generate(
    system='You are a food industry marketing consultant.',
    prompt='Invent a menu item for a pirate themed restaurant.',
)
```
</LanguageContent>

### Multi-turn conversations with messages

For multi-turn conversations, you can use the `messages` parameter instead of `prompt` to provide a conversation history. This is particularly useful when you need to maintain context across multiple interactions with the model.

<LanguageContent lang="js">
The `messages` parameter accepts an array of message objects, where each message has a `role` (one of `'system'`, `'user'`, `'model'`, or `'tool'`) and `content`:

```ts
const response = await ai.generate({
  messages: [
    { role: 'user', content: 'Hello, can you help me plan a trip?' },
    { role: 'model', content: 'Of course! I\'d be happy to help you plan a trip. Where are you thinking of going?' },
    { role: 'user', content: 'I want to visit Japan for two weeks in spring.' }
  ],
});
```

You can also combine `messages` with other parameters like `system` prompts:

```ts
const response = await ai.generate({
  system: 'You are a helpful travel assistant.',
  messages: [
    { role: 'user', content: 'What should I pack for Japan in spring?' }
  ],
});
```

**When to use `messages` vs. Chat API:**

- Use the `messages` parameter for simple multi-turn conversations where you manually manage the conversation history
- For persistent chat sessions with automatic history management, use the [Chat API](/unified-docs/chat) instead
</LanguageContent>

<LanguageContent lang="go">
```go
import "github.com/firebase/genkit/go/ai"

resp, err := genkit.Generate(ctx, g,
    ai.WithModelName("googleai/gemini-2.5-flash"),
    ai.WithMessages(
        ai.NewUserMessage(
            ai.NewTextPart("Hello, can you help me plan a trip?"),
        ),
        ai.NewModelMessage(
            ai.NewTextPart("Of course! I'd be happy to help you plan a trip. Where are you thinking of going?"),
        ),
        ai.NewUserMessage(
            ai.NewTextPart("I want to visit Japan for two weeks in spring."),
        ),
    ),
)
```

You can also combine messages with system prompts:

```go
resp, err := genkit.Generate(ctx, g,
    ai.WithSystem("You are a helpful travel assistant."),
    ai.WithMessages(
        ai.NewUserMessage(
            ai.NewTextPart("What should I pack for Japan in spring?"),
        ),
    ),
)
```
</LanguageContent>

<LanguageContent lang="python">
```python
from genkit.ai import Message

result = await ai.generate(
    messages=[
        Message(role='user', content='Hello, can you help me plan a trip?'),
        Message(role='model', content='Of course! I\'d be happy to help you plan a trip. Where are you thinking of going?'),
        Message(role='user', content='I want to visit Japan for two weeks in spring.'),
    ],
)
```

You can also combine messages with system prompts:

```python
result = await ai.generate(
    system='You are a helpful travel assistant.',
    messages=[
        Message(role='user', content='What should I pack for Japan in spring?'),
    ],
)
```

**When to use `messages` vs. Chat API:**

- Use the `messages` parameter for simple multi-turn conversations where you manually manage the conversation history
- For persistent chat sessions with automatic history management, use the [Chat API](/unified-docs/chat) instead
</LanguageContent>

### Model parameters

The `generate()` function takes a `config` parameter, through which you can
specify optional settings that control how the model generates content:

<LanguageContent lang="js">
```ts
const response = await ai.generate({
  prompt: 'Invent a menu item for a restaurant with a pirate theme.',
  config: {
    maxOutputTokens: 512,
    stopSequences: ['\n'],
    temperature: 1.0,
    topP: 0.95,
    topK: 40,
  },
});
```
</LanguageContent>

<LanguageContent lang="go">
```go
resp, err := genkit.Generate(ctx, g,
    ai.WithModelName("googleai/gemini-2.5-flash"),
    ai.WithPrompt("Invent a menu item for a pirate themed restaurant."),
    ai.WithConfig(&googlegenai.GeminiConfig{
        MaxOutputTokens: 500,
        StopSequences:   ["<end>", "<fin>"],
        Temperature:     0.5,
        TopP:            0.4,
        TopK:            50,
    }),
)
```
</LanguageContent>

<LanguageContent lang="python">
```python
result = await ai.generate(
    prompt='Invent a menu item for a pirate themed restaurant.',
    config={
      'max_output_tokens': 400,
      'stop_sequences': ['<end>', '<fin>'],
      'temperature': 1.2,
      'top_p': 0.4,
      'top_k': 50,
    },
)
```
</LanguageContent>

Common parameters supported by most models:

- **maxOutputTokens**: Maximum number of tokens to generate (roughly 2-4 tokens per English word)
- **stopSequences**: Text sequences that stop generation when encountered
- **temperature**: Controls randomness (0.0 = deterministic, higher = more creative)
- **topP**: Controls diversity by probability mass (0.0-1.0)
- **topK**: Controls diversity by limiting token choices (integer)

Use the Developer UI (`genkit start`) to experiment with different parameter values.

### Structured output

<ExampleLink
  title="Structured Output"
  description="View a live example of using structured output to generate a D&D character sheet."
  example="structured-output"
/>

Request structured output from models by specifying a schema:

<LanguageContent lang="js">
```ts
import { z } from 'genkit';

const MenuItemSchema = z.object({
  name: z.string().describe('The name of the menu item.'),
  description: z.string().describe('A description of the menu item.'),
  calories: z.number().describe('The estimated number of calories.'),
  allergens: z.array(z.string()).describe('Any known allergens in the menu item.'),
});

const response = await ai.generate({
  prompt: 'Suggest a menu item for a pirate-themed restaurant.',
  output: { schema: MenuItemSchema },
});
```

Model output schemas are specified using the [Zod](https://zod.dev/)
library. In addition to a schema definition language, Zod also provides runtime
type checking, which bridges the gap between static TypeScript types and the
unpredictable output of generative AI models.
</LanguageContent>

<LanguageContent lang="go">
```go
type MenuItem struct {
    Name        string   `json:"name"`
    Description string   `json:"description"`
    Calories    int      `json:"calories"`
    Allergens   []string `json:"allergens"`
}

resp, err := genkit.Generate(ctx, g,
    ai.WithPrompt("Invent a menu item for a pirate themed restaurant."),
    ai.WithOutputType(MenuItem{}),
)
if err != nil {
    log.Fatal(err) // One possible error is that the response does not conform to the type.
}
```

Model output types are specified as JSON schema using the
[`invopop/jsonschema`](https://github.com/invopop/jsonschema) package. This
provides runtime type checking, which bridges the gap between static Go types
and the unpredictable output of generative AI models.
</LanguageContent>

<LanguageContent lang="python">
```python
from pydantic import BaseModel

class MenuItemSchema(BaseModel):
    name: str
    description: str
    calories: int
    allergens: list[str]

result = await ai.generate(
    prompt='Invent a menu item for a pirate themed restaurant.',
    output_schema=MenuItemSchema,
)
```

Model output schemas are specified using [Pydantic Models](https://docs.pydantic.dev/latest/concepts/models/). In addition to a schema definition language, Pydantic also provides runtime
type checking, which bridges the gap between static Python types and the
unpredictable output of generative AI models.
</LanguageContent>

When you specify a schema in `generate()`, Genkit does several things behind the
scenes:

- Augments the prompt with additional guidance about the desired output format.
  This also has the side effect of specifying to the model what content exactly
  you want to generate (for example, not only suggest a menu item but also
  generate a description, a list of allergens, and so on).
- Parses the model output into a structured object.
- Verifies that the output conforms with the schema.

To get structured output from a successful generate call, use the response
object's `output` property:

<LanguageContent lang="js">
```ts
const menuItem = response.output; // Typed as z.infer<typeof MenuItemSchema>
console.log(menuItem?.name);
```

Note that the `output` property can be `null`. This can
happen when the model fails to generate output that conforms to the schema.
</LanguageContent>

<LanguageContent lang="go">
```go
var item MenuItem
if err := resp.Output(&item); err != nil {
    log.Fatalf(err)
}

log.Printf("%s (%d calories, %d allergens): %s\n",
    item.Name, item.Calories, len(item.Allergens), item.Description)
```

Alternatively, you can use `genkit.GenerateData()` for a more succinct call:

```go
item, resp, err := genkit.GenerateData[MenuItem](ctx, g,
    ai.WithPrompt("Invent a menu item for a pirate themed restaurant."),
)
if err != nil {
    log.Fatal(err)
}
```
</LanguageContent>

<LanguageContent lang="python">
```python
output = response.output
```
</LanguageContent>

#### Handling errors

The best strategy for dealing with schema validation errors will depend on your exact use
case, but here are some general hints:

- **Try a different model**. For structured output to succeed, the model must be
  capable of generating output in JSON. The most powerful LLMs, like Gemini and
  Claude, are versatile enough to do this; however, smaller models, such as some
  of the local models you would use with Ollama, might not be able to generate
  structured output reliably unless they have been specifically trained to do
  so.

- **Make use of coercion abilities**: You can specify in your schemas that
  the validation library should try to coerce non-conforming types into the type specified by the
  schema. If your schema includes primitive types other than strings, using
  coercion can reduce the number of `generate()` failures you experience.

- **Retry the generate() call**. If the model you've chosen only rarely fails to
  generate conformant output, you can treat the error as you would treat a
  network error, and simply retry the request using some kind of incremental
  back-off strategy.

### Streaming

When generating large amounts of text, you can improve the experience for your
users by presenting the output as it's generated&mdash;streaming the output. A
familiar example of streaming in action can be seen in most LLM chat apps: users
can read the model's response to their message as it's being generated, which
improves the perceived responsiveness of the application and enhances the
illusion of chatting with an intelligent counterpart.

In Genkit, you can stream output using the streaming methods:

<LanguageContent lang="js">
```ts
const { stream, response } = ai.generateStream({
  prompt: 'Tell a story.',
});

// Stream text chunks
for await (const chunk of stream) {
  console.log(chunk.text);
}

// Get final complete response
const finalResponse = await response;
console.log(finalResponse.text);
```

Streaming also works with structured output:

```ts
const { stream, response } = ai.generateStream({
  prompt: 'Suggest three pirate-themed menu items.',
  output: { schema: z.array(MenuItemSchema) },
});

for await (const chunk of stream) {
  console.log(chunk.output); // Accumulated output so far
}

const finalResponse = await response;
console.log(finalResponse.output);
```
</LanguageContent>

<LanguageContent lang="go">
```go
resp, err := genkit.Generate(ctx, g,
    ai.WithPrompt("Tell a story."),
    ai.WithStreaming(func(ctx context.Context, chunk *ai.ModelResponseChunk) error {
        // Stream text chunks
        log.Println(chunk.Text())
        return nil
    }),
)
if err != nil {
    log.Fatal(err)
}

// Get final complete response
log.Println(resp.Text())
```

Streaming also works with structured output:

```go
type MenuItems []MenuItem

resp, err := genkit.Generate(ctx, g,
    ai.WithPrompt("Suggest three pirate-themed menu items."),
    ai.WithOutputType(MenuItems{}),
    ai.WithStreaming(func(ctx context.Context, chunk *ai.ModelResponseChunk) error {
        // Accumulated output so far
        var items MenuItems
        if err := chunk.Output(&items); err == nil {
            log.Printf("Current items: %+v", items)
        }
        return nil
    }),
)
if err != nil {
    log.Fatal(err)
}

var finalItems MenuItems
resp.Output(&finalItems)
log.Printf("Final items: %+v", finalItems)
```
</LanguageContent>

<LanguageContent lang="python">
```python
stream, response = ai.generate_stream(
    prompt='Tell a story.',
)

# Stream text chunks
async for chunk in stream:
    print(chunk.text)

# Get final complete response
final_response = await response
print(final_response.text)
```

Streaming also works with structured output:

```python
from typing import List

class MenuItems(BaseModel):
    items: List[MenuItemSchema]

stream, response = ai.generate_stream(
    prompt='Suggest three pirate-themed menu items.',
    output_schema=MenuItems,
)

async for chunk in stream:
    print(chunk.output)  # Accumulated output so far

final_response = await response
print(final_response.output)
```
</LanguageContent>

Streaming structured output works a little differently from streaming text: the
`output` property of a response chunk is an object constructed from the
accumulation of the chunks that have been produced so far, rather than an object
representing a single chunk (which might not be valid on its own). **Every chunk
of structured output in a sense supersedes the chunk that came before it**.

### Multimodal input

<ExampleLink
  title="Image Analysis"
  description="See a live demo of how Genkit can enable image analysis using multimodal input."
  example="image-analysis"
/>

The examples you've seen so far have used text strings as model prompts. While
this remains the most common way to prompt generative AI models, many models can
also accept other media as prompts. Media prompts are most often used in
conjunction with text prompts that instruct the model to perform some operation
on the media, such as to caption an image or transcribe an audio recording.

The ability to accept media input and the types of media you can use are
completely dependent on the model and its API. For example, the Gemini 1.5
series of models can accept images, video, and audio as prompts.

To provide a media prompt to a model that supports it, instead of passing a
simple text prompt to `generate`, pass an array consisting of a media part and a
text part:

<LanguageContent lang="js">
```ts
const response = await ai.generate({
  prompt: [{ media: { url: 'https://.../image.jpg' } }, { text: 'What is in this image?' }],
});
```

You can also pass media data directly by encoding it as a data URL:

```ts
import { readFile } from 'node:fs/promises';

const data = await readFile('image.jpg');
const response = await ai.generate({
  prompt: [{ media: { url: `data:image/jpeg;base64,${data.toString('base64')}` } }, { text: 'What is in this image?' }],
});
```
</LanguageContent>

<LanguageContent lang="go">
```go
import (
    "encoding/base64"
    "io/ioutil"
)

resp, err := genkit.Generate(ctx, g,
    ai.WithModelName("googleai/gemini-2.5-flash"),
    ai.WithMessages(
        ai.NewUserMessage(
            ai.NewMediaPart("image/jpeg", "https://example.com/photo.jpg"),
            ai.NewTextPart("What is in this image?"),
        ),
    ),
)
```

You can also pass media data directly by encoding it as a data URL:

```go
image, err := ioutil.ReadFile("image.jpg")
if err != nil {
    log.Fatal(err)
}

resp, err := genkit.Generate(ctx, g,
    ai.WithModelName("googleai/gemini-2.5-flash"),
    ai.WithMessages(
        ai.NewUserMessage(
            ai.NewMediaPart("image/jpeg", "data:image/jpeg;base64,"+base64.StdEncoding.EncodeToString(image)),
            ai.NewTextPart("What is in this image?"),
        ),
    ),
)
```
</LanguageContent>

<LanguageContent lang="python">
```python
from genkit.ai import Part

result = await ai.generate(
    prompt=[
        Part(media={'url': 'https://example.com/photo.jpg'}),
        Part(text='What is in this image?'),
    ],
)
```

You can also pass media data directly by encoding it as a data URL:

```python
import base64
from genkit.ai import Part

# Read image bytes
with open('image.jpg', 'rb') as f:
    image_bytes = f.read()

base64_encoded_image = base64.b64encode(image_bytes).decode('utf-8')

result = await ai.generate(
    prompt=[
        Part(media={'url': f'data:image/jpeg;base64,{base64_encoded_image}'}),
        Part(text='What is in this image?'),
    ],
)
```
</LanguageContent>

All models that support media input support both data URLs and HTTPS URLs. Some
model plugins add support for other media sources. For example, the Vertex AI
plugin also lets you use Cloud Storage (`gs://`) URLs.

### Generating Media

While most examples in this guide focus on generating text with LLMs, Genkit also supports generating other types of media, including **images** and **audio**. Thanks to its unified `generate()` interface, working with media models is just as straightforward as generating text.

:::note
Genkit returns generated media as a **data URL**, a widely supported format for handling binary media in both browsers and Node.js environments.
:::

#### Image Generation

To generate an image, you can use models that support image generation. Here's an example using Google AI's image generation capabilities:

<LanguageContent lang="js">
```ts
import { googleAI } from '@genkit-ai/googleai';
import { parseDataUrl } from 'data-urls';
import { writeFile } from 'node:fs/promises';

const response = await ai.generate({
  model: googleAI.model('gemini-2.5-flash-preview-tts'), // Example model with media capabilities
  prompt: 'An illustration of a dog wearing a space suit, photorealistic',
  output: { format: 'media' },
});

const imagePart = response.output;
if (imagePart?.media?.url) {
  const parsed = parseDataUrl(imagePart.media.url);
  if (parsed) {
    await writeFile('dog.png', parsed.body);
  }
}
```
</LanguageContent>

<LanguageContent lang="go">
```go
// Image generation support varies by Go implementation
// Check the specific plugin documentation for media generation
```
</LanguageContent>

<LanguageContent lang="python">
```python
# Image generation support varies by Python implementation
# Check the specific plugin documentation for media generation
```
</LanguageContent>

### Next steps

#### Learn more about Genkit

- As an app developer, the primary way you influence the output of generative AI
  models is through prompting. Read [Prompt management](/docs/dotprompt) to learn how
  Genkit helps you develop effective prompts and manage them in your codebase.
- Although `generate()` is the nucleus of every generative AI powered
  application, real-world applications usually require additional work before
  and after invoking a generative AI model. To reflect this, Genkit introduces
  the concept of _flows_, which are defined like functions but add additional
  features such as observability and simplified deployment. To learn more, see
  [Defining workflows](/docs/flows).

#### Advanced LLM use

- Many of your users will have interacted with large language models for the first time through chatbots. Although LLMs are capable of much more than simulating conversations, it remains a familiar and useful style of interaction. Even when your users will not be interacting directly with the model in this way, the conversational style of prompting is a powerful way to influence the output generated by an AI model. Read [Multi-turn chats](/docs/chat) to learn how to use Genkit as part of an LLM chat implementation.
- One way to enhance the capabilities of LLMs is to prompt them with a list of
  ways they can request more information from you, or request you to perform
  some action. This is known as _tool calling_ or _function calling_. Models
  that are trained to support this capability can respond to a prompt with a
  specially-formatted response, which indicates to the calling application that
  it should perform some action and send the result back to the LLM along with
  the original prompt. Genkit has library functions that automate both the
  prompt generation and the call-response loop elements of a tool calling
  implementation. See [Tool calling](/docs/tool-calling) to learn more.
- Retrieval-augmented generation (RAG) is a technique used to introduce
  domain-specific information into a model's output. This is accomplished by
  inserting relevant information into a prompt before passing it on to the
  language model. A complete RAG implementation requires you to bring several
  technologies together: text embedding generation models, vector databases, and
  large language models. See [Retrieval-augmented generation (RAG)](/docs/rag) to
  learn how Genkit simplifies the process of coordinating these various
  elements.

#### Testing model output

As a software engineer, you're used to deterministic systems where the same
input always produces the same output. However, with AI models being
probabilistic, the output can vary based on subtle nuances in the input, the
model's training data, and even randomness deliberately introduced by parameters
like temperature.

Genkit's evaluators are structured ways to assess the quality of your LLM's
responses, using a variety of strategies. Read more on the
[Evaluation](/docs/evaluation) page.
